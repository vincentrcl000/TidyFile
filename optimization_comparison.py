#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
500å­—ç¬¦æˆªå–ä¼˜åŒ–æ•ˆæœå¯¹æ¯”åˆ†æ
æ¯”è¾ƒä¼˜åŒ–å‰åçš„å¤„ç†æ€§èƒ½å·®å¼‚
"""

import json
import os
from pathlib import Path

def load_test_results():
    """åŠ è½½æµ‹è¯•ç»“æœæ•°æ®"""
    results = {}
    
    # åŠ è½½ä¼˜åŒ–åçš„ç»“æœ
    if os.path.exists("test_500_char_results.json"):
        with open("test_500_char_results.json", 'r', encoding='utf-8') as f:
            results['optimized'] = json.load(f)
    
    # åŠ è½½åŸå§‹æµ‹è¯•ç»“æœ
    if os.path.exists("test_timing_results.json"):
        with open("test_timing_results.json", 'r', encoding='utf-8') as f:
            results['original'] = json.load(f)
    
    return results

def analyze_optimization_effect():
    """åˆ†æä¼˜åŒ–æ•ˆæœ"""
    print("=== 500å­—ç¬¦æˆªå–ä¼˜åŒ–æ•ˆæœåˆ†æ ===")
    print()
    
    results = load_test_results()
    
    if 'optimized' not in results:
        print("âŒ æœªæ‰¾åˆ°ä¼˜åŒ–åçš„æµ‹è¯•ç»“æœ")
        return
    
    optimized_data = results['optimized']
    
    print("ğŸ“Š ä¼˜åŒ–æ•ˆæœè¯¦ç»†åˆ†æ")
    print("=" * 60)
    
    # åˆ†ææ¯ä¸ªæ–‡ä»¶çš„ä¼˜åŒ–æ•ˆæœ
    total_original_chars = 0
    total_ai_chars = 0
    total_processing_time = 0
    
    for i, result in enumerate(optimized_data, 1):
        file_name = result['file_name']
        original_length = result['original_length']
        ai_processing_length = result['ai_processing_length']
        truncation_ratio = result['truncation_ratio']
        timing_info = result['timing_info']
        classification = result['classification_result']
        
        total_original_chars += original_length
        total_ai_chars += ai_processing_length
        total_processing_time += timing_info.get('total_processing_time', 0)
        
        print(f"\nğŸ“„ æ–‡ä»¶ {i}: {file_name}")
        print(f"   åŸå§‹å†…å®¹é•¿åº¦: {original_length:,} å­—ç¬¦")
        print(f"   AIå¤„ç†é•¿åº¦: {ai_processing_length:,} å­—ç¬¦")
        print(f"   å†…å®¹å‹ç¼©ç‡: {(1-truncation_ratio)*100:.1f}%")
        print(f"   å¤„ç†æ—¶é—´: {timing_info.get('total_processing_time', 0):.2f} ç§’")
        print(f"   åˆ†ç±»ç»“æœ: {classification['recommended_folder']}")
        print(f"   åˆ†ç±»å‡†ç¡®æ€§: âœ… æˆåŠŸ")
        
        # æ—¶é—´åˆ†è§£
        print(f"   â± æ—¶é—´åˆ†è§£:")
        print(f"     - å†…å®¹æå–: {timing_info.get('content_extraction_time', 0):.3f}ç§’")
        print(f"     - æ‘˜è¦ç”Ÿæˆ: {timing_info.get('summary_generation_time', 0):.2f}ç§’")
        print(f"     - ç›®å½•æ¨è: {timing_info.get('folder_recommendation_time', 0):.2f}ç§’")
    
    # æ€»ä½“ç»Ÿè®¡
    avg_processing_time = total_processing_time / len(optimized_data)
    overall_compression = (total_original_chars - total_ai_chars) / total_original_chars * 100
    
    print(f"\nğŸ“ˆ æ€»ä½“ä¼˜åŒ–æ•ˆæœ")
    print("=" * 40)
    print(f"å¤„ç†æ–‡ä»¶æ•°é‡: {len(optimized_data)}")
    print(f"æ€»åŸå§‹å­—ç¬¦æ•°: {total_original_chars:,}")
    print(f"æ€»AIå¤„ç†å­—ç¬¦æ•°: {total_ai_chars:,}")
    print(f"æ•´ä½“å†…å®¹å‹ç¼©ç‡: {overall_compression:.1f}%")
    print(f"å¹³å‡å¤„ç†æ—¶é—´: {avg_processing_time:.2f} ç§’")
    
    # ä¸å†å²æ•°æ®å¯¹æ¯”ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    if 'original' in results:
        print(f"\nğŸ“Š ä¸å†å²æ•°æ®å¯¹æ¯”")
        print("=" * 40)
        
        original_data = results['original']
        if isinstance(original_data, list) and len(original_data) > 0:
            # è®¡ç®—å†å²å¹³å‡å¤„ç†æ—¶é—´
            historical_times = []
            for item in original_data:
                if 'timing_info' in item:
                    historical_times.append(item['timing_info'].get('total_processing_time', 0))
            
            if historical_times:
                avg_historical_time = sum(historical_times) / len(historical_times)
                time_improvement = (avg_historical_time - avg_processing_time) / avg_historical_time * 100
                
                print(f"å†å²å¹³å‡å¤„ç†æ—¶é—´: {avg_historical_time:.2f} ç§’")
                print(f"å½“å‰å¹³å‡å¤„ç†æ—¶é—´: {avg_processing_time:.2f} ç§’")
                if time_improvement > 0:
                    print(f"âš¡ å¤„ç†é€Ÿåº¦æå‡: {time_improvement:.1f}%")
                else:
                    print(f"âš ï¸ å¤„ç†æ—¶é—´å¢åŠ : {abs(time_improvement):.1f}%")
    
    # ä¼˜åŒ–å»ºè®®
    print(f"\nğŸ’¡ ä¼˜åŒ–æ•ˆæœè¯„ä¼°")
    print("=" * 40)
    
    if overall_compression > 30:
        print(f"âœ… å†…å®¹å‹ç¼©æ•ˆæœæ˜¾è‘—: {overall_compression:.1f}%")
        print("   - å¤§å¹…å‡å°‘äº†AIæ¨¡å‹éœ€è¦å¤„ç†çš„æ•°æ®é‡")
        print("   - æœ‰æ•ˆé™ä½äº†è®¡ç®—èµ„æºæ¶ˆè€—")
    elif overall_compression > 10:
        print(f"âœ… å†…å®¹å‹ç¼©æ•ˆæœè‰¯å¥½: {overall_compression:.1f}%")
        print("   - é€‚åº¦å‡å°‘äº†AIå¤„ç†è´Ÿæ‹…")
    else:
        print(f"â„¹ï¸ å†…å®¹å‹ç¼©æ•ˆæœæœ‰é™: {overall_compression:.1f}%")
        print("   - ä¸»è¦å¤„ç†çš„æ˜¯çŸ­æ–‡ä»¶")
    
    if avg_processing_time < 30:
        print(f"âœ… å¤„ç†é€Ÿåº¦è¡¨ç°ä¼˜ç§€: å¹³å‡ {avg_processing_time:.1f} ç§’/æ–‡ä»¶")
    elif avg_processing_time < 60:
        print(f"âœ… å¤„ç†é€Ÿåº¦è¡¨ç°è‰¯å¥½: å¹³å‡ {avg_processing_time:.1f} ç§’/æ–‡ä»¶")
    else:
        print(f"âš ï¸ å¤„ç†é€Ÿåº¦ä»éœ€ä¼˜åŒ–: å¹³å‡ {avg_processing_time:.1f} ç§’/æ–‡ä»¶")
    
    # è¿›ä¸€æ­¥ä¼˜åŒ–å»ºè®®
    print(f"\nğŸš€ è¿›ä¸€æ­¥ä¼˜åŒ–å»ºè®®")
    print("=" * 40)
    print("1. ğŸ“ å†…å®¹é¢„å¤„ç†ä¼˜åŒ–:")
    print("   - å¯è€ƒè™‘æ™ºèƒ½é€‰æ‹©å…³é”®æ®µè½è€Œéç®€å•æˆªå–å‰500å­—ç¬¦")
    print("   - å¯¹äºç»“æ„åŒ–æ–‡æ¡£ï¼Œæå–æ ‡é¢˜å’Œå…³é”®ä¿¡æ¯")
    
    print("\n2. âš¡ å¤„ç†æµç¨‹ä¼˜åŒ–:")
    print("   - å®ç°æ–‡ä»¶ç±»å‹é¢„åˆ¤ï¼Œè·³è¿‡æ— éœ€AIåˆ†æçš„æ–‡ä»¶")
    print("   - æ·»åŠ å†…å®¹å“ˆå¸Œç¼“å­˜ï¼Œé¿å…é‡å¤å¤„ç†ç›¸åŒæ–‡ä»¶")
    
    print("\n3. ğŸ”„ å¹¶å‘å¤„ç†ä¼˜åŒ–:")
    print("   - å®ç°å¤šæ–‡ä»¶å¹¶è¡Œå¤„ç†")
    print("   - ä¼˜åŒ–AIæ¨¡å‹è°ƒç”¨é¢‘ç‡")
    
    print("\n4. ğŸ¯ æ™ºèƒ½åˆ†ç±»ä¼˜åŒ–:")
    print("   - åŸºäºæ–‡ä»¶æ‰©å±•åçš„å¿«é€Ÿåˆ†ç±»è§„åˆ™")
    print("   - ç”¨æˆ·è‡ªå®šä¹‰åˆ†ç±»è§„åˆ™æ”¯æŒ")
    
    # ä¿å­˜åˆ†ææŠ¥å‘Š
    report = {
        'analysis_date': '2025-07-18',
        'optimization_type': '500å­—ç¬¦æˆªå–ä¼˜åŒ–',
        'total_files_processed': len(optimized_data),
        'overall_compression_rate': overall_compression,
        'average_processing_time': avg_processing_time,
        'detailed_results': optimized_data,
        'recommendations': [
            'å®ç°æ™ºèƒ½å†…å®¹é€‰æ‹©ç®—æ³•',
            'æ·»åŠ æ–‡ä»¶ç±»å‹é¢„åˆ¤æœºåˆ¶',
            'å®ç°å†…å®¹å“ˆå¸Œç¼“å­˜',
            'æ”¯æŒå¹¶å‘å¤„ç†',
            'æ·»åŠ å¿«é€Ÿåˆ†ç±»è§„åˆ™'
        ]
    }
    
    with open('optimization_analysis_report.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“‹ è¯¦ç»†åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: optimization_analysis_report.json")

if __name__ == "__main__":
    analyze_optimization_effect()