#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIæ™ºèƒ½åˆ†ç±»ä¸“ç”¨æ–‡ä»¶æ•´ç†æ ¸å¿ƒæ¨¡å—
"""
import os
import shutil
import logging
import json
from datetime import datetime
from typing import List, Dict, Tuple, Optional, Any
from pathlib import Path
import ollama
from transfer_log_manager import TransferLogManager
from classification_rules_manager import ClassificationRulesManager
import PyPDF2
import docx
import time
import requests

class FileOrganizerError(Exception):
    pass

class QwenLongClient:
    """Qwen-Longåœ¨çº¿æ¨¡å‹å®¢æˆ·ç«¯"""
    def __init__(self, api_key: str = None):
        try:
            from openai import OpenAI
            self.api_key = api_key or "sk-9b728f2f153f4a81b507caeced3380d1"
            self.client = OpenAI(
                api_key=self.api_key,
                base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
            )
            self.model_name = "qwen-long"
            logging.info("Qwen-Longåœ¨çº¿æ¨¡å‹å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except ImportError:
            raise FileOrganizerError("éœ€è¦å®‰è£…openaiåº“: pip install openai")
        except Exception as e:
            raise FileOrganizerError(f"åˆå§‹åŒ–Qwen-Longå®¢æˆ·ç«¯å¤±è´¥: {e}")
    
    def chat_with_retry(self, messages: List[Dict], max_retries: int = 3) -> str:
        """ä¸Qwen-Longæ¨¡å‹å¯¹è¯ï¼Œæ”¯æŒé‡è¯•æœºåˆ¶"""
        last_error = None
        
        for attempt in range(max_retries):
            try:
                logging.info(f"å°è¯•ä½¿ç”¨Qwen-Longæ¨¡å‹ (ç¬¬{attempt + 1}æ¬¡)")
                
                completion = self.client.chat.completions.create(
                    model="qwen-long",
                    messages=messages,
                    # Qwen3æ¨¡å‹é€šè¿‡enable_thinkingå‚æ•°æ§åˆ¶æ€è€ƒè¿‡ç¨‹
                    extra_body={"enable_thinking": False},
                )
                
                if completion.choices and len(completion.choices) > 0:
                    content = completion.choices[0].message.content.strip()
                    if content:
                        logging.info("Qwen-Longæ¨¡å‹å“åº”æˆåŠŸ")
                        return content
                    else:
                        raise FileOrganizerError("Qwen-Longæ¨¡å‹è¿”å›ç©ºå†…å®¹")
                else:
                    raise FileOrganizerError("Qwen-Longæ¨¡å‹è¿”å›æ— æ•ˆå“åº”æ ¼å¼")
                    
            except Exception as e:
                last_error = e
                logging.warning(f"Qwen-Longæ¨¡å‹å“åº”å¤±è´¥ (ç¬¬{attempt + 1}æ¬¡): {e}")
                if attempt < max_retries - 1:
                    time.sleep(1)  # é‡è¯•å‰ç­‰å¾…1ç§’
                    continue
        
        error_msg = f"Qwen-Longæ¨¡å‹æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œæœ€åé”™è¯¯: {last_error}"
        logging.error(error_msg)
        raise FileOrganizerError(error_msg)

class OllamaClient:
    def __init__(self, model_name: str = None, host: str = None):
        self.model_name = model_name
        # ä¼˜å…ˆLM Studioï¼Œå…¶æ¬¡å±€åŸŸç½‘Ollamaï¼Œæœ€åæœ¬åœ°Ollama
        self.hosts_to_try = [
            "http://10.64.21.220:1234/v1",  # LM Studio
            "http://10.64.21.220:11434",    # å±€åŸŸç½‘Ollama
            "http://localhost:11434"        # æœ¬åœ°Ollama
        ] if host is None else [host]
        self.client = None
        self.available_models = []
        self._try_connect_and_select_model()

    def _try_connect_and_select_model(self):
        last_error = None
        for host in self.hosts_to_try:
            try:
                self.host = host
                if "1234/v1" in host:  # LM Studio API
                    self.client = self._create_lmstudio_client(host)
                else:  # Ollama API
                    self.client = ollama.Client(host=host)
                self._validate_connection()
                logging.info(f"æˆåŠŸè¿æ¥åˆ°AIæœåŠ¡: {host}")
                return
            except Exception as e:
                last_error = e
                logging.warning(f"è¿æ¥AIæœåŠ¡å¤±è´¥: {host}ï¼Œé”™è¯¯: {e}")
                continue
        raise FileOrganizerError(f"æ‰€æœ‰AIæœåŠ¡è¿æ¥å¤±è´¥ï¼Œæœ€åé”™è¯¯: {last_error}")

    def _create_lmstudio_client(self, host: str):
        """åˆ›å»ºLM Studioå®¢æˆ·ç«¯ï¼ˆå…¼å®¹Ollama APIï¼‰"""
        class LMStudioClient:
            def __init__(self, host):
                self.host = host
                self.base_url = host
            def list(self):
                try:
                    response = requests.get(f"{self.base_url}/models", timeout=10)
                    response.raise_for_status()
                    models_data = response.json()
                    # è½¬æ¢ä¸ºOllamaæ ¼å¼ï¼Œåªè¿”å›loadedçŠ¶æ€çš„æ¨¡å‹
                    models = []
                    for model in models_data.get('data', []):
                        # æ£€æŸ¥æ¨¡å‹çŠ¶æ€ï¼Œåªè¿”å›loadedçŠ¶æ€çš„æ¨¡å‹
                        if model.get('status') == 'loaded':
                            models.append({'name': model.get('id', '')})
                    return {'models': models}
                except Exception as e:
                    raise Exception(f"LM Studio APIè°ƒç”¨å¤±è´¥: {e}")
            def chat(self, **kwargs):
                try:
                    messages = kwargs.get('messages', [])
                    model = kwargs.get('model', '')
                    # è½¬æ¢ä¸ºLM Studioæ ¼å¼
                    lmstudio_messages = []
                    for msg in messages:
                        lmstudio_messages.append({
                            'role': msg.get('role', 'user'),
                            'content': msg.get('content', '')
                        })
                    payload = {
                        'model': model,
                        'messages': lmstudio_messages,
                        'stream': False
                    }
                    response = requests.post(f"{self.base_url}/chat/completions", 
                                           json=payload, timeout=60)
                    
                    # æ£€æŸ¥HTTPçŠ¶æ€ç 
                    if response.status_code != 200:
                        error_msg = f"LM Studio APIè¿”å›é”™è¯¯çŠ¶æ€ç : {response.status_code}"
                        try:
                            error_data = response.json()
                            if 'error' in error_data:
                                error_msg += f", é”™è¯¯ä¿¡æ¯: {error_data['error']}"
                        except:
                            error_msg += f", å“åº”å†…å®¹: {response.text[:200]}"
                        raise Exception(error_msg)
                    
                    result = response.json()
                    
                    # æ£€æŸ¥å“åº”æ ¼å¼
                    if 'choices' not in result or not result['choices']:
                        raise Exception(f"LM Studioå“åº”æ ¼å¼é”™è¯¯: {result}")
                    
                    choice = result['choices'][0]
                    if 'message' not in choice or 'content' not in choice['message']:
                        raise Exception(f"LM Studioå“åº”ç¼ºå°‘messageæˆ–content: {choice}")
                    
                    # è½¬æ¢ä¸ºOllamaæ ¼å¼
                    return {
                        'message': {
                            'content': choice['message']['content']
                        }
                    }
                except Exception as e:
                    raise Exception(f"LM Studio chatè°ƒç”¨å¤±è´¥: {e}")
        return LMStudioClient(host)

    def _validate_connection(self) -> None:
        try:
            models_response = self.client.list()
            if hasattr(models_response, 'models'):
                models_list = models_response.models
            elif isinstance(models_response, dict) and 'models' in models_response:
                models_list = models_response['models']
            else:
                models_list = models_response if isinstance(models_response, list) else []
            self.available_models = []
            for model in models_list:
                if isinstance(model, dict):
                    if 'name' in model:
                        model_name = model['name']
                        if isinstance(model_name, str):
                            self.available_models.append(model_name)
                        else:
                            self.available_models.append(str(model_name))
                    elif 'model' in model:
                        model_name = model['model']
                        if isinstance(model_name, str):
                            self.available_models.append(model_name)
                        else:
                            self.available_models.append(str(model_name))
                elif isinstance(model, str):
                    self.available_models.append(model)
                else:
                    model_name = None
                    if hasattr(model, 'model'):
                        model_name = getattr(model, 'model')
                    elif hasattr(model, 'name'):
                        model_name = getattr(model, 'name')
                    if model_name:
                        if isinstance(model_name, str):
                            self.available_models.append(model_name)
                        else:
                            self.available_models.append(str(model_name))
                    else:
                        model_str = str(model)
                        if "model='" in model_str:
                            import re
                            match = re.search(r"model='([^']+)'", model_str)
                            if match:
                                self.available_models.append(match.group(1))
                            else:
                                self.available_models.append(model_str)
                        else:
                            self.available_models.append(model_str)
            # ä¼˜å…ˆé€‰æ‹©qwen/qwen3-8bæ¨¡å‹ï¼ˆLM Studioï¼‰ï¼Œå…¶æ¬¡qwen3:8bï¼ˆOllamaï¼‰
            preferred_models = []
            qwen_qwen3_8b = [m for m in self.available_models if 'qwen/qwen3-8b' in m.lower()]
            if qwen_qwen3_8b:
                preferred_models.extend(qwen_qwen3_8b)
                logging.info(f"ä¼˜å…ˆé€‰æ‹©LM Studio qwen/qwen3-8bæ¨¡å‹: {qwen_qwen3_8b}")
            else:
                qwen3_8b = [m for m in self.available_models if 'qwen3:8b' in m.lower()]
                if qwen3_8b:
                    preferred_models.extend(qwen3_8b)
                    logging.info(f"ä¼˜å…ˆé€‰æ‹©Ollama qwen3:8bæ¨¡å‹: {qwen3_8b}")
                else:
                    # å…¶æ¬¡qwen3ç³»åˆ—
                    qwen3_models = [m for m in self.available_models if 'qwen3' in m.lower()]
                    if qwen3_models:
                        preferred_models.extend(qwen3_models)
                        logging.info(f"æ‰¾åˆ°qwen3ç³»åˆ—æ¨¡å‹: {qwen3_models}")
                    # å…¶æ¬¡deepseekç³»åˆ—
                    deepseek_models = [m for m in self.available_models if 'deepseek' in m.lower()]
                    if deepseek_models:
                        preferred_models.extend(deepseek_models)
                        logging.info(f"æ‰¾åˆ°deepseekç³»åˆ—æ¨¡å‹: {deepseek_models}")
                    # å…¶å®ƒ
                    other_models = [m for m in self.available_models if 'qwen3' not in m.lower() and 'deepseek' not in m.lower()]
                    preferred_models.extend(other_models)
            if preferred_models:
                self.model_name = preferred_models[0]
                logging.info(f"è‡ªåŠ¨é€‰æ‹©æ¨¡å‹: {self.model_name}")
            else:
                self.model_name = self.available_models[0]
                logging.info(f"ä½¿ç”¨é»˜è®¤æ¨¡å‹: {self.model_name}")
            logging.info(f"å¯ç”¨æ¨¡å‹åˆ—è¡¨: {self.available_models}")
        except Exception as e:
            raise FileOrganizerError(f"è¿æ¥ AI æœåŠ¡å¤±è´¥: {e}")
    
    def chat_with_retry(self, messages: List[Dict], max_retries: Optional[int] = None) -> str:
        if max_retries is None:
            max_retries = len(self.available_models)
        last_error = None
        
        # é¦–å…ˆå°è¯•å½“å‰å·²è¿æ¥çš„host
        models_to_try = [self.model_name] + [m for m in self.available_models if m != self.model_name]
        
        logging.info(f"å¼€å§‹å°è¯•å½“å‰hostçš„æ¨¡å‹ï¼Œå¯ç”¨æ¨¡å‹: {self.available_models}")
        logging.info(f"å°†å°è¯•çš„æ¨¡å‹é¡ºåº: {models_to_try[:max_retries]}")
        
        for attempt, model_name in enumerate(models_to_try[:max_retries]):
            try:
                if not isinstance(model_name, str) or not model_name:
                    logging.warning(f"è·³è¿‡æ— æ•ˆæ¨¡å‹å: {model_name}")
                    continue
                
                # ä½¿ç”¨å·²åˆå§‹åŒ–çš„å®¢æˆ·ç«¯ï¼Œè€Œä¸æ˜¯é‡æ–°åˆ›å»º
                if not self.client:
                    raise FileOrganizerError("å®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
                
                logging.info(f"å°è¯•ä½¿ç”¨æ¨¡å‹: {model_name} (ç¬¬{attempt + 1}æ¬¡å°è¯•)")
                
                # æ ¹æ®å®¢æˆ·ç«¯ç±»å‹ä½¿ç”¨ä¸åŒçš„è°ƒç”¨æ–¹å¼
                if hasattr(self.client, 'chat'):
                    # ä½¿ç”¨è‡ªå®šä¹‰å®¢æˆ·ç«¯ï¼ˆå¦‚LMStudioClientï¼‰
                    chat_params = {
                        'model': model_name,
                        'messages': messages
                    }
                    response = self.client.chat(**chat_params)
                else:
                    # ä½¿ç”¨Ollamaå®¢æˆ·ç«¯
                    chat_params = {
                        'model': model_name,
                        'messages': messages,
                        'options': {'enable_thinking': False}
                    }
                    response = self.client.chat(**chat_params)
                
                # éªŒè¯å“åº”æ ¼å¼
                if not response or 'message' not in response or 'content' not in response['message']:
                    raise FileOrganizerError(f"æ¨¡å‹ {model_name} è¿”å›æ— æ•ˆå“åº”æ ¼å¼: {response}")
                
                content = response['message']['content'].strip()
                if not content:
                    raise FileOrganizerError(f"æ¨¡å‹ {model_name} è¿”å›ç©ºå†…å®¹")
                
                if model_name != self.model_name:
                    logging.info(f"æ¨¡å‹åˆ‡æ¢æˆåŠŸ: {self.model_name} -> {model_name}")
                    self.model_name = model_name
                
                logging.info(f"æ¨¡å‹ {model_name} å“åº”æˆåŠŸ")
                return content
                
            except Exception as e:
                last_error = e
                logging.warning(f"æ¨¡å‹ {model_name} å“åº”å¤±è´¥: {e}")
                if attempt < max_retries - 1:
                    logging.info(f"å‡†å¤‡å°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹...")
                    continue
        
        # å½“å‰hostçš„æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥äº†ï¼Œå°è¯•å…¶ä»–host
        logging.warning(f"å½“å‰host {self.host} çš„æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥ï¼Œå°è¯•å…¶ä»–host...")
        
        # å°è¯•å…¶ä»–host
        for host in self.hosts_to_try:
            if host == self.host:  # è·³è¿‡å½“å‰å·²å¤±è´¥çš„host
                continue
                
            try:
                logging.info(f"å°è¯•è¿æ¥åˆ°host: {host}")
                if "1234/v1" in host:  # LM Studio API
                    client = self._create_lmstudio_client(host)
                else:  # Ollama API
                    client = ollama.Client(host=host)
                
                # è·å–è¯¥hostçš„å¯ç”¨æ¨¡å‹
                models_response = client.list()
                if hasattr(models_response, 'models'):
                    models_list = models_response.models
                elif isinstance(models_response, dict) and 'models' in models_response:
                    models_list = models_response['models']
                else:
                    models_list = models_response if isinstance(models_response, list) else []
                
                available_models = []
                for model in models_list:
                    if isinstance(model, dict):
                        if 'name' in model:
                            model_name = model['name']
                            if isinstance(model_name, str):
                                available_models.append(model_name)
                            else:
                                available_models.append(str(model_name))
                        elif 'model' in model:
                            model_name = model['model']
                            if isinstance(model_name, str):
                                available_models.append(model_name)
                            else:
                                available_models.append(str(model_name))
                    elif isinstance(model, str):
                        available_models.append(model)
                    else:
                        model_name = None
                        if hasattr(model, 'model'):
                            model_name = getattr(model, 'model')
                        elif hasattr(model, 'name'):
                            model_name = getattr(model, 'name')
                        if model_name:
                            if isinstance(model_name, str):
                                available_models.append(model_name)
                            else:
                                available_models.append(str(model_name))
                
                if not available_models:
                    logging.warning(f"host {host} æ²¡æœ‰å¯ç”¨æ¨¡å‹")
                    continue
                
                # ä¼˜å…ˆæ¨¡å‹é¡ºåº
                models_to_try = []
                qwen3_models = [m for m in available_models if 'qwen3' in m.lower()]
                if qwen3_models:
                    models_to_try.extend(qwen3_models)
                deepseek_models = [m for m in available_models if 'deepseek' in m.lower()]
                if deepseek_models:
                    models_to_try.extend(deepseek_models)
                other_models = [m for m in available_models if 'qwen3' not in m.lower() and 'deepseek' not in m.lower()]
                models_to_try.extend(other_models)
                
                if not models_to_try:
                    models_to_try = available_models
                
                logging.info(f"host {host} å¯ç”¨æ¨¡å‹: {available_models}")
                logging.info(f"å°†å°è¯•çš„æ¨¡å‹é¡ºåº: {models_to_try}")
                
                # å°è¯•è¯¥hostçš„æ¨¡å‹
                for model_name in models_to_try:
                    try:
                        logging.info(f"å°è¯•ä½¿ç”¨host {host} çš„æ¨¡å‹: {model_name}")
                        
                        if hasattr(client, 'chat'):
                            # ä½¿ç”¨è‡ªå®šä¹‰å®¢æˆ·ç«¯ï¼ˆå¦‚LMStudioClientï¼‰
                            chat_params = {
                                'model': model_name,
                                'messages': messages
                            }
                            response = client.chat(**chat_params)
                        else:
                            # ä½¿ç”¨Ollamaå®¢æˆ·ç«¯
                            chat_params = {
                                'model': model_name,
                                'messages': messages,
                                'options': {'enable_thinking': False}
                            }
                            response = client.chat(**chat_params)
                        
                        # éªŒè¯å“åº”æ ¼å¼
                        if not response or 'message' not in response or 'content' not in response['message']:
                            raise FileOrganizerError(f"æ¨¡å‹ {model_name} è¿”å›æ— æ•ˆå“åº”æ ¼å¼: {response}")
                        
                        content = response['message']['content'].strip()
                        if not content:
                            raise FileOrganizerError(f"æ¨¡å‹ {model_name} è¿”å›ç©ºå†…å®¹")
                        
                        # æ›´æ–°å½“å‰å®¢æˆ·ç«¯å’Œæ¨¡å‹ä¿¡æ¯
                        self.client = client
                        self.host = host
                        self.model_name = model_name
                        self.available_models = available_models
                        
                        logging.info(f"æˆåŠŸåˆ‡æ¢åˆ°host {host}ï¼Œä½¿ç”¨æ¨¡å‹: {model_name}")
                        return content
                        
                    except Exception as e:
                        last_error = e
                        logging.warning(f"host {host} çš„æ¨¡å‹ {model_name} å“åº”å¤±è´¥: {e}")
                        continue
                
                logging.warning(f"host {host} çš„æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥")
                
            except Exception as e:
                last_error = e
                logging.warning(f"è¿æ¥host {host} å¤±è´¥: {e}")
                continue
        
        # æ‰€æœ‰hostå’Œæ¨¡å‹éƒ½å¤±è´¥äº†
        error_msg = f"æ‰€æœ‰å¯ç”¨hostå’Œæ¨¡å‹éƒ½å“åº”å¤±è´¥ï¼Œæœ€åé”™è¯¯: {last_error}"
        logging.error(error_msg)
        raise FileOrganizerError(error_msg)

class FileOrganizer:
    def __init__(self, model_name: str = None, enable_transfer_log: bool = True):
        self.model_name = model_name
        self.ollama_client = None
        self.qwen_long_client = None
        self.enable_transfer_log = enable_transfer_log
        self.transfer_log_manager = None
        self.setup_logging()
        
        if self.enable_transfer_log:
            self.transfer_log_manager = TransferLogManager()
        
        # åˆå§‹åŒ–åˆ†ç±»è§„åˆ™ç®¡ç†å™¨
        self.classification_rules_manager = ClassificationRulesManager()
        
        # åˆå§‹åŒ–AIå‚æ•°
        self.ai_parameters = {
            'similarity_threshold': 0.7,
            'max_retries': 3,
            'content_extraction_length': 3000,
            'summary_length': 200,
            'classification_prompt_template': None
        }
    def setup_logging(self) -> None:
        """è®¾ç½®æ—¥å¿—é…ç½®ï¼Œä»…è¾“å‡ºåˆ°æ§åˆ¶å°"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler()
            ]
        )
        logging.info("æ–‡ä»¶æ•´ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    def initialize_ollama(self) -> None:
        """åˆå§‹åŒ–AIå®¢æˆ·ç«¯ï¼Œä¼˜å…ˆä½¿ç”¨qwen-longåœ¨çº¿æ¨¡å‹"""
        try:
            # é¦–å…ˆå°è¯•åˆå§‹åŒ–qwen-longåœ¨çº¿æ¨¡å‹
            try:
                self.qwen_long_client = QwenLongClient()
                logging.info("Qwen-Longåœ¨çº¿æ¨¡å‹å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
                return
            except Exception as e:
                logging.warning(f"Qwen-Longåœ¨çº¿æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            
            # å¦‚æœqwen-longå¤±è´¥ï¼Œå›é€€åˆ°æœ¬åœ°Ollamaæ¨¡å‹
            self.ollama_client = OllamaClient(self.model_name)
            logging.info("Ollama å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            raise FileOrganizerError(f"æ‰€æœ‰AIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
    def scan_target_folders(self, target_directory: str) -> List[str]:
        """æ‰«æç›®æ ‡æ–‡ä»¶å¤¹ï¼Œè¿”å›ç›¸å¯¹è·¯å¾„åˆ—è¡¨ï¼ˆå†…éƒ¨ä½¿ç”¨ï¼Œä¸è¾“å‡ºæ—¥å¿—ï¼‰"""
        try:
            target_path = Path(target_directory)
            if not target_path.exists():
                raise FileOrganizerError(f"ç›®æ ‡ç›®å½•ä¸å­˜åœ¨: {target_directory}")
            folders = []
            for item in target_path.rglob('*'):
                if item.is_dir() and item != target_path:  # æ’é™¤ç›®æ ‡ç›®å½•æœ¬èº«
                    relative_path = item.relative_to(target_path)
                    folders.append(str(relative_path))
            return folders
        except Exception as e:
            raise FileOrganizerError(f"æ‰«æç›®æ ‡æ–‡ä»¶å¤¹å¤±è´¥: {e}")
    def get_directory_tree_structure(self, target_directory: str) -> str:
        """ç”Ÿæˆç›®æ ‡ç›®å½•ç»“æ„ï¼Œè¿”å›å®Œæ•´è·¯å¾„åˆ—è¡¨"""
        try:
            target_path = Path(target_directory)
            if not target_path.exists():
                raise FileOrganizerError(f"ç›®æ ‡ç›®å½•ä¸å­˜åœ¨: {target_directory}")
            
            # ç›´æ¥æ‰«æå¹¶æ„å»ºç›®å½•ç»“æ„
            folders = []
            for item in target_path.rglob('*'):
                if item.is_dir() and item != target_path:  # æ’é™¤ç›®æ ‡ç›®å½•æœ¬èº«
                    relative_path = item.relative_to(target_path)
                    folders.append(str(relative_path))
            
            # æ„å»ºæ¸…æ™°çš„è·¯å¾„åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªå®Œæ•´è·¯å¾„
            path_lines = []
            for i, folder_path in enumerate(folders, 1):
                path_lines.append(f"{i}. {folder_path}")
            
            tree_structure = "\n".join(path_lines)
            logging.info(f"ç”Ÿæˆç›®å½•è·¯å¾„ç»“æ„ï¼Œå…± {len(path_lines)} è¡Œ")
            return tree_structure
        except Exception as e:
            raise FileOrganizerError(f"ç”Ÿæˆç›®å½•æ ‘ç»“æ„å¤±è´¥: {e}")
    def analyze_and_classify_file(self, file_path: str, target_directory: str) -> Dict[str, Any]:
        """
        æ–°çš„æ–‡ä»¶åˆ†æå’Œåˆ†ç±»æ–¹æ³•ï¼šå…ˆè§£æå†…å®¹ï¼Œç”Ÿæˆæ‘˜è¦ï¼Œå†æ¨èç›®å½•
        è¿”å›åŒ…å«æå–å†…å®¹ã€æ‘˜è¦å’Œæ¨èç›®å½•çš„å®Œæ•´ç»“æœ
        """
        start_time = time.time()
        timing_info = {}
        
        try:
            if not self.ollama_client and not self.qwen_long_client:
                init_start = time.time()
                self.initialize_ollama()
                timing_info['ollama_init_time'] = round(time.time() - init_start, 3)
            
            file_name = Path(file_path).name
            logging.info(f"å¼€å§‹åˆ†ææ–‡ä»¶: {file_name}")
            
            # ç¬¬ä¸€æ­¥ï¼šè§£ææ–‡ä»¶å†…å®¹
            extract_start = time.time()
            extracted_content = self._extract_file_content(file_path)
            extract_time = round(time.time() - extract_start, 3)
            timing_info['content_extraction_time'] = extract_time
            logging.info(f"æ–‡ä»¶å†…å®¹æå–å®Œæˆï¼Œé•¿åº¦: {len(extracted_content)} å­—ç¬¦ï¼Œè€—æ—¶: {extract_time}ç§’")
            
            # æ ¹æ®è®¾ç½®æˆªå–å†…å®¹ç”¨äºåç»­AIå¤„ç†ï¼Œæé«˜å¤„ç†æ•ˆç‡
            truncate_length = self.ai_parameters['content_extraction_length'] if self.ai_parameters['content_extraction_length'] < 2000 else len(extracted_content)
            content_for_ai = extracted_content[:truncate_length] if extracted_content else ""
            if len(extracted_content) > truncate_length:
                logging.info(f"å†…å®¹å·²æˆªå–è‡³å‰{truncate_length}å­—ç¬¦ç”¨äºAIå¤„ç†ï¼ˆåŸé•¿åº¦: {len(extracted_content)} å­—ç¬¦ï¼‰")
            
            # ç¬¬äºŒæ­¥ï¼šç”Ÿæˆ100å­—æ‘˜è¦ï¼ˆä½¿ç”¨æˆªå–åçš„å†…å®¹ï¼‰
            summary_start = time.time()
            summary = self._generate_content_summary(content_for_ai, file_name)
            summary_time = round(time.time() - summary_start, 3)
            timing_info['summary_generation_time'] = summary_time
            logging.info(f"å†…å®¹æ‘˜è¦ç”Ÿæˆå®Œæˆ: {summary[:50]}...ï¼Œè€—æ—¶: {summary_time}ç§’")
            
            # ç¬¬ä¸‰æ­¥ï¼šæ¨èæœ€åŒ¹é…çš„å­˜æ”¾ç›®å½•ï¼ˆä½¿ç”¨æˆªå–åçš„å†…å®¹ï¼‰
            recommend_start = time.time()
            recommended_folder, match_reason = self._recommend_target_folder(
                file_path, content_for_ai, summary, target_directory
            )
            recommend_time = round(time.time() - recommend_start, 3)
            timing_info['folder_recommendation_time'] = recommend_time
            
            total_time = round(time.time() - start_time, 3)
            timing_info['total_processing_time'] = total_time
            
            result = {
                'file_path': file_path,
                'file_name': file_name,
                'extracted_content': extracted_content,
                'content_summary': summary,
                'recommended_folder': recommended_folder,
                'match_reason': match_reason,
                'success': recommended_folder is not None,
                'timing_info': timing_info
            }
            
            if recommended_folder:
                logging.info(f"æ–‡ä»¶åˆ†æå®Œæˆ: {file_name} -> {recommended_folder}ï¼Œæ€»è€—æ—¶: {total_time}ç§’")
                logging.info(f"æ‘˜è¦: {summary}")
                logging.info(f"æ¨èç†ç”±: {match_reason}")
                logging.info(f"è¯¦ç»†è€—æ—¶ - å†…å®¹æå–: {extract_time}ç§’, æ‘˜è¦ç”Ÿæˆ: {summary_time}ç§’, ç›®å½•æ¨è: {recommend_time}ç§’")
            else:
                logging.warning(f"æ–‡ä»¶åˆ†æå¤±è´¥: {file_name}ï¼Œæ€»è€—æ—¶: {total_time}ç§’")
            
            return result
            
        except Exception as e:
            total_time = round(time.time() - start_time, 3)
            timing_info['total_processing_time'] = total_time
            logging.error(f"æ–‡ä»¶åˆ†æå¤±è´¥: {e}ï¼Œæ€»è€—æ—¶: {total_time}ç§’")
            return {
                'file_path': file_path,
                'file_name': Path(file_path).name,
                'extracted_content': '',
                'content_summary': '',
                'recommended_folder': None,
                'match_reason': f"åˆ†æå¤±è´¥: {str(e)}",
                'success': False,
                'error': str(e),
                'timing_info': timing_info
            }
    
    def classify_file(self, file_path: str, target_directory: str) -> tuple:
        """
        ä¿æŒåŸæœ‰çš„åˆ†ç±»æ–¹æ³•å…¼å®¹æ€§
        """
        result = self.analyze_and_classify_file(file_path, target_directory)
        return result['recommended_folder'], result['match_reason'], result['success']
    def _build_classification_prompt(self, file_path: str, target_directory: str) -> str:
        file_name = Path(file_path).name
        file_extension = Path(file_path).suffix.lower()
        file_content = ""
        content_readable = False
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                file_content = f.read()[:2000]
                printable_ratio = sum(1 for c in file_content if c.isprintable() or c.isspace()) / len(file_content) if file_content else 0
                if printable_ratio > 0.7:
                    content_readable = True
                else:
                    file_content = "æ–‡ä»¶å†…å®¹ä¸ºäºŒè¿›åˆ¶æ ¼å¼ï¼Œæ— æ³•è¯»å–"
        except Exception:
            for encoding in ['gbk', 'gb2312', 'latin-1']:
                try:
                    with open(file_path, 'r', encoding=encoding, errors='ignore') as f:
                        file_content = f.read()[:2000]
                        printable_ratio = sum(1 for c in file_content if c.isprintable() or c.isspace()) / len(file_content) if file_content else 0
                        if printable_ratio > 0.7:
                            content_readable = True
                            break
                        else:
                            file_content = "æ–‡ä»¶å†…å®¹ä¸ºäºŒè¿›åˆ¶æ ¼å¼ï¼Œæ— æ³•è¯»å–"
                except Exception:
                    continue
            if not content_readable:
                file_content = "æ— æ³•è¯»å–æ–‡ä»¶å†…å®¹"
        directory_structure = self.get_directory_tree_structure(target_directory)
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡ä»¶åˆ†ç±»åŠ©æ‰‹ã€‚è¯·é˜…è¯»åŸæ–‡ä»¶çš„å‰500ä¸ªå­—ï¼Œæˆ–è€…è¯†åˆ«ç¬¬ä¸€é¡µå†…å®¹ï¼Œæ ¹æ®æ–‡ä»¶å†…å®¹åˆ¤æ–­æ–‡ä»¶åº”è¯¥å½’ç±»åˆ°ä¸‹åˆ—å“ªä¸ªæ–‡ä»¶å¤¹ä¸­ï¼Œæ³¨æ„ï¼Œæ¯ä¸ªæ–‡ä»¶æ ¹æ®æ–‡ä»¶å†…å®¹åªåŒ¹é…ä¸€ä¸ªæ–‡ä»¶å¤¹ï¼ŒåŒ¹é…åŸåˆ™æ˜¯ï¼š

- ä¼˜å…ˆåŒ¹é…ï¼šæ–‡ä»¶å†…å®¹ä¸»é¢˜å’Œæ–‡ä»¶å¤¹ååŠæ–‡ä»¶è·¯å¾„åæœ€ç›¸ç¬¦
- é™çº§åŒ¹é…ï¼šå¦‚æœè¯»å–ä¸åˆ°æ–‡ä»¶å†…å®¹ï¼Œåˆ™ç”¨åŸæ–‡ä»¶åå’Œç›®æ ‡æ–‡ä»¶å¤¹ååŠæ–‡ä»¶å¤¹è·¯å¾„åæ¥åŒ¹é…

æ–‡ä»¶ä¿¡æ¯ï¼š
- åŸæ–‡ä»¶åï¼š{file_name}
- æ–‡ä»¶æ‰©å±•åï¼š{file_extension}
- æ–‡ä»¶å†…å®¹ï¼š{file_content if content_readable else "æ— æ³•è¯»å–å†…å®¹"}

ç›®æ ‡ç›®å½•æ–‡ä»¶å¤¹æ¸…å•å¦‚ä¸‹ï¼š
{directory_structure}

è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
åŸæ–‡ä»¶å|ç›®æ ‡æ–‡ä»¶å¤¹|åŒ¹é…ç†ç”±

å…¶ä¸­åŒ¹é…ç†ç”±æ ¼å¼ä¸ºï¼š
å†…å®¹åŒ¹é…ï¼š{{ç”¨ä¸å¤šäº20ä¸ªå­—æè¿°åŒ¹é…ç†ç”±}}
æˆ–è€…
æ— æ³•è¯»å–å†…å®¹ï¼Œé‡‡ç”¨æ–‡ä»¶å¤¹ååŒ¹é…ï¼š{{ç”¨ä¸å¤šäº20ä¸ªå­—æè¿°åŒ¹é…ç†ç”±}}

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ ¼å¼è¾“å‡ºï¼Œåªè¾“å‡ºä¸€è¡Œç»“æœã€‚
"""
        return prompt
    
    def _extract_file_content(self, file_path: str, max_length: int = 3000) -> str:
        """
        æå–æ–‡ä»¶å†…å®¹ï¼ˆä»file_reader.pyå¤åˆ¶çš„å®Œæ•´å®ç°ï¼‰
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            max_length: æœ€å¤§æå–é•¿åº¦
            
        Returns:
            æå–çš„æ–‡ä»¶å†…å®¹
        """
        try:
            file_path = Path(file_path)
            if not file_path.exists():
                raise Exception(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            
            file_extension = file_path.suffix.lower()
            logging.info(f"æ­£åœ¨æå–æ–‡ä»¶å†…å®¹: {file_path.name} (ç±»å‹: {file_extension})")
            
            # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©ä¸åŒçš„æå–æ–¹æ³•
            if file_extension == '.pdf':
                return self._extract_pdf_content(file_path, max_length)
            elif file_extension in ['.docx', '.doc']:
                return self._extract_docx_content(file_path, max_length)
            elif file_extension in ['.txt', '.md', '.py', '.js', '.html', '.css', '.json', '.xml', '.csv']:
                return self._extract_text_content(file_path, max_length)
            elif file_extension in ['.jpg', '.jpeg', '.png', '.bmp', '.gif']:
                return self._extract_image_info(file_path)
            else:
                # å°è¯•ä½œä¸ºæ–‡æœ¬æ–‡ä»¶è¯»å–
                return self._extract_text_content(file_path, max_length)
                
        except Exception as e:
            logging.error(f"æå–æ–‡ä»¶å†…å®¹å¤±è´¥: {e}")
            return f"æ— æ³•æå–æ–‡ä»¶å†…å®¹: {str(e)}"
    
    def _extract_pdf_content(self, file_path: Path, max_length: int) -> str:
        """
        æå–PDFæ–‡ä»¶å†…å®¹ï¼ˆä»file_reader.pyå¤åˆ¶çš„å®Œæ•´å®ç°ï¼‰
        """
        try:
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                content = ""
                
                # è¯»å–å‰å‡ é¡µå†…å®¹
                for page_num in range(min(3, len(pdf_reader.pages))):
                    page = pdf_reader.pages[page_num]
                    content += page.extract_text() + "\n"
                    
                    if len(content) >= max_length:
                        break
                
                return content[:max_length] if content else "PDFæ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–æ— æ³•æå–"
                
        except Exception as e:
            return f"PDFæ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}"
    
    def _extract_docx_content(self, file_path: Path, max_length: int) -> str:
        """
        æå–Wordæ–‡æ¡£å†…å®¹ï¼ˆä»file_reader.pyå¤åˆ¶çš„å®Œæ•´å®ç°ï¼‰
        """
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”å¯è¯»
            if not file_path.exists():
                return "æ–‡ä»¶ä¸å­˜åœ¨"
            
            if not file_path.is_file():
                return "è·¯å¾„ä¸æ˜¯æ–‡ä»¶"
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = file_path.stat().st_size
            if file_size == 0:
                return "æ–‡ä»¶ä¸ºç©º"
            
            # å°è¯•è¯»å–Wordæ–‡æ¡£
            try:
                doc = docx.Document(file_path)
                content = ""
                
                # æå–æ®µè½å†…å®¹
                for paragraph in doc.paragraphs:
                    if paragraph.text.strip():  # è·³è¿‡ç©ºæ®µè½
                        content += paragraph.text.strip() + "\n"
                        if len(content) >= max_length:
                            break
                
                # å¦‚æœæ®µè½å†…å®¹ä¸ºç©ºï¼Œå°è¯•æå–è¡¨æ ¼å†…å®¹
                if not content.strip():
                    for table in doc.tables:
                        for row in table.rows:
                            for cell in row.cells:
                                if cell.text.strip():
                                    content += cell.text.strip() + " "
                        content += "\n"
                        if len(content) >= max_length:
                            break
                
                return content[:max_length].strip() if content.strip() else "Wordæ–‡æ¡£å†…å®¹ä¸ºç©ºæˆ–æ— æ³•æå–"
                
            except Exception as docx_error:
                # å¦‚æœæ˜¯.docæ–‡ä»¶æˆ–æŸåçš„.docxæ–‡ä»¶ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
                if file_path.suffix.lower() == '.doc':
                    return "ä¸æ”¯æŒ.docæ ¼å¼ï¼Œè¯·è½¬æ¢ä¸º.docxæ ¼å¼"
                else:
                    return f"Wordæ–‡æ¡£æ ¼å¼é”™è¯¯æˆ–æ–‡ä»¶æŸå: {str(docx_error)}"
            
        except Exception as e:
            return f"Wordæ–‡æ¡£è¯»å–å¤±è´¥: {str(e)}"
    
    def _extract_text_content(self, file_path: Path, max_length: int) -> str:
        """
        æå–æ–‡æœ¬æ–‡ä»¶å†…å®¹ï¼ˆä»file_reader.pyå¤åˆ¶çš„å®Œæ•´å®ç°ï¼‰
        """
        try:
            # å°è¯•å¤šç§ç¼–ç æ ¼å¼
            encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1']
            
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding, errors='ignore') as f:
                        content = f.read(max_length)
                        
                        # æ£€æŸ¥å†…å®¹æ˜¯å¦å¯è¯»
                        printable_ratio = sum(1 for c in content if c.isprintable() or c.isspace()) / len(content) if content else 0
                        if printable_ratio > 0.7:
                            return content
                        
                except Exception:
                    continue
            
            return "æ–‡ä»¶å†…å®¹ä¸ºäºŒè¿›åˆ¶æ ¼å¼ï¼Œæ— æ³•è¯»å–"
            
        except Exception as e:
            return f"æ–‡æœ¬æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}"
    
    def _extract_image_info(self, file_path: Path) -> str:
        """
        æå–å›¾ç‰‡æ–‡ä»¶ä¿¡æ¯ï¼ˆä»file_reader.pyå¤åˆ¶çš„å®Œæ•´å®ç°ï¼‰
        """
        try:
            from PIL import Image
            with Image.open(file_path) as img:
                info = f"å›¾ç‰‡æ–‡ä»¶ä¿¡æ¯:\n"
                info += f"æ ¼å¼: {img.format}\n"
                info += f"å°ºå¯¸: {img.size[0]} x {img.size[1]}\n"
                info += f"æ¨¡å¼: {img.mode}\n"
                
                # å¦‚æœæœ‰EXIFä¿¡æ¯ï¼Œæå–ä¸€äº›åŸºæœ¬ä¿¡æ¯
                if hasattr(img, '_getexif') and img._getexif():
                    info += "åŒ…å«EXIFä¿¡æ¯\n"
                
                return info
                
        except Exception as e:
            return f"å›¾ç‰‡æ–‡ä»¶ä¿¡æ¯æå–å¤±è´¥: {str(e)}"
    
    def _generate_content_summary(self, content: str, file_name: str, summary_length: int = None) -> str:
        """
        ç”Ÿæˆæ–‡ä»¶å†…å®¹æ‘˜è¦
        
        Args:
            content: æ–‡ä»¶å†…å®¹
            file_name: æ–‡ä»¶å
            summary_length: æ‘˜è¦é•¿åº¦ï¼ˆå­—æ•°ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
        """
        try:
            if not content or content.startswith("æ— æ³•") or content.startswith("æ–‡ä»¶å†…å®¹ä¸ºäºŒè¿›åˆ¶"):
                return f"æ— æ³•ç”Ÿæˆæ‘˜è¦ï¼š{content[:50]}..."
            
            if not self.ollama_client and not self.qwen_long_client:
                self.initialize_ollama()
            
            # ä½¿ç”¨ä¼ å…¥çš„æ‘˜è¦é•¿åº¦ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
            target_length = summary_length if summary_length is not None else self.ai_parameters['summary_length']
            
            # æ„å»ºæ›´æ˜ç¡®çš„æç¤ºè¯ï¼Œé¿å…æ€è€ƒè¿‡ç¨‹è¾“å‡º
            prompt = f"""è¯·ä¸ºä»¥ä¸‹æ–‡ä»¶å†…å®¹ç”Ÿæˆä¸€ä¸ª{target_length}å­—ä»¥å†…çš„ä¸­æ–‡æ‘˜è¦ã€‚

æ–‡ä»¶åï¼š{file_name}

æ–‡ä»¶å†…å®¹ï¼š
{content}

è¦æ±‚ï¼š
1. æ¦‚æ‹¬æ–‡ä»¶çš„ä¸»è¦å†…å®¹å’Œä¸»é¢˜
2. çªå‡ºå…³é”®ä¿¡æ¯å’Œè¦ç‚¹
3. è¯­è¨€ç®€æ´æ˜äº†
4. å­—æ•°æ§åˆ¶åœ¨{target_length}å­—ä»¥å†…
5. ç›´æ¥è¾“å‡ºæ‘˜è¦å†…å®¹ï¼Œä¸è¦åŒ…å«ä»»ä½•æ€è€ƒè¿‡ç¨‹æˆ–è¯´æ˜æ–‡å­—
6. ä¸è¦ä½¿ç”¨"<think>"æ ‡ç­¾æˆ–ä»»ä½•æ€è€ƒè¿‡ç¨‹æè¿°

æ‘˜è¦ï¼š/no_think"""

            # åœ¨ä¼ é€’ç»™å¤§æ¨¡å‹çš„å®Œæ•´å†…å®¹æœ€å°¾éƒ¨æ·»åŠ /no_thinkæ ‡ç­¾
            final_prompt = prompt

            # ä½¿ç”¨ç³»ç»Ÿæç¤ºè¯æ¥æŠ‘åˆ¶æ€è€ƒè¿‡ç¨‹
            messages = [
                {
                    'role': 'system',
                    'content': 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£æ‘˜è¦åŠ©æ‰‹ã€‚é‡è¦ï¼šä¸è¦è¾“å‡ºä»»ä½•æ¨ç†è¿‡ç¨‹ã€æ€è€ƒæ­¥éª¤æˆ–è§£é‡Šã€‚ç›´æ¥æŒ‰è¦æ±‚è¾“å‡ºç»“æœã€‚åªè¾“å‡ºæ‘˜è¦å†…å®¹ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–ä¿¡æ¯ã€‚'
                },
                {
                    'role': 'user',
                    'content': final_prompt
                }
            ]

            # ä¼˜å…ˆä½¿ç”¨qwen-longå®¢æˆ·ç«¯ï¼Œå¦‚æœå¤±è´¥åˆ™å›é€€åˆ°ollamaå®¢æˆ·ç«¯
            if self.qwen_long_client:
                try:
                    summary = self.qwen_long_client.chat_with_retry(messages)
                except Exception as e:
                    logging.warning(f"Qwen-Longæ¨¡å‹æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼Œå›é€€åˆ°Ollama: {e}")
                    if self.ollama_client:
                        summary = self.ollama_client.chat_with_retry(messages)
                    else:
                        raise e
            else:
                summary = self.ollama_client.chat_with_retry(messages)
            
            # æ¸…ç†å¯èƒ½çš„æ€è€ƒè¿‡ç¨‹æ ‡ç­¾å’Œå†…å®¹
            summary = summary.replace('<think>', '').replace('</think>', '').strip()
            
            # ç§»é™¤å¸¸è§çš„æ€è€ƒè¿‡ç¨‹å¼€å¤´
            think_prefixes = [
                'å¥½çš„ï¼Œ', 'å¥½ï¼Œ', 'å—¯ï¼Œ', 'æˆ‘æ¥', 'æˆ‘éœ€è¦', 'é¦–å…ˆï¼Œ', 'è®©æˆ‘', 'ç°åœ¨æˆ‘è¦',
                'ç”¨æˆ·å¸Œæœ›', 'ç”¨æˆ·è¦æ±‚', 'ç”¨æˆ·è®©æˆ‘', 'æ ¹æ®', 'åŸºäº', 'è€ƒè™‘åˆ°', 'è®©æˆ‘å…ˆä»”ç»†çœ‹çœ‹',
                'ç”¨æˆ·ç»™äº†æˆ‘è¿™ä¸ªæŸ¥è¯¢', 'ç”¨æˆ·ç»™äº†æˆ‘è¿™ä¸ªä»»åŠ¡', 'ç”¨æˆ·ç»™äº†ä¸€ä¸ªä»»åŠ¡',
                'é¦–å…ˆï¼Œæˆ‘å¾—çœ‹ä¸€ä¸‹', 'é¦–å…ˆï¼Œæˆ‘è¦ç†è§£', 'é¦–å…ˆï¼Œæˆ‘å¾—ä»”ç»†çœ‹çœ‹',
                'å¥½çš„ï¼Œç”¨æˆ·è®©æˆ‘', 'ç”¨æˆ·è®©æˆ‘ç”Ÿæˆ', 'å†…å®¹æ¥è‡ªæ–‡ä»¶', 'é‡ç‚¹åŒ…æ‹¬', 'é¦–å…ˆï¼Œæˆ‘éœ€è¦ç¡®è®¤'
            ]
            
            # æ›´æ¿€è¿›çš„æ¸…ç†ï¼šç§»é™¤æ‰€æœ‰ä»¥æ€è€ƒè¿‡ç¨‹å¼€å¤´çš„å¥å­
            lines = summary.split('\n')
            cleaned_lines = []
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦ä»¥æ€è€ƒè¿‡ç¨‹å¼€å¤´
                is_think_line = False
                for prefix in think_prefixes:
                    if line.lower().startswith(prefix.lower()):
                        is_think_line = True
                        break
                
                # å¦‚æœä¸æ˜¯æ€è€ƒè¿‡ç¨‹ï¼Œä¿ç•™è¿™ä¸€è¡Œ
                if not is_think_line:
                    cleaned_lines.append(line)
            
            # å¦‚æœæ¸…ç†åæ²¡æœ‰å†…å®¹ï¼Œå°è¯•æ›´ç®€å•çš„æ–¹æ³•
            if not cleaned_lines:
                # æ‰¾åˆ°ç¬¬ä¸€ä¸ªä¸æ˜¯æ€è€ƒè¿‡ç¨‹çš„å¥å­
                sentences = summary.split('ã€‚')
                for sentence in sentences:
                    sentence = sentence.strip()
                    if not sentence:
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦ä»¥æ€è€ƒè¿‡ç¨‹å¼€å¤´
                    is_think_sentence = False
                    for prefix in think_prefixes:
                        if sentence.lower().startswith(prefix.lower()):
                            is_think_sentence = True
                            break
                    
                    if not is_think_sentence:
                        cleaned_lines.append(sentence)
                        break
            
            # å¦‚æœè¿˜æ˜¯æ²¡æœ‰å†…å®¹ï¼Œä½¿ç”¨åŸå§‹æ‘˜è¦çš„æœ€åä¸€éƒ¨åˆ†
            if not cleaned_lines:
                # å–æœ€å100ä¸ªå­—ç¬¦ä½œä¸ºæ‘˜è¦
                summary = summary[-100:] if len(summary) > 100 else summary
            
            # é‡æ–°ç»„åˆæ¸…ç†åçš„å†…å®¹
            if cleaned_lines:
                summary = 'ã€‚'.join(cleaned_lines)
            
            # ç¡®ä¿æ‘˜è¦é•¿åº¦ä¸è¶…è¿‡é™åˆ¶
            if len(summary) > target_length:
                summary = summary[:target_length-3] + "..."
            
            return summary.strip()
            
        except Exception as e:
            logging.error(f"ç”Ÿæˆæ‘˜è¦å¤±è´¥: {e}")
            return f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def _clean_ai_response(self, response: str) -> str:
        """æ¸…ç†AIå“åº”ä¸­çš„æ€è€ƒè¿‡ç¨‹"""
        if not response:
            return response
        
        print(f"ğŸ”§ å¼€å§‹æ¸…ç†AIå“åº”ï¼ŒåŸå§‹é•¿åº¦: {len(response)}")
        print(f"ğŸ”§ åŸå§‹å“åº”: {response}")
        
        # æ¸…ç†å¯èƒ½çš„æ€è€ƒè¿‡ç¨‹æ ‡ç­¾å’Œå†…å®¹
        response = response.replace('<think>', '').replace('</think>', '').strip()
        
        # ç§»é™¤å¸¸è§çš„æ€è€ƒè¿‡ç¨‹å¼€å¤´
        think_prefixes = [
            'å¥½çš„ï¼Œ', 'å¥½ï¼Œ', 'å—¯ï¼Œ', 'æˆ‘æ¥', 'æˆ‘éœ€è¦', 'é¦–å…ˆï¼Œ', 'è®©æˆ‘', 'ç°åœ¨æˆ‘è¦',
            'ç”¨æˆ·å¸Œæœ›', 'ç”¨æˆ·è¦æ±‚', 'ç”¨æˆ·è®©æˆ‘', 'æ ¹æ®', 'åŸºäº', 'è€ƒè™‘åˆ°', 'è®©æˆ‘å…ˆä»”ç»†çœ‹çœ‹',
            'ç”¨æˆ·ç»™äº†æˆ‘è¿™ä¸ªæŸ¥è¯¢', 'ç”¨æˆ·ç»™äº†æˆ‘è¿™ä¸ªä»»åŠ¡', 'ç”¨æˆ·ç»™äº†ä¸€ä¸ªä»»åŠ¡',
            'é¦–å…ˆï¼Œæˆ‘å¾—çœ‹ä¸€ä¸‹', 'é¦–å…ˆï¼Œæˆ‘è¦ç†è§£', 'é¦–å…ˆï¼Œæˆ‘å¾—ä»”ç»†çœ‹çœ‹',
            'å¥½çš„ï¼Œç”¨æˆ·è®©æˆ‘', 'ç”¨æˆ·è®©æˆ‘ç”Ÿæˆ', 'å†…å®¹æ¥è‡ªæ–‡ä»¶', 'é‡ç‚¹åŒ…æ‹¬', 'é¦–å…ˆï¼Œæˆ‘éœ€è¦ç¡®è®¤'
        ]
        
        # æ›´æ¿€è¿›çš„æ¸…ç†ï¼šç§»é™¤æ‰€æœ‰ä»¥æ€è€ƒè¿‡ç¨‹å¼€å¤´çš„å¥å­
        lines = response.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # æ£€æŸ¥æ˜¯å¦ä»¥æ€è€ƒè¿‡ç¨‹å¼€å¤´
            is_think_line = False
            for prefix in think_prefixes:
                if line.lower().startswith(prefix.lower()):
                    is_think_line = True
                    break
            
            # å¦‚æœä¸æ˜¯æ€è€ƒè¿‡ç¨‹ï¼Œä¿ç•™è¿™ä¸€è¡Œ
            if not is_think_line:
                cleaned_lines.append(line)
        
        print(f"ğŸ”§ æ¸…ç†åè¡Œæ•°: {len(cleaned_lines)}")
        
        # å¦‚æœæ¸…ç†åæ²¡æœ‰å†…å®¹ï¼Œå°è¯•æ›´ç®€å•çš„æ–¹æ³•
        if not cleaned_lines:
            print(f"ğŸ”§ æ¸…ç†åæ²¡æœ‰å†…å®¹ï¼Œå°è¯•å¥å­åˆ†å‰²æ–¹æ³•")
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªä¸æ˜¯æ€è€ƒè¿‡ç¨‹çš„å¥å­
            sentences = response.split('ã€‚')
            for sentence in sentences:
                sentence = sentence.strip()
                if not sentence:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦ä»¥æ€è€ƒè¿‡ç¨‹å¼€å¤´
                is_think_sentence = False
                for prefix in think_prefixes:
                    if sentence.lower().startswith(prefix.lower()):
                        is_think_sentence = True
                        break
                
                if not is_think_sentence:
                    cleaned_lines.append(sentence)
                    break
        
        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰å†…å®¹ï¼Œä½¿ç”¨åŸå§‹å“åº”çš„æœ€åä¸€éƒ¨åˆ†
        if not cleaned_lines:
            print(f"ğŸ”§ ä»ç„¶æ²¡æœ‰å†…å®¹ï¼Œä½¿ç”¨æœ€å100ä¸ªå­—ç¬¦")
            # å–æœ€å100ä¸ªå­—ç¬¦
            response = response[-100:] if len(response) > 100 else response
            print(f"ğŸ”§ æˆªæ–­åå“åº”: {response}")
            return response
        
        # é‡æ–°ç»„åˆæ¸…ç†åçš„å†…å®¹ï¼Œä¿æŒæ¢è¡Œç¬¦ä»¥æ”¯æŒå¤šè¡Œæ¨èè·¯å¾„
        if cleaned_lines:
            response = '\n'.join(cleaned_lines)
        
        print(f"ğŸ”§ æœ€ç»ˆæ¸…ç†ç»“æœ: {response}")
        return response.strip()
    
    def _recommend_target_folder(self, file_path: str, content: str, summary: str, target_directory: str, retry_count: int = 0) -> tuple:
        """
        åŸºäºæ–‡ä»¶å†…å®¹å’Œæ‘˜è¦æ¨èæœ€åŒ¹é…çš„ç›®æ ‡æ–‡ä»¶å¤¹
        æ”¯æŒé‡è¯•æœºåˆ¶ï¼Œå½“AIè¿”å›ä¸å­˜åœ¨çš„æ–‡ä»¶å¤¹æ—¶è‡ªåŠ¨é‡è¯•
        """
        try:
            if not self.ollama_client and not self.qwen_long_client:
                self.initialize_ollama()
            
            file_name = Path(file_path).name
            file_full_path = str(Path(file_path).absolute())
            
            directory_structure = self.get_directory_tree_structure(target_directory)
            
            print(f"ğŸ“„ æºæ–‡ä»¶å®Œæ•´è·¯å¾„: {file_full_path}")
            print(f"ğŸ“‹ ç›®å½•ç»“æ„:\n{directory_structure}")
            if retry_count > 0:
                print(f"ğŸ”„ ç¬¬ {retry_count} æ¬¡é‡è¯•AIåˆ†ç±»")
            
            # å­˜å‚¨AIæ¨èçš„å®Œæ•´è·¯å¾„
            self.recommended_folder_path = None
            # å­˜å‚¨æºæ–‡ä»¶å®Œæ•´è·¯å¾„
            self.source_file_path = file_full_path
            
            # åˆ¤æ–­æ˜¯å¦æœ‰æœ‰æ•ˆçš„å†…å®¹å’Œæ‘˜è¦
            has_valid_content = content and not content.startswith("æ— æ³•") and not content.startswith("æ–‡ä»¶å†…å®¹ä¸ºäºŒè¿›åˆ¶")
            has_valid_summary = summary and not summary.startswith("æ— æ³•") and not summary.startswith("æ‘˜è¦ç”Ÿæˆå¤±è´¥")
            
            print(f"ğŸ“„ æ–‡ä»¶å†…å®¹æœ‰æ•ˆ: {has_valid_content}")
            print(f"ğŸ“ æ‘˜è¦æœ‰æ•ˆ: {has_valid_summary}")
            if has_valid_summary:
                print(f"ğŸ“ æ‘˜è¦å†…å®¹: {summary[:100]}...")
            
            # è·å–ç”¨æˆ·è‡ªå®šä¹‰åˆ†ç±»è§„åˆ™
            custom_rules = self.classification_rules_manager.get_rules_for_prompt(
                self.scan_target_folders(target_directory)
            )
            
            # æ„å»ºé€šç”¨çš„åˆ†ç±»æç¤ºè¯
            if has_valid_content and has_valid_summary:
                prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡ä»¶åˆ†ç±»ä¸“å®¶ã€‚è¯·æ ¹æ®æ–‡ä»¶å†…å®¹åˆ†ç±»åˆ°æœ€åˆé€‚çš„ç›®æ ‡æ–‡ä»¶å¤¹ã€‚

æ–‡ä»¶ä¿¡æ¯ï¼š
- æ–‡ä»¶åï¼š{file_name}
- å†…å®¹æ‘˜è¦ï¼š{summary}

å¯é€‰çš„ç›®æ ‡æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆå¿…é¡»ä¸¥æ ¼ä»ä»¥ä¸‹åˆ—è¡¨ä¸­é€‰æ‹©ï¼‰ï¼š
{directory_structure}

{custom_rules}

åˆ†ç±»è¦æ±‚ï¼š
1. å¿…é¡»ä¸¥æ ¼ä»ä¸Šè¿°æ–‡ä»¶å¤¹è·¯å¾„åˆ—è¡¨ä¸­å¤åˆ¶å®Œæ•´çš„è·¯å¾„
2. ä¸èƒ½ä¿®æ”¹ã€ç¼©å†™æˆ–æ·»åŠ ä»»ä½•å†…å®¹åˆ°è·¯å¾„
3. ä¸èƒ½åˆ›å»ºæˆ–æƒ³è±¡ä¸å­˜åœ¨çš„æ–‡ä»¶å¤¹
4. ä¼˜å…ˆé€‰æ‹©æœ€å…·ä½“çš„åˆ†ç±»ï¼ˆè·¯å¾„è¶Šæ·±è¶Šå…·ä½“ï¼‰
5. æŒ‰åŒ¹é…åº¦ä»é«˜åˆ°ä½è¿”å›å‰ä¸‰ä¸ªè·¯å¾„
6. æ¯è¡Œä¸€ä¸ªè·¯å¾„ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–å†…å®¹
7. ä»”ç»†åˆ†ææ–‡ä»¶å†…å®¹ä¸»é¢˜ï¼Œé€‰æ‹©æœ€åŒ¹é…çš„æ–‡ä»¶å¤¹

è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼æŒ‰æ­¤æ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªå®Œæ•´è·¯å¾„ï¼‰ï¼š
ç¬¬ä¸€æ¨èï¼š[å®Œæ•´è·¯å¾„1]
ç¬¬äºŒæ¨èï¼š[å®Œæ•´è·¯å¾„2]
ç¬¬ä¸‰æ¨èï¼š[å®Œæ•´è·¯å¾„3]

è¯·å¼€å§‹æ¨èï¼š"""
            else:
                # æ— æ³•è·å–æœ‰æ•ˆå†…å®¹æ—¶ï¼Œä½¿ç”¨æ–‡ä»¶åè¿›è¡Œåˆ†ç±»
                file_extension = Path(file_name).suffix.lower()
                
                prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡ä»¶åˆ†ç±»ä¸“å®¶ã€‚ç”±äºæ— æ³•è¯»å–æ–‡ä»¶å†…å®¹ï¼Œè¯·æ ¹æ®æ–‡ä»¶åå’Œæ‰©å±•ååˆ†ç±»åˆ°æœ€åˆé€‚çš„ç›®æ ‡æ–‡ä»¶å¤¹ã€‚

æ–‡ä»¶ä¿¡æ¯ï¼š
- æ–‡ä»¶åï¼š{file_name}
- æ–‡ä»¶æ‰©å±•åï¼š{file_extension}

å¯é€‰çš„ç›®æ ‡æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆå¿…é¡»ä¸¥æ ¼ä»ä»¥ä¸‹åˆ—è¡¨ä¸­é€‰æ‹©ï¼‰ï¼š
{directory_structure}

{custom_rules}

åˆ†ç±»è¦æ±‚ï¼š
1. å¿…é¡»ä¸¥æ ¼ä»ä¸Šè¿°æ–‡ä»¶å¤¹è·¯å¾„åˆ—è¡¨ä¸­å¤åˆ¶å®Œæ•´çš„è·¯å¾„
2. ä¸èƒ½ä¿®æ”¹ã€ç¼©å†™æˆ–æ·»åŠ ä»»ä½•å†…å®¹åˆ°è·¯å¾„
3. ä¸èƒ½åˆ›å»ºæˆ–æƒ³è±¡ä¸å­˜åœ¨çš„æ–‡ä»¶å¤¹
4. ä¼˜å…ˆé€‰æ‹©æœ€å…·ä½“çš„åˆ†ç±»ï¼ˆè·¯å¾„è¶Šæ·±è¶Šå…·ä½“ï¼‰
5. æŒ‰åŒ¹é…åº¦ä»é«˜åˆ°ä½è¿”å›å‰ä¸‰ä¸ªè·¯å¾„
6. æ¯è¡Œä¸€ä¸ªè·¯å¾„ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–å†…å®¹
7. ä»”ç»†åˆ†ææ–‡ä»¶åå…³é”®è¯ï¼Œé€‰æ‹©æœ€åŒ¹é…çš„æ–‡ä»¶å¤¹

è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼æŒ‰æ­¤æ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªå®Œæ•´è·¯å¾„ï¼‰ï¼š
ç¬¬ä¸€æ¨èï¼š[å®Œæ•´è·¯å¾„1]
ç¬¬äºŒæ¨èï¼š[å®Œæ•´è·¯å¾„2]
ç¬¬ä¸‰æ¨èï¼š[å®Œæ•´è·¯å¾„3]

è¯·å¼€å§‹æ¨èï¼š"""
            
            # åœ¨ä¼ é€’ç»™å¤§æ¨¡å‹çš„å®Œæ•´å†…å®¹æœ€å°¾éƒ¨æ·»åŠ /no_thinkæ ‡ç­¾
            final_prompt = prompt + "\n\n/no_think"
            
            print(f"ğŸ¤– å‘é€ç»™AIçš„æç¤ºè¯:\n{final_prompt}")
            
            # ä½¿ç”¨ç³»ç»Ÿæç¤ºè¯æ¥æŠ‘åˆ¶æ€è€ƒè¿‡ç¨‹
            messages = [
                {
                    'role': 'system',
                    'content': 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡ä»¶åˆ†ç±»ä¸“å®¶ã€‚é‡è¦ï¼šä¸è¦è¾“å‡ºä»»ä½•æ¨ç†è¿‡ç¨‹ã€æ€è€ƒæ­¥éª¤æˆ–è§£é‡Šã€‚ç›´æ¥æŒ‰è¦æ±‚è¾“å‡ºç»“æœã€‚åªè¾“å‡ºå®Œæ•´è·¯å¾„ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–ä¿¡æ¯ã€‚ä¼˜å…ˆé€‰æ‹©æœ€å…·ä½“çš„åˆ†ç±»ï¼ˆè·¯å¾„è¶Šæ·±è¶Šå…·ä½“ï¼‰ã€‚'
                },
                {
                    'role': 'user',
                    'content': final_prompt
                }
            ]
            
            # ä¼˜å…ˆä½¿ç”¨qwen-longå®¢æˆ·ç«¯ï¼Œå¦‚æœå¤±è´¥åˆ™å›é€€åˆ°ollamaå®¢æˆ·ç«¯
            if self.qwen_long_client:
                try:
                    result = self.qwen_long_client.chat_with_retry(messages)
                    print(f"ğŸ¤– Qwen-Long AIåŸå§‹è¿”å›ç»“æœ: {result}")
                except Exception as e:
                    logging.warning(f"Qwen-Longæ¨¡å‹åˆ†ç±»å¤±è´¥ï¼Œå›é€€åˆ°Ollama: {e}")
                    if self.ollama_client:
                        result = self.ollama_client.chat_with_retry(messages)
                        print(f"ğŸ¤– Ollama AIåŸå§‹è¿”å›ç»“æœ: {result}")
                    else:
                        raise e
            else:
                result = self.ollama_client.chat_with_retry(messages)
                print(f"ğŸ¤– Ollama AIåŸå§‹è¿”å›ç»“æœ: {result}")
            
            # æ¸…ç†ç»“æœä¸­çš„æ€è€ƒè¿‡ç¨‹ï¼ˆç°åœ¨åœ¨_parse_classification_resultä¸­ç»Ÿä¸€å¤„ç†ï¼‰
            result = result.strip()
            
            # å¦‚æœç»“æœä»ç„¶ä»¥æ€è€ƒè¿‡ç¨‹å¼€å¤´ï¼Œå°è¯•æ›´æ¿€è¿›çš„æ¸…ç†
            if any(keyword in result.lower() for keyword in ['å¥½ï¼Œ', 'å—¯ï¼Œ', 'æˆ‘æ¥', 'æˆ‘éœ€è¦', 'é¦–å…ˆï¼Œ', 'è®©æˆ‘']):
                # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå®Œæ•´çš„å¥å­
                sentences = result.split('ã€‚')
                if len(sentences) > 1:
                    # è·³è¿‡ç¬¬ä¸€ä¸ªå¥å­ï¼ˆé€šå¸¸æ˜¯æ€è€ƒè¿‡ç¨‹ï¼‰ï¼Œä½¿ç”¨ç¬¬äºŒä¸ªå¥å­å¼€å§‹
                    result = 'ã€‚'.join(sentences[1:]).strip()
            
            # è§£æAIåˆ†ç±»ç»“æœ
            recommended_folder, match_reason = self._parse_classification_result(result, target_directory, content, summary)
            
            # æ£€æŸ¥åˆ†ç±»è´¨é‡ï¼šå¦‚æœæ¨èçš„æ˜¯è¿‡äºå®½æ³›çš„åˆ†ç±»ï¼Œå°è¯•é‡æ–°åˆ†ç±»
            if recommended_folder and retry_count < 2:
                # æ£€æŸ¥è·¯å¾„æ·±åº¦ï¼Œå¦‚æœå¤ªæµ…å¯èƒ½æ˜¯è¿‡äºå®½æ³›çš„åˆ†ç±»
                path_depth = len([p for p in recommended_folder.split('\\') + recommended_folder.split('/') if p.strip()])
                if path_depth <= 2:  # è·¯å¾„æ·±åº¦å°äºç­‰äº2å¯èƒ½æ˜¯è¿‡äºå®½æ³›
                    print(f"âš ï¸  AIæ¨èäº†è¿‡äºå®½æ³›çš„åˆ†ç±»: {recommended_folder}ï¼Œå‡†å¤‡ç¬¬ {retry_count + 1} æ¬¡é‡è¯•...")
                    logging.warning(f"AIæ¨èäº†è¿‡äºå®½æ³›çš„åˆ†ç±»: {recommended_folder}ï¼Œå‡†å¤‡ç¬¬ {retry_count + 1} æ¬¡é‡è¯•")
                    
                    # åœ¨é‡è¯•æ—¶å¼ºè°ƒè¦é€‰æ‹©æ›´å…·ä½“çš„åˆ†ç±»
                    return self._recommend_target_folder(file_path, content, summary, target_directory, retry_count + 1)
            
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…å¤±è´¥ï¼Œå¦‚æœæ˜¯ä¸”æœªè¶…è¿‡é‡è¯•æ¬¡æ•°ï¼Œåˆ™é‡è¯•
            if recommended_folder is None and retry_count < 2:  # æœ€å¤šé‡è¯•2æ¬¡
                print(f"âš ï¸  AIåˆ†ç±»å¤±è´¥ï¼Œå‡†å¤‡ç¬¬ {retry_count + 1} æ¬¡é‡è¯•...")
                logging.warning(f"AIåˆ†ç±»å¤±è´¥ï¼Œå‡†å¤‡ç¬¬ {retry_count + 1} æ¬¡é‡è¯•")
                
                # é€’å½’è°ƒç”¨è‡ªèº«è¿›è¡Œé‡è¯•
                return self._recommend_target_folder(file_path, content, summary, target_directory, retry_count + 1)
            
            return recommended_folder, match_reason
            
        except Exception as e:
            logging.error(f"æ¨èç›®æ ‡æ–‡ä»¶å¤¹å¤±è´¥: {e}")
            return None, f"æ¨èå¤±è´¥: {str(e)}"
    def _extract_recommended_paths(self, ai_result: str) -> List[str]:
        """
        ä»AIè¿”å›ç»“æœä¸­æå–ä¸‰ä¸ªæ¨èè·¯å¾„
        
        Args:
            ai_result: AIè¿”å›çš„åŸå§‹ç»“æœ
            
        Returns:
            List[str]: æå–åˆ°çš„æ¨èè·¯å¾„åˆ—è¡¨
        """
        recommended_paths = []
        
        try:
            lines = ai_result.strip().split('\n')
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                    
                # æŸ¥æ‰¾åŒ…å«æ¨èè·¯å¾„çš„è¡Œ
                if any(keyword in line for keyword in ['ç¬¬ä¸€æ¨èï¼š', 'ç¬¬äºŒæ¨èï¼š', 'ç¬¬ä¸‰æ¨èï¼š', 'æ¨èï¼š']):
                    # æå–å†’å·åçš„è·¯å¾„
                    if 'ï¼š' in line:
                        path = line.split('ï¼š', 1)[1].strip()
                        if path and path not in recommended_paths:
                            recommended_paths.append(path)
                elif line.startswith('ã€') and 'ã€‘' in line:
                    # ç›´æ¥è¯†åˆ«ä»¥ã€å¼€å¤´çš„è·¯å¾„æ ¼å¼
                    if line not in recommended_paths:
                        recommended_paths.append(line)
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ¼å¼åŒ–çš„æ¨èï¼Œå°è¯•æŒ‰è¡Œæå–å‰ä¸‰ä¸ªæœ‰æ•ˆè·¯å¾„
            if not recommended_paths:
                for line in lines:
                    line = line.strip()
                    if line and line.startswith('ã€') and 'ã€‘' in line:
                        if line not in recommended_paths:
                            recommended_paths.append(line)
                        if len(recommended_paths) >= 3:
                            break
            
            # é™åˆ¶æœ€å¤šè¿”å›3ä¸ªè·¯å¾„
            return recommended_paths[:3]
            
        except Exception as e:
            logging.error(f"æå–æ¨èè·¯å¾„æ—¶å‡ºé”™: {e}")
            return []
    
    def _parse_classification_result(self, result: str, target_directory: str, file_content: str = None, file_summary: str = None) -> tuple:
        """
        è§£æAIåˆ†ç±»ç»“æœï¼Œæ”¯æŒç›¸å…³æ€§è¯„åˆ†æ’åºå’Œä¸‰ä¸ªæ¨èè·¯å¾„çš„ä¾æ¬¡éªŒè¯
        
        Args:
            result: AIè¿”å›çš„åˆ†ç±»ç»“æœ
            target_directory: ç›®æ ‡ç›®å½•è·¯å¾„
            file_content: æ–‡ä»¶å†…å®¹ï¼ˆç”¨äºç›¸å…³æ€§è®¡ç®—ï¼‰
            file_summary: æ–‡ä»¶æ‘˜è¦ï¼ˆç”¨äºç›¸å…³æ€§è®¡ç®—ï¼‰
            
        Returns:
            tuple: (æ¨èæ–‡ä»¶å¤¹è·¯å¾„, åŒ¹é…ç†ç”±)
        """
        try:
            result = result.strip()
            logging.info(f"æ­£åœ¨è§£æAIåˆ†ç±»ç»“æœ: {result[:100]}...")
            
            print(f"ğŸ” è§£æAIåˆ†ç±»ç»“æœ: {result}")
            
            # æ¸…ç†AIè¿”å›ç»“æœä¸­çš„æ€è€ƒè¿‡ç¨‹æ ‡ç­¾å’Œå†…å®¹
            result_clean = self._clean_ai_response(result)
            logging.info(f"æ¸…ç†åçš„åˆ†ç±»ç»“æœ: {result_clean[:100]}...")
            print(f"ğŸ§¹ æ¸…ç†åçš„ç»“æœ: {result_clean}")
            
            # æå–ä¸‰ä¸ªæ¨èè·¯å¾„
            recommended_paths = self._extract_recommended_paths(result_clean)
            
            if not recommended_paths:
                logging.warning(f"æ— æ³•ä»AIç»“æœä¸­æå–æ¨èè·¯å¾„")
                print(f"âš ï¸ æ— æ³•æå–æ¨èè·¯å¾„")
                return None, f"AIæ™ºèƒ½åˆ†ç±»å¤±è´¥ï¼Œæ— æ³•è§£ææ¨èè·¯å¾„: {result_clean[:50]}..."
            
            print(f"ğŸ“‹ æå–åˆ° {len(recommended_paths)} ä¸ªæ¨èè·¯å¾„: {recommended_paths}")
            
            # å¦‚æœæœ‰æ–‡ä»¶å†…å®¹å’Œæ‘˜è¦ï¼Œè¿›è¡Œç›¸å…³æ€§åˆ†æå’Œæ’åº
            if file_content and file_summary:
                print(f"ğŸ” å¼€å§‹ç›¸å…³æ€§åˆ†æå’Œæ’åº...")
                sorted_paths = self._filter_irrelevant_folders(file_content, file_summary, recommended_paths, target_directory)
                
                if sorted_paths:
                    recommended_paths = sorted_paths
                    print(f"âœ… ç›¸å…³æ€§æ’åºå®Œæˆï¼ŒæŒ‰è¯„åˆ†æ’åºåçš„è·¯å¾„: {sorted_paths}")
                else:
                    print(f"âš ï¸ ç›¸å…³æ€§æ’åºåæ— ç›¸å…³è·¯å¾„ï¼Œä½¿ç”¨åŸå§‹æ¨è")
            
            # ä¾æ¬¡éªŒè¯æ¯ä¸ªæ¨èè·¯å¾„ï¼ˆç°åœ¨æŒ‰ç›¸å…³æ€§è¯„åˆ†æ’åºï¼‰
            for i, recommended_path in enumerate(recommended_paths, 1):
                target_path = Path(target_directory) / recommended_path
                
                print(f"ğŸ” éªŒè¯ç¬¬{i}ä¸ªæ¨èè·¯å¾„: {recommended_path}")
                print(f"ğŸ¯ å®Œæ•´ç›®æ ‡è·¯å¾„: {target_path}")
                
                # éªŒè¯è·¯å¾„æ˜¯å¦å­˜åœ¨ä¸”ä¸ºç›®å½•
                if target_path.exists() and target_path.is_dir():
                    logging.info(f"æ‰¾åˆ°æœ‰æ•ˆçš„ç›®æ ‡æ–‡ä»¶å¤¹(ç¬¬{i}é€‰æ‹©): {recommended_path}")
                    print(f"âœ… æ‰¾åˆ°æœ‰æ•ˆç›®æ ‡æ–‡ä»¶å¤¹(ç¬¬{i}é€‰æ‹©): {recommended_path}")
                    
                    # å­˜å‚¨æ¨èçš„å®Œæ•´è·¯å¾„
                    self.recommended_folder_path = recommended_path
                    print(f"ğŸ’¾ å­˜å‚¨æ¨èè·¯å¾„: {self.recommended_folder_path}")
                    
                    # ç”Ÿæˆæœ‰æ„ä¹‰çš„åŒ¹é…ç†ç”±
                    match_reason = f"AIæ™ºèƒ½åˆ†ç±»(ç¬¬{i}é€‰æ‹©)ï¼šæ ¹æ®æ–‡ä»¶å†…å®¹åŒ¹é…åˆ° {recommended_path}"
                    
                    return recommended_path, match_reason
                else:
                    logging.warning(f"ç¬¬{i}ä¸ªæ¨èè·¯å¾„ä¸å­˜åœ¨: {target_path}")
                    print(f"âŒ ç¬¬{i}ä¸ªæ¨èè·¯å¾„ä¸å­˜åœ¨: {recommended_path}")
            
            # å¦‚æœæ‰€æœ‰æ¨èè·¯å¾„éƒ½æ— æ•ˆ
            logging.error(f"æ‰€æœ‰æ¨èè·¯å¾„éƒ½æ— æ•ˆ: {recommended_paths}")
            print(f"ğŸ’¥ æ‰€æœ‰æ¨èè·¯å¾„éƒ½æ— æ•ˆ")
            
            return None, f"AIæ™ºèƒ½åˆ†ç±»å¤±è´¥ï¼Œä¸‰ä¸ªæ¨èè·¯å¾„éƒ½ä¸å­˜åœ¨: {', '.join(recommended_paths[:3])}"
            
        except Exception as e:
            logging.error(f"è§£æåˆ†ç±»ç»“æœå¤±è´¥: {e}, åŸå§‹ç»“æœ: {result}")
            return None, f"è§£æåˆ†ç±»ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
    def scan_source_files(self, source_directory: str) -> List[str]:
        try:
            source_path = Path(source_directory)
            if not source_path.exists():
                raise FileOrganizerError(f"æºç›®å½•ä¸å­˜åœ¨: {source_directory}")
            files = []
            for item in source_path.rglob('*'):
                if item.is_file():
                    files.append(str(item))
            logging.info(f"æ‰«æåˆ° {len(files)} ä¸ªå¾…æ•´ç†æ–‡ä»¶")
            return files
        except Exception as e:
            raise FileOrganizerError(f"æ‰«ææºæ–‡ä»¶å¤±è´¥: {e}")
    def scan_files(self, source_directory: str) -> List[Dict[str, object]]:
        try:
            source_path = Path(source_directory)
            if not source_path.exists():
                raise FileOrganizerError(f"æºç›®å½•ä¸å­˜åœ¨: {source_directory}")
            files = []
            for item in source_path.rglob('*'):
                if item.is_file():
                    files.append({
                        'name': item.name,
                        'path': str(item),
                        'size': item.stat().st_size,
                        'extension': item.suffix.lower()
                    })
            logging.info(f"æ‰«æåˆ° {len(files)} ä¸ªå¾…æ•´ç†æ–‡ä»¶")
            return files
        except Exception as e:
            raise FileOrganizerError(f"æ‰«ææºæ–‡ä»¶å¤±è´¥: {e}")

    def preview_classification(self, source_directory: str, target_directory: str) -> List[Dict[str, object]]:
        try:
            source_files = self.scan_source_files(source_directory)
            if not source_files:
                raise FileOrganizerError("æºç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶")
            preview_results = []
            logging.info(f"å¼€å§‹é¢„è§ˆåˆ†ç±»ï¼Œå…± {len(source_files)} ä¸ªæ–‡ä»¶")
            preview_count = len(source_files)
            for i, file_path in enumerate(source_files, 1):
                file_name = Path(file_path).name
                try:
                    logging.info(f"æ­£åœ¨å¤„ç†æ–‡ä»¶ {i}/{preview_count}: {file_name}")
                    print(f"\n=== å¤„ç†æ–‡ä»¶ {i}/{preview_count}: {file_name} ===")
                    # ä½¿ç”¨æ–°çš„åˆ†ææ–¹æ³•
                    analysis_result = self.analyze_and_classify_file(file_path, target_directory)
                    
                    if analysis_result['success']:
                        timing_info = analysis_result.get('timing_info', {})
                        preview_results.append({
                            'file_path': file_path,
                            'file_name': file_name,
                            'target_folder': analysis_result['recommended_folder'],
                            'match_reason': analysis_result['match_reason'],
                            'classification_method': 'AI_Enhanced',
                            'success': True,
                            'extracted_content': analysis_result['extracted_content'][:200] + "..." if len(analysis_result['extracted_content']) > 200 else analysis_result['extracted_content'],
                            'content_summary': analysis_result['content_summary'],
                            'timing_info': timing_info
                        })
                        total_time = timing_info.get('total_processing_time', 0)
                        extract_time = timing_info.get('content_extraction_time', 0)
                        summary_time = timing_info.get('summary_generation_time', 0)
                        recommend_time = timing_info.get('folder_recommendation_time', 0)
                        
                        logging.info(f"æ–‡ä»¶ {file_name} åˆ†ææˆåŠŸ: {analysis_result['recommended_folder']} ({analysis_result['match_reason']})ï¼Œæ€»è€—æ—¶: {total_time}ç§’")
                        logging.info(f"å†…å®¹æ‘˜è¦: {analysis_result['content_summary']}")
                        logging.info(f"è¯¦ç»†è€—æ—¶ - å†…å®¹æå–: {extract_time}ç§’, æ‘˜è¦ç”Ÿæˆ: {summary_time}ç§’, ç›®å½•æ¨è: {recommend_time}ç§’")
                    else:
                        timing_info = analysis_result.get('timing_info', {})
                        preview_results.append({
                            'file_path': file_path,
                            'file_name': file_name,
                            'target_folder': None,
                            'match_reason': analysis_result['match_reason'],
                            'classification_method': 'Failed',
                            'success': False,
                            'error': analysis_result.get('error', analysis_result['match_reason']),
                            'extracted_content': analysis_result.get('extracted_content', ''),
                            'content_summary': analysis_result.get('content_summary', ''),
                            'timing_info': timing_info
                        })
                        total_time = timing_info.get('total_processing_time', 0)
                        logging.warning(f"æ–‡ä»¶ {file_name} åˆ†æå¤±è´¥: {analysis_result['match_reason']}ï¼Œæ€»è€—æ—¶: {total_time}ç§’")
                except Exception as e:
                    error_msg = f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}"
                    logging.error(f"æ–‡ä»¶ {file_name} å¤„ç†å¼‚å¸¸: {e}")
                    preview_results.append({
                        'file_path': file_path,
                        'file_name': file_name,
                        'target_folder': None,
                        'match_reason': error_msg,
                        'classification_method': 'Error',
                        'success': False,
                        'error': error_msg
                    })
            success_count = sum(1 for result in preview_results if result['success'])
            logging.info(f"é¢„è§ˆåˆ†ç±»å®Œæˆï¼ŒæˆåŠŸåˆ†ç±» {success_count}/{len(preview_results)} ä¸ªæ–‡ä»¶")
            
            # ç”Ÿæˆé¢„è§ˆç»“æœJSONæ–‡ä»¶ï¼ˆæ ¼å¼ä¸preview_ai_result.jsonä¿æŒä¸€è‡´ï¼‰
            try:
                ai_preview_results = []
                for result in preview_results:
                    if result['success']:
                        ai_preview_result = {
                            "æºæ–‡ä»¶è·¯å¾„": result['file_path'],
                            "æ–‡ä»¶æ‘˜è¦": result['content_summary'],
                            "æœ€åŒ¹é…çš„ç›®æ ‡ç›®å½•": result['target_folder'],
                            "åŒ¹é…ç†ç”±": result['match_reason'],
                            "å¤„ç†è€—æ—¶ä¿¡æ¯": {
                                "æ€»è€—æ—¶(ç§’)": result['timing_info'].get('total_processing_time', 0),
                                "å†…å®¹æå–è€—æ—¶(ç§’)": result['timing_info'].get('content_extraction_time', 0),
                                "æ‘˜è¦ç”Ÿæˆè€—æ—¶(ç§’)": result['timing_info'].get('summary_generation_time', 0),
                                "ç›®å½•æ¨èè€—æ—¶(ç§’)": result['timing_info'].get('folder_recommendation_time', 0)
                            }
                        }
                        ai_preview_results.append(ai_preview_result)
                
                # ä¿å­˜é¢„è§ˆç»“æœåˆ°JSONæ–‡ä»¶
                preview_result_file = 'preview_ai_result.json'
                with open(preview_result_file, 'w', encoding='utf-8') as f:
                    json.dump(ai_preview_results, f, ensure_ascii=False, indent=2)
                logging.info(f"AIé¢„è§ˆç»“æœå·²ä¿å­˜åˆ°: {preview_result_file}")
                
            except Exception as e:
                logging.warning(f"ç”Ÿæˆé¢„è§ˆç»“æœæ–‡ä»¶å¤±è´¥: {e}")
            
            return preview_results
        except Exception as e:
            raise FileOrganizerError(f"é¢„è§ˆåˆ†ç±»å¤±è´¥: {e}")
    def organize_file(self, file_path: str, target_directory: str) -> Tuple[bool, str]:
        try:
            if not self.ollama_client and not self.qwen_long_client:
                self.initialize_ollama()
            
            file_path_obj = Path(file_path)
            filename = file_path_obj.name
            source_file_full_path = str(file_path_obj.absolute())
            
            print(f"ğŸ“„ æºæ–‡ä»¶å®Œæ•´è·¯å¾„: {source_file_full_path}")
            
            # ä½¿ç”¨AIåˆ†ç±»è·å–æ¨èæ–‡ä»¶å¤¹
            target_folder, match_reason, success = self.classify_file(str(file_path_obj), target_directory)
            if not success or not target_folder:
                return False, "æ— æ³•ç¡®å®šç›®æ ‡æ–‡ä»¶å¤¹"
            
            # ä½¿ç”¨å­˜å‚¨çš„æ¨èè·¯å¾„å˜é‡
            if hasattr(self, 'recommended_folder_path') and self.recommended_folder_path:
                target_folder_full_path = Path(target_directory) / self.recommended_folder_path
                print(f"ğŸ”§ ä½¿ç”¨å­˜å‚¨çš„æ¨èè·¯å¾„: {self.recommended_folder_path}")
                print(f"ğŸ¯ ç›®æ ‡æ–‡ä»¶å¤¹å®Œæ•´è·¯å¾„: {target_folder_full_path}")
            else:
                target_folder_full_path = Path(target_directory) / target_folder
                print(f"âš ï¸  ä½¿ç”¨é»˜è®¤è·¯å¾„: {target_folder}")
                print(f"ğŸ¯ ç›®æ ‡æ–‡ä»¶å¤¹å®Œæ•´è·¯å¾„: {target_folder_full_path}")
            
            # æ„å»ºå®Œæ•´çš„è¿ç§»å‘½ä»¤ä¿¡æ¯
            migration_info = {
                'source_file': source_file_full_path,
                'target_folder': str(target_folder_full_path),
                'filename': filename,
                'match_reason': match_reason
            }
            
            print(f"ğŸ“‹ è¿ç§»ä¿¡æ¯: {migration_info}")
            
            # æ£€æŸ¥å¹¶åˆ›å»ºå¹´ä»½å­æ–‡ä»¶å¤¹
            final_target_folder = self._check_and_create_year_folder(target_folder_full_path, filename)
            
            # ç¡®ä¿ç›®æ ‡æ–‡ä»¶å¤¹å­˜åœ¨
            final_target_folder.mkdir(parents=True, exist_ok=True)
            
            # æ„å»ºç›®æ ‡æ–‡ä»¶è·¯å¾„
            target_file_path = final_target_folder / filename
            if target_file_path.exists():
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                name_parts = filename.rsplit('.', 1)
                if len(name_parts) == 2:
                    new_filename = f"{name_parts[0]}_{timestamp}.{name_parts[1]}"
                else:
                    new_filename = f"{filename}_{timestamp}"
                target_file_path = final_target_folder / new_filename
                print(f"âš ï¸  æ–‡ä»¶åå†²çªï¼Œé‡å‘½åä¸º: {new_filename}")
            
            # æ‰§è¡Œæ–‡ä»¶å¤åˆ¶
            shutil.copy2(source_file_full_path, str(target_file_path))
            
            # è®°å½•è¿ç§»æ—¥å¿—
            migration_log = f"æ–‡ä»¶è¿ç§»å®Œæˆ: {source_file_full_path} -> {target_file_path}"
            logging.info(migration_log)
            print(f"âœ… {migration_log}")
            
            return True, str(target_file_path)
        except Exception as e:
            error_msg = f"æ•´ç†æ–‡ä»¶å¤±è´¥: {e}"
            logging.error(error_msg)
            return False, error_msg
    def organize_files(self, files=None, target_base_dir=None, copy_mode=True, source_directory=None, target_directory=None, dry_run=False, progress_callback=None) -> Dict[str, object]:
        try:
            if source_directory and target_directory:
                files = self.scan_files(source_directory)
                target_base_dir = target_directory
                copy_mode = True
            if not files or not target_base_dir:
                raise FileOrganizerError("ç¼ºå°‘å¿…è¦å‚æ•°")
            log_session_name = None
            if self.enable_transfer_log and self.transfer_log_manager and not dry_run:
                try:
                    session_name = f"organize_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                    log_session_name = self.transfer_log_manager.start_transfer_session(session_name)
                    logging.info(f"å¼€å§‹è½¬ç§»æ—¥å¿—ä¼šè¯: {session_name}")
                except Exception as e:
                    logging.warning(f"å¯åŠ¨è½¬ç§»æ—¥å¿—ä¼šè¯å¤±è´¥: {e}")
            if not files:
                raise FileOrganizerError("æ²¡æœ‰æ–‡ä»¶éœ€è¦æ•´ç†")
            results = {
                'total_files': len(files),
                'processed_files': 0,
                'successful_moves': 0,
                'failed_moves': 0,
                'skipped_files': 0,
                'success': [],
                'failed': [],
                'errors': [],
                'move_details': [],
                'ai_responses': [],
                'start_time': datetime.now(),
                'end_time': None,
                'transfer_log_file': log_session_name,
                'dry_run': dry_run
            }
            
            # åˆå§‹åŒ–AIç»“æœæ–‡ä»¶
            ai_result_file = 'ai_organize_result.json'
            # ä¸å†åœ¨å†…å­˜ä¸­ä¿å­˜å®Œæ•´çš„ai_resultsåˆ—è¡¨ï¼Œåªä¿å­˜å¿…è¦çš„è¿ç§»ä¿¡æ¯
            migration_queue = []  # åªä¿å­˜æºè·¯å¾„å’Œç›®æ ‡è·¯å¾„çš„ç®€å•ä¿¡æ¯
            
            # ç¡®ä¿AIç»“æœæ–‡ä»¶å­˜åœ¨ï¼Œä½†ä¸æ¸…ç©ºç°æœ‰å†…å®¹
            if not os.path.exists(ai_result_file):
                with open(ai_result_file, 'w', encoding='utf-8') as f:
                    json.dump([], f, ensure_ascii=False, indent=2)
                logging.info(f"åˆ›å»ºæ–°çš„AIç»“æœæ–‡ä»¶: {ai_result_file}")
            else:
                logging.info(f"AIç»“æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œå°†è¿½åŠ æ–°è®°å½•: {ai_result_file}")
            logging.info(f"å¼€å§‹å®‰å…¨æ–‡ä»¶æ•´ç†ï¼Œå…± {len(files)} ä¸ªæ–‡ä»¶")
            print(f"\n=== å¼€å§‹AIæ™ºèƒ½æ–‡ä»¶æ•´ç† ===")
            print(f"æºç›®å½•: {source_directory if source_directory else 'æŒ‡å®šæ–‡ä»¶åˆ—è¡¨'}")
            print(f"ç›®æ ‡ç›®å½•: {target_base_dir}")
            print(f"å¾…å¤„ç†æ–‡ä»¶æ€»æ•°: {len(files)}")
            print(f"æ“ä½œæ¨¡å¼: {'å¤åˆ¶' if copy_mode else 'ç§»åŠ¨'}")
            print("=" * 50)
            
            for i, file_info in enumerate(files, 1):
                file_path = str(file_info['path'])
                filename = str(file_info['name'])
                
                # è°ƒç”¨è¿›åº¦å›è°ƒ
                if progress_callback:
                    progress_callback(i, len(files), filename)
                
                # æ§åˆ¶å°è¾“å‡ºå½“å‰å¤„ç†è¿›åº¦
                print(f"\n[{i}/{len(files)}] æ­£åœ¨å¤„ç†: {filename}")
                
                try:
                    logging.info(f"æ­£åœ¨å¤„ç†æ–‡ä»¶ {i}/{len(files)}: {filename}")
                    print(f"  ğŸ” æ­£åœ¨åˆ†ææ–‡ä»¶å†…å®¹...", end="", flush=True)
                    
                    # è·å–è¯¦ç»†çš„åˆ†æç»“æœï¼ˆåªè°ƒç”¨ä¸€æ¬¡ï¼‰
                    analysis_result = self.analyze_and_classify_file(file_path, target_base_dir)
                    target_folder = analysis_result.get('recommended_folder')
                    match_reason = analysis_result.get('match_reason', '')
                    success = analysis_result.get('success', False)
                    
                    results['ai_responses'].append({
                        'file_name': filename,
                        'target_folder': target_folder,
                        'match_reason': match_reason,
                        'success': success
                    })
                    
                    # æ„å»ºAIç»“æœé¡¹ï¼ˆåŸºç¡€ä¿¡æ¯ï¼‰
                    ai_result_item = {
                        "å¤„ç†æ—¶é—´": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        "æ–‡ä»¶å": filename,
                        "æºæ–‡ä»¶è·¯å¾„": file_path,
                        "æ–‡ä»¶æ‘˜è¦": analysis_result.get('content_summary', ''),
                        "æœ€åŒ¹é…çš„ç›®æ ‡ç›®å½•": analysis_result.get('recommended_folder', ''),
                        "åŒ¹é…ç†ç”±": analysis_result.get('match_reason', ''),
                        "å¤„ç†è€—æ—¶ä¿¡æ¯": {
                            "æ€»è€—æ—¶(ç§’)": analysis_result.get('timing_info', {}).get('total_processing_time', 0),
                            "å†…å®¹æå–è€—æ—¶(ç§’)": analysis_result.get('timing_info', {}).get('content_extraction_time', 0),
                            "æ‘˜è¦ç”Ÿæˆè€—æ—¶(ç§’)": analysis_result.get('timing_info', {}).get('summary_generation_time', 0),
                            "ç›®å½•æ¨èè€—æ—¶(ç§’)": analysis_result.get('timing_info', {}).get('folder_recommendation_time', 0)
                        }
                    }
                    
                    # ä¿å­˜åˆ°è¿ç§»é˜Ÿåˆ—ï¼Œç­‰å¾…è¿ç§»æˆåŠŸåå†™å…¥å®Œæ•´ä¿¡æ¯
                    migration_queue.append({
                        'source_path': file_path,
                        'target_folder': target_folder,
                        'filename': filename,
                        'match_reason': match_reason,
                        'ai_result_item': ai_result_item
                    })
                    
                    if not success or not target_folder:
                        error_msg = f"æ–‡ä»¶ {filename} åˆ†ç±»å¤±è´¥: {match_reason}ï¼Œå·²è·³è¿‡ï¼Œæœªåšä»»ä½•å¤„ç†"
                        print(f"\r  âŒ åˆ†ç±»å¤±è´¥: {match_reason}")
                        logging.warning(error_msg)
                        results['errors'].append(error_msg)
                        results['skipped_files'] += 1
                        results['failed'].append({
                            'source_path': file_path,
                            'error': error_msg
                        })
                        
                        # æ›´æ–°è¿ç§»é˜Ÿåˆ—ä¸­æœ€åä¸€é¡¹çš„å¤±è´¥çŠ¶æ€å¹¶ç«‹å³å†™å…¥
                        if migration_queue:
                            migration_queue[-1]['ai_result_item'].update({
                                "å¤„ç†çŠ¶æ€": "åˆ†ç±»å¤±è´¥",
                                "é”™è¯¯ä¿¡æ¯": match_reason
                            })
                            # ç«‹å³å†™å…¥å¤±è´¥è®°å½•
                            self._append_ai_result_to_file(ai_result_file, migration_queue[-1]['ai_result_item'])
                        
                        continue
                    
                    print(f"\r  âœ… æ¨èç›®å½•: {target_folder}")
                    print(f"     ç†ç”±: {match_reason}")
                    target_folder_path = Path(target_base_dir) / target_folder
                    if not target_folder_path.exists():
                        error_msg = f"ç›®æ ‡æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {target_folder}"
                        logging.error(error_msg)
                        results['errors'].append(error_msg)
                        results['failed_moves'] += 1
                        results['failed'].append({
                            'source_path': file_path,
                            'error': error_msg
                        })
                        continue
                    original_target_path = target_folder_path / filename
                    target_file_path = original_target_path
                    if target_file_path.exists():
                        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                        name_parts = filename.rsplit('.', 1)
                        if len(name_parts) == 2:
                            new_filename = f"{name_parts[0]}_{timestamp}.{name_parts[1]}"
                        else:
                            new_filename = f"{filename}_{timestamp}"
                        target_file_path = target_folder_path / new_filename
                        logging.info(f"æ–‡ä»¶åå†²çªï¼Œé‡å‘½åä¸º: {target_file_path.name}")
                    if not dry_run:
                        try:
                            if not Path(file_path).exists():
                                error_msg = f"æºæ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
                                logging.error(error_msg)
                                results['errors'].append(error_msg)
                                results['failed_moves'] += 1
                                results['failed'].append({
                                    'source_path': file_path,
                                    'error': error_msg
                                })
                                continue
                            if copy_mode:
                                shutil.copy2(file_path, str(target_file_path))
                                operation = "copy"
                                operation_cn = "å¤åˆ¶"
                            else:
                                shutil.move(file_path, str(target_file_path))
                                operation = "move"
                                operation_cn = "ç§»åŠ¨"
                            if target_file_path.exists():
                                if not copy_mode and Path(file_path).exists():
                                    error_msg = f"æ–‡ä»¶ç§»åŠ¨éªŒè¯å¤±è´¥: {filename}"
                                    logging.error(error_msg)
                                    results['errors'].append(error_msg)
                                    results['failed_moves'] += 1
                                    results['failed'].append({
                                        'source_path': file_path,
                                        'error': error_msg
                                    })
                                    continue
                                print(f"     âœ… {operation_cn}æˆåŠŸ: {filename} -> {target_folder}")
                                logging.info(f"æ–‡ä»¶å®‰å…¨{operation_cn}æˆåŠŸ: {filename} -> {target_folder} ({match_reason})")
                                results['successful_moves'] += 1
                                
                                # æ›´æ–°è¿ç§»é˜Ÿåˆ—ä¸­å¯¹åº”é¡¹çš„æœ€ç»ˆè·¯å¾„å¹¶ç«‹å³å†™å…¥
                                for queue_item in migration_queue:
                                    if queue_item['source_path'] == file_path:
                                        queue_item['ai_result_item'].update({
                                            "æœ€ç»ˆç›®æ ‡è·¯å¾„": str(target_file_path),
                                            "æ“ä½œç±»å‹": operation_cn,
                                            "å¤„ç†çŠ¶æ€": "æˆåŠŸ"
                                        })
                                        # ç«‹å³å†™å…¥æˆåŠŸè®°å½•
                                        self._append_ai_result_to_file(ai_result_file, queue_item['ai_result_item'])
                                        break
                            else:
                                error_msg = f"æ–‡ä»¶{operation_cn}éªŒè¯å¤±è´¥: {filename}"
                                logging.error(error_msg)
                                results['errors'].append(error_msg)
                                results['failed_moves'] += 1
                                results['failed'].append({
                                    'source_path': file_path,
                                    'error': error_msg
                                })
                                continue
                        except Exception as move_error:
                            error_msg = f"{operation_cn}æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {move_error}"
                            logging.error(error_msg)
                            results['errors'].append(error_msg)
                            results['failed_moves'] += 1
                            results['failed'].append({
                                'source_path': file_path,
                                'error': error_msg
                            })
                            continue
                    else:
                        operation = "copy" if copy_mode else "move"
                        operation_cn = "å¤åˆ¶" if copy_mode else "ç§»åŠ¨"
                        logging.info(f"[è¯•è¿è¡Œ] æ–‡ä»¶å°†{operation_cn}: {filename} -> {target_folder} ({match_reason})")
                        results['successful_moves'] += 1
                        
                        # è¯•è¿è¡Œæ¨¡å¼ä¸‹æ›´æ–°è¿ç§»é˜Ÿåˆ—ä¸­å¯¹åº”é¡¹å¹¶ç«‹å³å†™å…¥
                        for queue_item in migration_queue:
                            if queue_item['source_path'] == file_path:
                                queue_item['ai_result_item'].update({
                                    "æœ€ç»ˆç›®æ ‡è·¯å¾„": str(target_file_path),
                                    "æ“ä½œç±»å‹": operation_cn,
                                    "å¤„ç†çŠ¶æ€": "è¯•è¿è¡ŒæˆåŠŸ"
                                })
                                # ç«‹å³å†™å…¥è¯•è¿è¡Œè®°å½•
                                self._append_ai_result_to_file(ai_result_file, queue_item['ai_result_item'])
                                break
                    if self.enable_transfer_log and self.transfer_log_manager and not dry_run:
                        try:
                            file_size_raw = file_info.get('size', 0)
                            file_size = int(file_size_raw) if isinstance(file_size_raw, (int, float, str)) and str(file_size_raw).isdigit() else 0
                            self.transfer_log_manager.log_transfer_operation(
                                source_path=file_path,
                                target_path=str(target_file_path),
                                operation_type=operation,
                                target_folder=target_folder,
                                success=True,
                                file_size=file_size
                            )
                        except Exception as e:
                            logging.warning(f"è®°å½•è½¬ç§»æ—¥å¿—å¤±è´¥: {e}")
                    results['success'].append({
                        'source_path': file_path,
                        'target_path': str(target_file_path),
                        'target_folder': target_folder,
                        'operation': operation
                    })
                    results['move_details'].append({
                        'source_file': file_path,
                        'file_name': filename,
                        'target_folder': target_folder,
                        'target_path': str(target_file_path),
                        'match_reason': match_reason,
                        'classification_method': 'AI',
                        'renamed': str(target_file_path) != str(original_target_path),
                        'operation': operation
                    })
                except Exception as e:
                    error_msg = f"å¤„ç†æ–‡ä»¶ {filename} æ—¶å‡ºç°å¼‚å¸¸: {e}"
                    logging.error(error_msg)
                    results['errors'].append(error_msg)
                    results['failed_moves'] += 1
                    if self.enable_transfer_log and self.transfer_log_manager and not dry_run:
                        try:
                            operation_type = "copy" if copy_mode else "move"
                            self.transfer_log_manager.log_transfer_operation(
                                source_path=file_path,
                                target_path="",
                                operation_type=operation_type,
                                target_folder="",
                                success=False,
                                error_message=error_msg
                            )
                        except Exception as log_e:
                            logging.warning(f"è®°å½•å¤±è´¥è½¬ç§»æ—¥å¿—å¤±è´¥: {log_e}")
                    results['failed'].append({
                        'source_path': file_path,
                        'error': error_msg
                    })
                finally:
                    results['processed_files'] += 1
            results['end_time'] = datetime.now()
            
            # è¾“å‡ºå¤„ç†å®Œæˆæ€»ç»“
            print(f"\n=== æ–‡ä»¶æ•´ç†å®Œæˆ ===")
            print(f"æ€»æ–‡ä»¶æ•°: {results['total_files']}")
            print(f"æˆåŠŸå¤„ç†: {results['successful_moves']} ä¸ª")
            print(f"å¤„ç†å¤±è´¥: {results['failed_moves']} ä¸ª")
            print(f"è·³è¿‡æ–‡ä»¶: {results['skipped_files']} ä¸ª")
            duration = (results['end_time'] - results['start_time']).total_seconds()
            print(f"æ€»è€—æ—¶: {duration:.1f} ç§’")
            print("=" * 50)
            
            # AIç»“æœå·²åœ¨å¤„ç†è¿‡ç¨‹ä¸­å®æ—¶å†™å…¥
            results['ai_result_file'] = ai_result_file
            logging.info(f"AIåˆ†æç»“æœå·²å®æ—¶å†™å…¥: {ai_result_file}")
            
            if self.enable_transfer_log and self.transfer_log_manager and not dry_run:
                try:
                    session_summary = self.transfer_log_manager.end_transfer_session()
                    logging.info(f"è½¬ç§»æ—¥å¿—ä¼šè¯ç»“æŸ: {session_summary}")
                except Exception as e:
                    logging.warning(f"ç»“æŸè½¬ç§»æ—¥å¿—ä¼šè¯å¤±è´¥: {e}")
            
            logging.info(f"å®‰å…¨æ–‡ä»¶æ•´ç†å®Œæˆ: æˆåŠŸ {results['successful_moves']}, å¤±è´¥ {results['failed_moves']}, è·³è¿‡ {results['skipped_files']}")
            
            # åˆ é™¤æºæ–‡ä»¶åŠŸèƒ½å·²ç§»è‡³GUIç•Œé¢ï¼Œä¸å†åœ¨æ§åˆ¶å°è¯¢é—®
            if not dry_run and results['successful_moves'] > 0:
                logging.info(f"æˆåŠŸå¤„ç† {results['successful_moves']} ä¸ªæ–‡ä»¶ï¼Œåˆ é™¤æºæ–‡ä»¶åŠŸèƒ½è¯·åœ¨GUIç•Œé¢ä½¿ç”¨")
            
            return results
        except Exception as e:
            raise FileOrganizerError(f"æ‰¹é‡æ•´ç†æ–‡ä»¶å¤±è´¥: {e}")
    def get_transfer_logs(self) -> List[str]:
        if not self.enable_transfer_log or not self.transfer_log_manager:
            raise FileOrganizerError("è½¬ç§»æ—¥å¿—åŠŸèƒ½æœªå¯ç”¨")
        return self.transfer_log_manager.get_transfer_logs()
    def get_transfer_log_summary(self, log_file_path: str) -> Dict:
        if not self.enable_transfer_log or not self.transfer_log_manager:
            raise FileOrganizerError("è½¬ç§»æ—¥å¿—åŠŸèƒ½æœªå¯ç”¨")
        return self.transfer_log_manager.get_session_summary(log_file_path)
    def restore_files_from_log(self, log_file_path: str, operation_ids: Optional[List[int]] = None, dry_run: bool = True) -> Dict:
        if not self.enable_transfer_log or not self.transfer_log_manager:
            raise FileOrganizerError("è½¬ç§»æ—¥å¿—åŠŸèƒ½æœªå¯ç”¨")
        try:
            restore_results = self.transfer_log_manager.restore_from_log(
                log_file_path=log_file_path,
                operation_ids=operation_ids if operation_ids is not None else [],
                dry_run=dry_run
            )
            logging.info(f"æ–‡ä»¶æ¢å¤å®Œæˆ: æˆåŠŸ {restore_results['successful_restores']}, å¤±è´¥ {restore_results['failed_restores']}, è·³è¿‡ {restore_results['skipped_operations']}")
            return restore_results
        except Exception as e:
            raise FileOrganizerError(f"æ–‡ä»¶æ¢å¤å¤±è´¥: {e}")
    def cleanup_old_transfer_logs(self, days_to_keep: int = 30) -> int:
        if not self.enable_transfer_log or not self.transfer_log_manager:
            raise FileOrganizerError("è½¬ç§»æ—¥å¿—åŠŸèƒ½æœªå¯ç”¨")
        return self.transfer_log_manager.cleanup_old_logs(days_to_keep)
    def _append_ai_result_to_file(self, ai_result_file: str, ai_result_item: dict) -> None:
        """è¿½åŠ AIç»“æœåˆ°æ–‡ä»¶ï¼Œé¿å…åœ¨å†…å­˜ä¸­ä¿å­˜å¤§é‡æ•°æ®"""
        try:
            # è¯»å–ç°æœ‰æ•°æ®
            existing_data = []
            if os.path.exists(ai_result_file):
                try:
                    with open(ai_result_file, 'r', encoding='utf-8') as f:
                        content = f.read().strip()
                        if content:  # åªæœ‰å½“æ–‡ä»¶ä¸ä¸ºç©ºæ—¶æ‰å°è¯•è§£æJSON
                            existing_data = json.loads(content)
                        else:
                            existing_data = []  # ç©ºæ–‡ä»¶æ—¶åˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨
                except json.JSONDecodeError:
                    # å¦‚æœJSONè§£æå¤±è´¥ï¼Œåˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨
                    existing_data = []
                    logging.warning(f"AIç»“æœæ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œé‡æ–°åˆå§‹åŒ–: {ai_result_file}")
            
            # è¿½åŠ æ–°æ•°æ®
            existing_data.append(ai_result_item)
            
            # å†™å›æ–‡ä»¶
            with open(ai_result_file, 'w', encoding='utf-8') as f:
                json.dump(existing_data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            logging.error(f"è¿½åŠ AIç»“æœåˆ°æ–‡ä»¶å¤±è´¥: {e}")
    
    def _update_last_ai_result(self, ai_result_file: str, updates: dict) -> None:
        """æ›´æ–°æ–‡ä»¶ä¸­æœ€åä¸€æ¡AIç»“æœè®°å½•"""
        try:
            # è¯»å–ç°æœ‰æ•°æ®
            existing_data = []
            if os.path.exists(ai_result_file):
                try:
                    with open(ai_result_file, 'r', encoding='utf-8') as f:
                        content = f.read().strip()
                        if content:  # åªæœ‰å½“æ–‡ä»¶ä¸ä¸ºç©ºæ—¶æ‰å°è¯•è§£æJSON
                            existing_data = json.loads(content)
                        else:
                            existing_data = []  # ç©ºæ–‡ä»¶æ—¶åˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨
                except json.JSONDecodeError:
                    # å¦‚æœJSONè§£æå¤±è´¥ï¼Œåˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨
                    existing_data = []
                    logging.warning(f"AIç»“æœæ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œé‡æ–°åˆå§‹åŒ–: {ai_result_file}")
            
            # æ›´æ–°æœ€åä¸€æ¡è®°å½•
            if existing_data:
                existing_data[-1].update(updates)
                
                # å†™å›æ–‡ä»¶
                with open(ai_result_file, 'w', encoding='utf-8') as f:
                    json.dump(existing_data, f, ensure_ascii=False, indent=2)
                    
        except Exception as e:
            logging.error(f"æ›´æ–°AIç»“æœæ–‡ä»¶å¤±è´¥: {e}")

    # def _ask_delete_source_files(self, successful_moves: List[Dict]) -> None:
    #     """è¯¢é—®ç”¨æˆ·æ˜¯å¦åˆ é™¤æºæ–‡ä»¶ - å·²ç§»è‡³GUIç•Œé¢ï¼Œæ­¤æ–¹æ³•ä¸å†ä½¿ç”¨"""
    #     # åˆ é™¤æºæ–‡ä»¶åŠŸèƒ½å·²ç§»è‡³GUIç•Œé¢ï¼Œä¸å†åœ¨æ§åˆ¶å°è¯¢é—®
    #     pass

    def get_file_summary(self, file_path: str, max_length: int = 50, max_pages: int = 2, max_seconds: int = 10) -> str:
        """
        è‡ªåŠ¨é€‚é… txt/pdf/docx æ ¼å¼ï¼Œè¿”å›å‰max_lengthå­—æ‘˜è¦ï¼ŒPDF/Wordåªå–å‰max_pagesé¡µï¼Œå•æ–‡ä»¶å¤„ç†è¶…æ—¶max_secondsç§’ã€‚
        """
        ext = Path(file_path).suffix.lower()
        start_time = time.time()
        try:
            if ext == '.txt':
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    return f.read(max_length)
            elif ext == '.pdf':
                with open(file_path, 'rb') as f:
                    reader = PyPDF2.PdfReader(f)
                    text = ''
                    for i, page in enumerate(reader.pages):
                        if i >= max_pages or (time.time() - start_time) > max_seconds:
                            break
                        page_text = page.extract_text() or ''
                        text += page_text
                        if len(text) >= max_length:
                            break
                    if (time.time() - start_time) > max_seconds:
                        return 'æå–è¶…æ—¶ï¼Œå·²è·³è¿‡'
                    return text[:max_length] if text else 'æœªèƒ½æå–æ­£æ–‡'
            elif ext == '.docx':
                doc = docx.Document(file_path)
                text = ''
                for i, para in enumerate(doc.paragraphs):
                    if (time.time() - start_time) > max_seconds:
                        break
                    text += para.text
                    if len(text) >= max_length:
                        break
                if (time.time() - start_time) > max_seconds:
                    return 'æå–è¶…æ—¶ï¼Œå·²è·³è¿‡'
                return text[:max_length] if text else 'æœªèƒ½æå–æ­£æ–‡'
            else:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    return f.read(max_length)
        except Exception as e:
            if (time.time() - start_time) > max_seconds:
                return 'æå–è¶…æ—¶ï¼Œå·²è·³è¿‡'
            return f'æ‘˜è¦è·å–å¤±è´¥: {e}'
    
    def batch_read_documents(self, folder_path: str, progress_callback=None, summary_length: int = 200) -> Dict[str, Any]:
        """
        æ‰¹é‡è§£è¯»æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰æ–‡æ¡£ï¼Œç”Ÿæˆæ‘˜è¦å¹¶ä¿å­˜åˆ°AIç»“æœæ–‡ä»¶
        
        Args:
            folder_path: è¦è§£è¯»çš„æ–‡ä»¶å¤¹è·¯å¾„
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶ (current, total, filename) å‚æ•°
            summary_length: æ‘˜è¦é•¿åº¦ï¼ˆå­—æ•°ï¼‰ï¼Œé»˜è®¤200å­—
            
        Returns:
            åŒ…å«å¤„ç†ç»“æœçš„å­—å…¸
        """
        start_time = time.time()
        timing_info = {}
        
        try:
            if not self.ollama_client and not self.qwen_long_client:
                init_start = time.time()
                self.initialize_ollama()
                timing_info['ollama_init_time'] = round(time.time() - init_start, 3)
            
            folder_path = Path(folder_path)
            if not folder_path.exists() or not folder_path.is_dir():
                raise FileOrganizerError(f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨æˆ–ä¸æ˜¯æœ‰æ•ˆç›®å½•: {folder_path}")
            
            # æ”¯æŒçš„æ–‡æ¡£æ ¼å¼
            supported_extensions = {'.txt', '.pdf', '.docx', '.doc', '.md', '.py', '.js', '.html', '.css', '.json', '.xml', '.csv'}
            
            # æ‰«ææ–‡ä»¶å¤¹è·å–æ‰€æœ‰æ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶
            document_files = []
            for file_path in folder_path.rglob('*'):
                if file_path.is_file() and file_path.suffix.lower() in supported_extensions:
                    document_files.append(file_path)
            
            if not document_files:
                return {
                    'total_files': 0,
                    'processed_files': 0,
                    'successful_reads': 0,
                    'failed_reads': 0,
                    'errors': [],
                    'timing_info': timing_info,
                    'start_time': datetime.now(),
                    'end_time': datetime.now()
                }
            
            # åˆå§‹åŒ–ç»“æœç»Ÿè®¡
            results = {
                'total_files': len(document_files),
                'processed_files': 0,
                'successful_reads': 0,
                'failed_reads': 0,
                'errors': [],
                'timing_info': timing_info,
                'start_time': datetime.now(),
                'end_time': None
            }
            
            # åˆå§‹åŒ–AIç»“æœæ–‡ä»¶
            ai_result_file = 'ai_organize_result.json'
            if not os.path.exists(ai_result_file):
                with open(ai_result_file, 'w', encoding='utf-8') as f:
                    json.dump([], f, ensure_ascii=False, indent=2)
                logging.info(f"åˆ›å»ºæ–°çš„AIç»“æœæ–‡ä»¶: {ai_result_file}")
            else:
                logging.info(f"AIç»“æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œå°†è¿½åŠ æ–°è®°å½•: {ai_result_file}")
            

            
            logging.info(f"å¼€å§‹æ‰¹é‡æ–‡æ¡£è§£è¯»ï¼Œå…± {len(document_files)} ä¸ªæ–‡ä»¶")
            print(f"\n=== å¼€å§‹æ‰¹é‡æ–‡æ¡£è§£è¯» ===")
            print(f"æºæ–‡ä»¶å¤¹: {folder_path}")
            print(f"å¾…å¤„ç†æ–‡ä»¶æ€»æ•°: {len(document_files)}")
            print("=" * 50)
            
            # é€ä¸ªå¤„ç†æ–‡æ¡£æ–‡ä»¶
            for i, file_path in enumerate(document_files, 1):
                filename = file_path.name
                
                # è°ƒç”¨è¿›åº¦å›è°ƒ
                if progress_callback:
                    progress_callback(i, len(document_files), filename)
                
                # æ§åˆ¶å°è¾“å‡ºå½“å‰å¤„ç†è¿›åº¦
                print(f"\n[{i}/{len(document_files)}] æ­£åœ¨è§£è¯»: {filename}")
                
                try:
                    logging.info(f"æ­£åœ¨è§£è¯»æ–‡ä»¶ {i}/{len(document_files)}: {filename}")
                    print(f"  ğŸ“– æ­£åœ¨æå–æ–‡ä»¶å†…å®¹...", end="", flush=True)
                    
                    # æå–æ–‡ä»¶å†…å®¹
                    extract_start = time.time()
                    extracted_content = self._extract_file_content(str(file_path))
                    extract_time = round(time.time() - extract_start, 3)
                    
                    # æ ¹æ®è®¾ç½®æˆªå–å†…å®¹ç”¨äºAIå¤„ç†
                    truncate_length = self.ai_parameters['content_extraction_length'] if self.ai_parameters['content_extraction_length'] < 2000 else len(extracted_content)
                    content_for_ai = extracted_content[:truncate_length] if extracted_content else ""
                    
                    print(f"\r  ğŸ“– æ­£åœ¨ç”Ÿæˆæ‘˜è¦...", end="", flush=True)
                    
                    # ç”Ÿæˆæ‘˜è¦
                    summary_start = time.time()
                    summary = self._generate_content_summary(content_for_ai, filename, summary_length)
                    summary_time = round(time.time() - summary_start, 3)
                    
                    print(f"\r  âœ… è§£è¯»å®Œæˆ")
                    print(f"     æ‘˜è¦: {summary[:100]}{'...' if len(summary) > 100 else ''}")
                    
                    # æ„å»ºAIç»“æœé¡¹ï¼ˆä¸è¿ç§»åŠŸèƒ½æ ¼å¼å…¼å®¹ï¼‰
                    ai_result_item = {
                        "å¤„ç†æ—¶é—´": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        "æ–‡ä»¶å": filename,
                        "æºæ–‡ä»¶è·¯å¾„": str(file_path),  # è¿™é‡Œä½œä¸ºç›®æ ‡è·¯å¾„ï¼Œå› ä¸ºæ–‡ä»¶å·²åœ¨ç›®æ ‡ä½ç½®
                        "æ–‡ä»¶æ‘˜è¦": summary,
                        "æœ€åŒ¹é…çš„ç›®æ ‡ç›®å½•": str(file_path.parent),  # å½“å‰æ–‡ä»¶å¤¹ä½œä¸ºç›®æ ‡ç›®å½•
                        "åŒ¹é…ç†ç”±": "æ‰¹é‡æ–‡æ¡£è§£è¯»",
                        "å¤„ç†è€—æ—¶ä¿¡æ¯": {
                            "æ€»è€—æ—¶(ç§’)": round(extract_time + summary_time, 3),
                            "å†…å®¹æå–è€—æ—¶(ç§’)": extract_time,
                            "æ‘˜è¦ç”Ÿæˆè€—æ—¶(ç§’)": summary_time,
                            "ç›®å½•æ¨èè€—æ—¶(ç§’)": 0
                        },
                        "æœ€ç»ˆç›®æ ‡è·¯å¾„": str(file_path),  # æ–‡ä»¶å½“å‰ä½ç½®å°±æ˜¯æœ€ç»ˆä½ç½®
                        "æ“ä½œç±»å‹": "æ–‡æ¡£è§£è¯»",
                        "å¤„ç†çŠ¶æ€": "è§£è¯»æˆåŠŸ"
                    }
                    
                    # ä¿å­˜åˆ°AIç»“æœæ–‡ä»¶
                    self._append_ai_result_to_file(ai_result_file, ai_result_item)
                    
                    results['successful_reads'] += 1
                    logging.info(f"æ–‡æ¡£è§£è¯»æˆåŠŸ: {filename}ï¼Œæ‘˜è¦é•¿åº¦: {len(summary)} å­—ç¬¦")
                    
                except Exception as e:
                    error_msg = f"è§£è¯»æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {str(e)}"
                    print(f"\r  âŒ è§£è¯»å¤±è´¥: {str(e)}")
                    logging.error(error_msg)
                    results['errors'].append(error_msg)
                    results['failed_reads'] += 1
                    
                    # ä¿å­˜å¤±è´¥è®°å½•
                    try:
                        ai_result_item = {
                            "å¤„ç†æ—¶é—´": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                            "æ–‡ä»¶å": filename,
                            "æºæ–‡ä»¶è·¯å¾„": str(file_path),
                            "æ–‡ä»¶æ‘˜è¦": "",
                            "æœ€åŒ¹é…çš„ç›®æ ‡ç›®å½•": str(file_path.parent),
                            "åŒ¹é…ç†ç”±": "æ‰¹é‡æ–‡æ¡£è§£è¯»",
                            "å¤„ç†è€—æ—¶ä¿¡æ¯": {
                                "æ€»è€—æ—¶(ç§’)": 0,
                                "å†…å®¹æå–è€—æ—¶(ç§’)": 0,
                                "æ‘˜è¦ç”Ÿæˆè€—æ—¶(ç§’)": 0,
                                "ç›®å½•æ¨èè€—æ—¶(ç§’)": 0
                            },
                            "æœ€ç»ˆç›®æ ‡è·¯å¾„": str(file_path),
                            "æ“ä½œç±»å‹": "æ–‡æ¡£è§£è¯»",
                            "å¤„ç†çŠ¶æ€": "è§£è¯»å¤±è´¥",
                            "é”™è¯¯ä¿¡æ¯": str(e)
                        }
                        self._append_ai_result_to_file(ai_result_file, ai_result_item)
                    except Exception as save_error:
                        logging.warning(f"ä¿å­˜å¤±è´¥è®°å½•æ—¶å‡ºé”™: {save_error}")
                
                finally:
                    results['processed_files'] += 1
            
            results['end_time'] = datetime.now()
            
            # è¾“å‡ºå¤„ç†å®Œæˆæ€»ç»“
            print(f"\n=== æ‰¹é‡æ–‡æ¡£è§£è¯»å®Œæˆ ===")
            print(f"æ€»æ–‡ä»¶æ•°: {results['total_files']}")
            print(f"æˆåŠŸè§£è¯»: {results['successful_reads']} ä¸ª")
            print(f"è§£è¯»å¤±è´¥: {results['failed_reads']} ä¸ª")
            duration = (results['end_time'] - results['start_time']).total_seconds()
            print(f"æ€»è€—æ—¶: {duration:.1f} ç§’")
            print("=" * 50)
            
            logging.info(f"æ‰¹é‡æ–‡æ¡£è§£è¯»å®Œæˆ: æˆåŠŸ {results['successful_reads']}, å¤±è´¥ {results['failed_reads']}")
            logging.info(f"è§£è¯»ç»“æœå·²ä¿å­˜åˆ°: {ai_result_file}")
            
            return results
            
        except Exception as e:
            raise FileOrganizerError(f"æ‰¹é‡æ–‡æ¡£è§£è¯»å¤±è´¥: {e}")

    def _extract_folder_keywords(self, folder_path: str) -> List[str]:
        """
        ä»æ–‡ä»¶å¤¹è·¯å¾„ä¸­æå–æœ‰æ„ä¹‰çš„å…³é”®è¯
        
        Args:
            folder_path: æ–‡ä»¶å¤¹è·¯å¾„
            
        Returns:
            List[str]: å…³é”®è¯åˆ—è¡¨
        """
        try:
            # æå–è·¯å¾„ä¸­çš„æ–‡ä»¶å¤¹åç§°
            path_parts = folder_path.split('\\') + folder_path.split('/')
            keywords = []
            
            for part in path_parts:
                part = part.strip()
                if not part:
                    continue
                
                # ç§»é™¤ç¼–å·å‰ç¼€ï¼Œå¦‚"ã€7-4-5ã€‘"
                if 'ã€' in part and 'ã€‘' in part:
                    part = part.split('ã€‘', 1)[1] if 'ã€‘' in part else part
                
                # è¿‡æ»¤æ‰æ•°å­—å’Œå•ä¸ªå­—ç¬¦
                if len(part) <= 1 or part.isdigit():
                    continue
                
                # è¿‡æ»¤æ‰åŒ…å«è¿å­—ç¬¦çš„æ•°å­—ä»£ç ï¼Œå¦‚"7-4-5"
                if '-' in part and all(segment.isdigit() for segment in part.split('-')):
                    continue
                
                # è¿‡æ»¤æ‰çº¯è‹±æ–‡ç¼©å†™
                if part.isupper() and len(part) <= 3:
                    continue
                
                # é¿å…é‡å¤æ·»åŠ 
                if part not in keywords:
                    keywords.append(part)
                
                # å¯¹äºå¤åˆè¯æ±‡ï¼Œä¹Ÿæå–å…¶ä¸­çš„å…³é”®è¯
                if '\\' in part or '/' in part:
                    sub_parts = part.replace('\\', ' ').replace('/', ' ').split()
                    for sub_part in sub_parts:
                        sub_part = sub_part.strip()
                        if len(sub_part) > 1 and sub_part not in keywords:
                            keywords.append(sub_part)
            
            return keywords
        except Exception as e:
            logging.error(f"æå–æ–‡ä»¶å¤¹å…³é”®è¯å¤±è´¥: {e}")
            return []

    def _calculate_folder_relevance(self, file_content: str, file_summary: str, folder_path: str) -> float:
        """
        è®¡ç®—æ–‡ä»¶å†…å®¹ä¸æ–‡ä»¶å¤¹è·¯å¾„çš„ç›¸å…³æ€§è¯„åˆ†
        
        Args:
            file_content: æ–‡ä»¶å†…å®¹
            file_summary: æ–‡ä»¶æ‘˜è¦
            folder_path: æ–‡ä»¶å¤¹è·¯å¾„
            
        Returns:
            float: ç›¸å…³æ€§è¯„åˆ† (0.0-1.0)
        """
        try:
            # æå–æ–‡ä»¶å¤¹è·¯å¾„ä¸­çš„å…³é”®è¯
            folder_keywords = self._extract_folder_keywords(folder_path)
            if not folder_keywords:
                return 0.0
            
            # åˆå¹¶æ–‡ä»¶å†…å®¹å’Œæ‘˜è¦è¿›è¡Œåˆ†æ
            analysis_text = f"{file_content} {file_summary}".lower()
            
            # å®šä¹‰é€šç”¨çš„åŒä¹‰è¯å’Œç›¸å…³è¯æ±‡æ˜ å°„ï¼ˆåŸºäºå¸¸è§è¯æ±‡ï¼‰
            synonym_mapping = {
                # é€šç”¨ä¸šåŠ¡è¯æ±‡
                'ç»¼åˆ': ['ç»¼åˆ', 'ç­–ç•¥', 'æŠ•èµ„ç­–ç•¥', 'ç­–ç•¥æŠ¥å‘Š', 'ç»¼åˆåˆ†æ', 'ç»¼åˆç®¡ç†'],
                'ç ”ç©¶': ['ç ”ç©¶', 'åˆ†æ', 'è°ƒç ”', 'è°ƒæŸ¥', 'æŠ¥å‘Š', 'ä¸“é¢˜', 'ç ”ç©¶æŠ¥å‘Š'],
                'æŠ•èµ„': ['æŠ•èµ„', 'æŠ•èµ„ç­–ç•¥', 'æŠ•èµ„åˆ†æ', 'æŠ•èµ„ç ”ç©¶', 'æŠ•èµ„ç†å¿µ'],
                'è¡Œä¸š': ['è¡Œä¸š', 'äº§ä¸š', 'é¢†åŸŸ', 'æ¿å—', 'ç»†åˆ†', 'è¡Œä¸šç ”ç©¶'],
                'å…¬å¸': ['å…¬å¸', 'ä¼ä¸š', 'æœºæ„', 'é›†å›¢', 'ä¸»ä½“', 'å…¬å¸ç ”ç©¶'],
                'ç»æµ': ['ç»æµ', 'å®è§‚ç»æµ', 'ç»æµåˆ†æ', 'ç»æµç ”ç©¶', 'ç»æµæ¨¡å‹'],
                'çŸ¥è¯†': ['çŸ¥è¯†', 'ç™¾ç§‘', 'ç§‘æ™®', 'æ•™è‚²', 'å­¦ä¹ '],
                'é¡¹ç›®': ['é¡¹ç›®', 'åˆä½œ', 'åˆä½œé¡¹ç›®', 'é¡¹ç›®åˆä½œ'],
                'èµ„æ–™': ['èµ„æ–™', 'æ–‡æ¡£', 'æ–‡ä»¶', 'ææ–™', 'ä¿¡æ¯'],
                'æŠ¥å‘Š': ['æŠ¥å‘Š', 'ç ”ç©¶æŠ¥å‘Š', 'åˆ†ææŠ¥å‘Š', 'ç­–ç•¥æŠ¥å‘Š', 'ä¸“é¢˜æŠ¥å‘Š'],
                'é›†åˆ': ['é›†åˆ', 'æ±‡æ€»', 'æ•´ç†', 'åˆ†ç±»', 'é›†åˆ'],
                'åˆ†ç±»': ['åˆ†ç±»', 'åˆ†ç±»ç ”ç©¶', 'è¡Œä¸šåˆ†ç±»', 'ç ”ç©¶åˆ†ç±»'],
                'æƒç›Š': ['æƒç›Š', 'æƒç›ŠæŠ•èµ„', 'è‚¡ç¥¨', 'è‚¡æƒ', 'æƒç›Šç±»'],
                'ä¼°å€¼': ['ä¼°å€¼', 'ä¼°å€¼æ–¹æ³•', 'ä¼°å€¼ç ”ç©¶', 'ä»·å€¼è¯„ä¼°'],
                'æ–¹æ³•': ['æ–¹æ³•', 'ç ”ç©¶æ–¹æ³•', 'åˆ†ææ–¹æ³•', 'ä¼°å€¼æ–¹æ³•'],
            }
            
            # ç®€åŒ–çš„ç›¸å…³æ€§è®¡ç®—ï¼šä¸»è¦åŸºäºå…³é”®è¯åŒ¹é…å’Œè·¯å¾„é•¿åº¦æƒé‡
            # åˆ é™¤å¤æ‚çš„ç›¸æ–¥æ€§æ£€æŸ¥ï¼Œè®©AIè‡ªå·±åˆ¤æ–­
            
            # è®¡ç®—ç›¸å…³æ€§è¯„åˆ†
            total_score = 0.0
            matched_keywords = []
            
            for keyword in folder_keywords:
                keyword_lower = keyword.lower()
                
                # ç›´æ¥åŒ¹é…ï¼ˆå®Œå…¨åŒ¹é…ï¼‰
                if keyword_lower in analysis_text:
                    total_score += 1.0
                    matched_keywords.append(keyword)
                    continue
                
                # éƒ¨åˆ†åŒ¹é…ï¼ˆå…³é”®è¯æ˜¯æ–‡ä»¶å†…å®¹ä¸­æŸä¸ªè¯æ±‡çš„å­ä¸²ï¼‰
                if any(keyword_lower in word.lower() for word in analysis_text.split()):
                    total_score += 0.9
                    matched_keywords.append(f"{keyword}(éƒ¨åˆ†åŒ¹é…)")
                    continue
                
                # åŒä¹‰è¯åŒ¹é…
                if keyword in synonym_mapping:
                    synonyms = synonym_mapping[keyword]
                    for synonym in synonyms:
                        if synonym.lower() in analysis_text:
                            total_score += 0.8  # åŒä¹‰è¯åŒ¹é…æƒé‡ç¨ä½
                            matched_keywords.append(f"{keyword}(åŒä¹‰è¯:{synonym})")
                            break
            
            # è®¡ç®—åŸºç¡€ç›¸å…³æ€§è¯„åˆ†
            if folder_keywords:
                base_relevance_score = total_score / len(folder_keywords)
                
                # è·¯å¾„é•¿åº¦æƒé‡ï¼šè·¯å¾„è¶Šé•¿ï¼ˆåˆ†ç±»è¶Šç»†è‡´ï¼‰ï¼Œæƒé‡è¶Šé«˜
                path_depth = len([p for p in folder_path.split('\\') + folder_path.split('/') if p.strip()])
                path_length_bonus = min(0.3, path_depth * 0.05)  # æœ€å¤šå¢åŠ 0.3åˆ†
                
                # æœ€ç»ˆç›¸å…³æ€§è¯„åˆ† = åŸºç¡€è¯„åˆ† + è·¯å¾„é•¿åº¦å¥–åŠ±
                relevance_score = base_relevance_score + path_length_bonus
                
                print(f"ğŸ“Š ç›¸å…³æ€§è¯„åˆ†: {relevance_score:.3f} (åŸºç¡€: {base_relevance_score:.3f}, è·¯å¾„å¥–åŠ±: {path_length_bonus:.3f}, åŒ¹é…å…³é”®è¯: {matched_keywords})")
                return relevance_score
            
            return 0.0
            
        except Exception as e:
            logging.error(f"è®¡ç®—æ–‡ä»¶å¤¹ç›¸å…³æ€§å¤±è´¥: {e}")
            return 0.0

    def _filter_irrelevant_folders(self, file_content: str, file_summary: str, recommended_paths: List[str], target_directory: str) -> List[str]:
        """
        è¿‡æ»¤æ‰ä¸æ–‡ä»¶å†…å®¹ä¸ç›¸å…³çš„æ–‡ä»¶å¤¹è·¯å¾„ï¼Œå¹¶æŒ‰ç›¸å…³æ€§è¯„åˆ†æ’åº
        
        Args:
            file_content: æ–‡ä»¶å†…å®¹
            file_summary: æ–‡ä»¶æ‘˜è¦
            recommended_paths: AIæ¨èçš„æ–‡ä»¶å¤¹è·¯å¾„åˆ—è¡¨
            target_directory: ç›®æ ‡ç›®å½•
            
        Returns:
            List[str]: æŒ‰ç›¸å…³æ€§è¯„åˆ†æ’åºçš„æ–‡ä»¶å¤¹è·¯å¾„åˆ—è¡¨
        """
        try:
            print(f"ğŸ” å¼€å§‹ç›¸å…³æ€§åˆ†æå’Œæ’åº...")
            print(f"ğŸ“‹ åŸå§‹æ¨èè·¯å¾„: {recommended_paths}")
            
            relevance_scores = {}
            
            for path in recommended_paths:
                # è®¡ç®—ç›¸å…³æ€§è¯„åˆ†
                relevance_score = self._calculate_folder_relevance(file_content, file_summary, path)
                relevance_scores[path] = relevance_score
                
                print(f"ğŸ“Š {path}: ç›¸å…³æ€§è¯„åˆ† {relevance_score:.3f}")
            
            # ç®€åŒ–è¿‡æ»¤é€»è¾‘ï¼šåªè¿‡æ»¤æ‰ç›¸å…³æ€§è¯„åˆ†ä¸º0çš„è·¯å¾„ï¼Œä¿ç•™æ‰€æœ‰æœ‰è¯„åˆ†çš„è·¯å¾„
            filtered_paths = [path for path, score in relevance_scores.items() if score > 0]
            
            if not filtered_paths:
                print(f"âš ï¸ æ‰€æœ‰è·¯å¾„ç›¸å…³æ€§è¯„åˆ†éƒ½ä¸º0ï¼Œè¿”å›åŸå§‹æ¨è")
                return recommended_paths  # å›é€€åˆ°åŸå§‹æ¨èï¼Œè€Œä¸æ˜¯æ‹’ç»åˆ†ç±»
            
            # æŒ‰ç›¸å…³æ€§è¯„åˆ†ä»é«˜åˆ°ä½æ’åº
            sorted_paths = sorted(filtered_paths, key=lambda x: relevance_scores[x], reverse=True)
            
            print(f"âœ… ç›¸å…³æ€§æ’åºå®Œæˆ:")
            for i, path in enumerate(sorted_paths, 1):
                print(f"  {i}. {path} (è¯„åˆ†: {relevance_scores[path]:.3f})")
            
            return sorted_paths
            
        except Exception as e:
            logging.error(f"ç›¸å…³æ€§è¿‡æ»¤å¤±è´¥: {e}")
            print(f"âŒ ç›¸å…³æ€§è¿‡æ»¤å¤±è´¥: {e}")
            return recommended_paths

    def _extract_year_from_filename(self, filename: str) -> Optional[int]:
        """
        ä»æ–‡ä»¶åä¸­æå–å¹´ä»½ä¿¡æ¯
        æ”¯æŒå¤šç§å¹´ä»½æ ¼å¼ï¼š2024ã€2025ã€24ã€25ç­‰
        """
        import re
        
        # æå–4ä½å¹´ä»½ (2024, 2025ç­‰)
        year_patterns = [
            r'20\d{2}',  # 2020-2099
            r'19\d{2}',  # 1900-1999
        ]
        
        for pattern in year_patterns:
            matches = re.findall(pattern, filename)
            if matches:
                # è¿”å›æ‰¾åˆ°çš„ç¬¬ä¸€ä¸ªå¹´ä»½
                return int(matches[0])
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°4ä½å¹´ä»½ï¼Œå°è¯•2ä½å¹´ä»½ (24, 25ç­‰)
        short_year_pattern = r'\b(2[0-9]|1[0-9])\b'  # 10-29
        matches = re.findall(short_year_pattern, filename)
        if matches:
            year = int(matches[0])
            # å‡è®¾20xxå¹´
            if year >= 20:
                return 2000 + year
            else:
                return 1900 + year
        
        return None

    def _check_and_create_year_folder(self, target_folder_path: Path, filename: str) -> Path:
        """
        æ£€æŸ¥ç›®æ ‡æ–‡ä»¶å¤¹ä¸­æ˜¯å¦åŒ…å«æ–‡ä»¶å¹´ä»½ï¼Œå¦‚æœæ²¡æœ‰åˆ™åˆ›å»ºå¹´ä»½å­æ–‡ä»¶å¤¹
        å¹¶å‚è€ƒåŸç›®å½•ç»“æ„åˆ›å»ºåˆé€‚çš„å­ç›®å½•
        """
        # æå–æ–‡ä»¶åä¸­çš„å¹´ä»½
        file_year = self._extract_year_from_filename(filename)
        
        if not file_year:
            print(f"ğŸ“… æ— æ³•ä»æ–‡ä»¶åä¸­æå–å¹´ä»½: {filename}")
            return target_folder_path
        
        print(f"ğŸ“… ä»æ–‡ä»¶åæå–åˆ°å¹´ä»½: {file_year}")
        
        # æ£€æŸ¥ç›®æ ‡æ–‡ä»¶å¤¹ä¸­æ˜¯å¦å·²æœ‰è¯¥å¹´ä»½çš„å­æ–‡ä»¶å¤¹
        year_folder_name = str(file_year)
        
        # æ£€æŸ¥ç›®æ ‡æ–‡ä»¶å¤¹åŠå…¶å­æ–‡ä»¶å¤¹ä¸­æ˜¯å¦åŒ…å«è¯¥å¹´ä»½
        existing_year_folders = []
        if target_folder_path.exists():
            for item in target_folder_path.iterdir():
                if item.is_dir():
                    # æ£€æŸ¥æ–‡ä»¶å¤¹åæ˜¯å¦åŒ…å«å¹´ä»½
                    if year_folder_name in item.name:
                        existing_year_folders.append(item)
                    # é€’å½’æ£€æŸ¥å­æ–‡ä»¶å¤¹
                    for subitem in item.rglob("*"):
                        if subitem.is_dir() and year_folder_name in subitem.name:
                            existing_year_folders.append(subitem)
        
        if existing_year_folders:
            print(f"ğŸ“… æ‰¾åˆ°ç°æœ‰å¹´ä»½æ–‡ä»¶å¤¹: {[str(f) for f in existing_year_folders]}")
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„å¹´ä»½æ–‡ä»¶å¤¹
            return existing_year_folders[0]
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¹´ä»½æ–‡ä»¶å¤¹ï¼Œéœ€è¦åˆ›å»ºæ–°çš„å¹´ä»½æ–‡ä»¶å¤¹ç»“æ„
        # åˆ†æåŸç›®å½•ç»“æ„ï¼Œåˆ›å»ºåˆé€‚çš„å¹´ä»½ç›®å½•
        
        # è·å–ç›®æ ‡ç›®å½•çš„æ ¹ç›®å½•ï¼ˆé€šå¸¸æ˜¯ç›®æ ‡åŸºç¡€ç›®å½•ï¼‰
        # ä¾‹å¦‚ï¼šå¦‚æœç›®æ ‡è·¯å¾„æ˜¯ "F:\æµ©æ€»å‚é˜…èµ„æ–™\ã€01ã€‘ç­–ç•¥æŠ¥å‘Šé›†åˆ\2021\2021å¹´æŠ•èµ„ç­–ç•¥\ä¸­ä¿¡å»ºæŠ•\è¡Œä¸š"
        # æˆ‘ä»¬éœ€è¦æ‰¾åˆ° "F:\æµ©æ€»å‚é˜…èµ„æ–™\ã€01ã€‘ç­–ç•¥æŠ¥å‘Šé›†åˆ" ä½œä¸ºæ ¹ç›®å½•
        
        # æŸ¥æ‰¾åŒ…å«å¹´ä»½çš„ç›®å½•å±‚çº§
        target_parts = list(target_folder_path.parts)
        
        # ä»åå¾€å‰æŸ¥æ‰¾ç¬¬ä¸€ä¸ªåŒ…å«å¹´ä»½çš„ç›®å½•
        year_index = -1
        for i, part in enumerate(target_parts):
            if part.isdigit() and len(part) == 4 and part.startswith('20'):
                year_index = i
                break
        
        if year_index != -1:
            # æ‰¾åˆ°äº†å¹´ä»½ç›®å½•ï¼Œæ›¿æ¢å®ƒ
            new_parts = target_parts.copy()
            new_parts[year_index] = str(file_year)
            new_year_folder_path = Path(*new_parts)
        else:
            # æ²¡æœ‰æ‰¾åˆ°å¹´ä»½ç›®å½•ï¼Œåœ¨ç›®æ ‡ç›®å½•ä¸‹åˆ›å»ºå¹´ä»½æ–‡ä»¶å¤¹
            new_year_folder_path = target_folder_path / year_folder_name
        
        print(f"ğŸ“… åˆ›å»ºæ–°çš„å¹´ä»½ç›®å½•ç»“æ„: {new_year_folder_path}")
        new_year_folder_path.mkdir(parents=True, exist_ok=True)
        
        return new_year_folder_path