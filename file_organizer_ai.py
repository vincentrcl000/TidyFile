#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIæ™ºèƒ½åˆ†ç±»ä¸“ç”¨æ–‡ä»¶æ•´ç†æ ¸å¿ƒæ¨¡å—
"""
import os
import shutil
import logging
import json
import re
from datetime import datetime
from typing import List, Dict, Tuple, Optional, Any
from pathlib import Path
import ollama
from transfer_log_manager import TransferLogManager
from classification_rules_manager import ClassificationRulesManager
import PyPDF2
from PyPDF2 import PdfReader, PdfWriter
import docx
from docx import Document
import time
import requests
from ai_client_manager import get_ai_manager, chat_with_ai, refresh_ai_clients

class FileOrganizerError(Exception):
    pass

class FileOrganizer:
    def __init__(self, model_name: str = None, enable_transfer_log: bool = True):
        self.model_name = model_name
        self.enable_transfer_log = enable_transfer_log
        self.transfer_log_manager = None
        self.setup_logging()
        
        if self.enable_transfer_log:
            self.transfer_log_manager = TransferLogManager()
        
        # åˆå§‹åŒ–åˆ†ç±»è§„åˆ™ç®¡ç†å™¨
        self.classification_rules_manager = ClassificationRulesManager()
        
        # åˆå§‹åŒ–AIç®¡ç†å™¨
        self.ai_manager = get_ai_manager()
        
        # åˆå§‹åŒ–AIå‚æ•°
        self.ai_parameters = {
            'similarity_threshold': 0.7,
            'max_retries': 3,
            'content_extraction_length': 3000,
            'summary_length': 200,
            'classification_prompt_template': None
        }
        
        # æ–°å¢ï¼šæ–‡ä»¶ç¼“å­˜å’Œæ ‡ç­¾ç³»ç»Ÿ
        self.file_cache = {}  # ç¼“å­˜æ–‡ä»¶ä¿¡æ¯å’Œå…ƒæ•°æ®
        self.level_tags = {}  # ç¼“å­˜å„çº§æ ‡ç­¾
        self.global_cache = {}  # å…¨å±€ç¼“å­˜ï¼Œç”¨äºæ€»ç»“å’Œåˆ é™¤æºæ–‡ä»¶ç­‰åŠŸèƒ½
    def setup_logging(self) -> None:
        """è®¾ç½®æ—¥å¿—é…ç½®ï¼Œä»…è¾“å‡ºåˆ°æ§åˆ¶å°"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler()
            ]
        )
        logging.info("æ–‡ä»¶æ•´ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    def initialize_ollama(self) -> None:
        """åˆå§‹åŒ–AIå®¢æˆ·ç«¯ï¼Œä½¿ç”¨ç»Ÿä¸€çš„AIç®¡ç†å™¨"""
        try:
            # åˆ·æ–°AIå®¢æˆ·ç«¯
            refresh_ai_clients()
            logging.info("AIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            raise FileOrganizerError(f"AIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
    def scan_target_folders(self, target_directory: str) -> List[str]:
        """æ‰«æç›®æ ‡æ–‡ä»¶å¤¹ï¼Œè¿”å›ç›¸å¯¹è·¯å¾„åˆ—è¡¨ï¼ˆå†…éƒ¨ä½¿ç”¨ï¼Œä¸è¾“å‡ºæ—¥å¿—ï¼‰"""
        try:
            target_path = Path(target_directory)
            if not target_path.exists():
                raise FileOrganizerError(f"ç›®æ ‡ç›®å½•ä¸å­˜åœ¨: {target_directory}")
            folders = []
            for item in target_path.rglob('*'):
                if item.is_dir() and item != target_path:  # æ’é™¤ç›®æ ‡ç›®å½•æœ¬èº«
                    relative_path = item.relative_to(target_path)
                    folders.append(str(relative_path))
            return folders
        except Exception as e:
            raise FileOrganizerError(f"æ‰«æç›®æ ‡æ–‡ä»¶å¤¹å¤±è´¥: {e}")
    def get_directory_tree_structure(self, target_directory: str) -> str:
        """ç”Ÿæˆç›®æ ‡ç›®å½•ç»“æ„ï¼Œè¿”å›å®Œæ•´è·¯å¾„åˆ—è¡¨"""
        try:
            target_path = Path(target_directory)
            if not target_path.exists():
                raise FileOrganizerError(f"ç›®æ ‡ç›®å½•ä¸å­˜åœ¨: {target_directory}")
            
            # ç›´æ¥æ‰«æå¹¶æ„å»ºç›®å½•ç»“æ„
            folders = []
            for item in target_path.rglob('*'):
                if item.is_dir() and item != target_path:  # æ’é™¤ç›®æ ‡ç›®å½•æœ¬èº«
                    relative_path = item.relative_to(target_path)
                    folders.append(str(relative_path))
            
            # æ„å»ºæ¸…æ™°çš„è·¯å¾„åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªå®Œæ•´è·¯å¾„
            path_lines = []
            for i, folder_path in enumerate(folders, 1):
                path_lines.append(f"{i}. {folder_path}")
            
            tree_structure = "\n".join(path_lines)
            logging.info(f"ç”Ÿæˆç›®å½•è·¯å¾„ç»“æ„ï¼Œå…± {len(path_lines)} è¡Œ")
            return tree_structure
        except Exception as e:
            raise FileOrganizerError(f"ç”Ÿæˆç›®å½•æ ‘ç»“æ„å¤±è´¥: {e}")
    def analyze_and_classify_file(self, file_path: str, target_directory: str) -> Dict[str, Any]:
        """
        æ–°çš„æ–‡ä»¶åˆ†æå’Œåˆ†ç±»æ–¹æ³•ï¼šå…ˆè§£æå†…å®¹ï¼Œç”Ÿæˆæ‘˜è¦ï¼Œå†æ¨èç›®å½•
        è¿”å›åŒ…å«æå–å†…å®¹ã€æ‘˜è¦å’Œæ¨èç›®å½•çš„å®Œæ•´ç»“æœ
        """
        start_time = time.time()
        timing_info = {}
        
        try:
            if not hasattr(self, 'ai_manager') or self.ai_manager is None:
                init_start = time.time()
                self.initialize_ollama()
                timing_info['ollama_init_time'] = round(time.time() - init_start, 3)
            
            file_name = Path(file_path).name
            logging.info(f"å¼€å§‹åˆ†ææ–‡ä»¶: {file_name}")
            
            # ç¬¬ä¸€æ­¥ï¼šæå–æ–‡ä»¶å…ƒæ•°æ®å¹¶ç¼“å­˜
            metadata_start = time.time()
            file_metadata = self._extract_file_metadata(file_path)
            self.file_cache[file_path] = file_metadata
            metadata_time = round(time.time() - metadata_start, 3)
            timing_info['metadata_extraction_time'] = metadata_time
            logging.info(f"æ–‡ä»¶å…ƒæ•°æ®æå–å®Œæˆï¼Œè€—æ—¶: {metadata_time}ç§’")
            
            # ç¬¬äºŒæ­¥ï¼šè§£ææ–‡ä»¶å†…å®¹
            extract_start = time.time()
            extracted_content = self._extract_file_content(file_path)
            extract_time = round(time.time() - extract_start, 3)
            timing_info['content_extraction_time'] = extract_time
            logging.info(f"æ–‡ä»¶å†…å®¹æå–å®Œæˆï¼Œé•¿åº¦: {len(extracted_content)} å­—ç¬¦ï¼Œè€—æ—¶: {extract_time}ç§’")
            
            # æ ¹æ®è®¾ç½®æˆªå–å†…å®¹ç”¨äºåç»­AIå¤„ç†ï¼Œæé«˜å¤„ç†æ•ˆç‡
            truncate_length = self.ai_parameters['content_extraction_length'] if self.ai_parameters['content_extraction_length'] < 2000 else len(extracted_content)
            content_for_ai = extracted_content[:truncate_length] if extracted_content else ""
            if len(extracted_content) > truncate_length:
                logging.info(f"å†…å®¹å·²æˆªå–è‡³å‰{truncate_length}å­—ç¬¦ç”¨äºAIå¤„ç†ï¼ˆåŸé•¿åº¦: {len(extracted_content)} å­—ç¬¦ï¼‰")
            
            # ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆ100å­—æ‘˜è¦ï¼ˆä½¿ç”¨æˆªå–åçš„å†…å®¹ï¼‰
            summary_start = time.time()
            summary = self._generate_content_summary(content_for_ai, file_name)
            summary_time = round(time.time() - summary_start, 3)
            timing_info['summary_generation_time'] = summary_time
            logging.info(f"å†…å®¹æ‘˜è¦ç”Ÿæˆå®Œæˆ: {summary[:50]}...ï¼Œè€—æ—¶: {summary_time}ç§’")
            
            # ç¬¬å››æ­¥ï¼šä½¿ç”¨é€’å½’é€å±‚åŒ¹é…æ¨èæœ€åŒ¹é…çš„å­˜æ”¾ç›®å½•
            recommend_start = time.time()
            recommended_folder, level_tags, match_reason = self._recommend_target_folder_recursive(
                file_path, content_for_ai, summary, target_directory
            )
            recommend_time = round(time.time() - recommend_start, 3)
            timing_info['folder_recommendation_time'] = recommend_time
            
            total_time = round(time.time() - start_time, 3)
            timing_info['total_processing_time'] = total_time
            
            result = {
                'file_path': file_path,
                'file_name': file_name,
                'file_metadata': file_metadata,
                'extracted_content': extracted_content,
                'content_summary': summary,
                'recommended_folder': recommended_folder,
                'level_tags': level_tags,
                'match_reason': match_reason,
                'success': recommended_folder is not None,
                'timing_info': timing_info
            }
            
            if recommended_folder:
                logging.info(f"æ–‡ä»¶åˆ†æå®Œæˆ: {file_name} -> {recommended_folder}ï¼Œæ€»è€—æ—¶: {total_time}ç§’")
                logging.info(f"æ‘˜è¦: {summary}")
                logging.info(f"æ¨èç†ç”±: {match_reason}")
                logging.info(f"å„çº§æ ‡ç­¾: {level_tags}")
                logging.info(f"è¯¦ç»†è€—æ—¶ - å…ƒæ•°æ®æå–: {metadata_time}ç§’, å†…å®¹æå–: {extract_time}ç§’, æ‘˜è¦ç”Ÿæˆ: {summary_time}ç§’, ç›®å½•æ¨è: {recommend_time}ç§’")
            else:
                logging.warning(f"æ–‡ä»¶åˆ†æå¤±è´¥: {file_name}ï¼Œæ€»è€—æ—¶: {total_time}ç§’")
            
            return result
            
        except Exception as e:
            total_time = round(time.time() - start_time, 3)
            timing_info['total_processing_time'] = total_time
            logging.error(f"æ–‡ä»¶åˆ†æå¤±è´¥: {e}ï¼Œæ€»è€—æ—¶: {total_time}ç§’")
            return {
                'file_path': file_path,
                'file_name': Path(file_path).name,
                'file_metadata': {},
                'extracted_content': '',
                'content_summary': '',
                'recommended_folder': None,
                'level_tags': [],
                'match_reason': f"åˆ†æå¤±è´¥: {str(e)}",
                'success': False,
                'error': str(e),
                'timing_info': timing_info
            }
    
    def classify_file(self, file_path: str, target_directory: str) -> tuple:
        """
        ä¿æŒåŸæœ‰çš„åˆ†ç±»æ–¹æ³•å…¼å®¹æ€§
        """
        result = self.analyze_and_classify_file(file_path, target_directory)
        return result['recommended_folder'], result['match_reason'], result['success']
    def _build_classification_prompt(self, file_path: str, target_directory: str) -> str:
        file_name = Path(file_path).name
        file_extension = Path(file_path).suffix.lower()
        file_content = ""
        content_readable = False
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                file_content = f.read()[:2000]
                printable_ratio = sum(1 for c in file_content if c.isprintable() or c.isspace()) / len(file_content) if file_content else 0
                if printable_ratio > 0.7:
                    content_readable = True
                else:
                    file_content = "æ–‡ä»¶å†…å®¹ä¸ºäºŒè¿›åˆ¶æ ¼å¼ï¼Œæ— æ³•è¯»å–"
        except Exception:
            for encoding in ['gbk', 'gb2312', 'latin-1']:
                try:
                    with open(file_path, 'r', encoding=encoding, errors='ignore') as f:
                        file_content = f.read()[:2000]
                        printable_ratio = sum(1 for c in file_content if c.isprintable() or c.isspace()) / len(file_content) if file_content else 0
                        if printable_ratio > 0.7:
                            content_readable = True
                            break
                        else:
                            file_content = "æ–‡ä»¶å†…å®¹ä¸ºäºŒè¿›åˆ¶æ ¼å¼ï¼Œæ— æ³•è¯»å–"
                except Exception:
                    continue
            if not content_readable:
                file_content = "æ— æ³•è¯»å–æ–‡ä»¶å†…å®¹"
        directory_structure = self.get_directory_tree_structure(target_directory)
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡ä»¶åˆ†ç±»åŠ©æ‰‹ã€‚è¯·é˜…è¯»åŸæ–‡ä»¶çš„å‰500ä¸ªå­—ï¼Œæˆ–è€…è¯†åˆ«ç¬¬ä¸€é¡µå†…å®¹ï¼Œæ ¹æ®æ–‡ä»¶å†…å®¹åˆ¤æ–­æ–‡ä»¶åº”è¯¥å½’ç±»åˆ°ä¸‹åˆ—å“ªä¸ªæ–‡ä»¶å¤¹ä¸­ï¼Œæ³¨æ„ï¼Œæ¯ä¸ªæ–‡ä»¶æ ¹æ®æ–‡ä»¶å†…å®¹åªåŒ¹é…ä¸€ä¸ªæ–‡ä»¶å¤¹ï¼ŒåŒ¹é…åŸåˆ™æ˜¯ï¼š

- ä¼˜å…ˆåŒ¹é…ï¼šæ–‡ä»¶å†…å®¹ä¸»é¢˜å’Œæ–‡ä»¶å¤¹ååŠæ–‡ä»¶è·¯å¾„åæœ€ç›¸ç¬¦
- é™çº§åŒ¹é…ï¼šå¦‚æœè¯»å–ä¸åˆ°æ–‡ä»¶å†…å®¹ï¼Œåˆ™ç”¨åŸæ–‡ä»¶åå’Œç›®æ ‡æ–‡ä»¶å¤¹ååŠæ–‡ä»¶å¤¹è·¯å¾„åæ¥åŒ¹é…

æ–‡ä»¶ä¿¡æ¯ï¼š
- åŸæ–‡ä»¶åï¼š{file_name}
- æ–‡ä»¶æ‰©å±•åï¼š{file_extension}
- æ–‡ä»¶å†…å®¹ï¼š{file_content if content_readable else "æ— æ³•è¯»å–å†…å®¹"}

ç›®æ ‡ç›®å½•æ–‡ä»¶å¤¹æ¸…å•å¦‚ä¸‹ï¼š
{directory_structure}

è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
åŸæ–‡ä»¶å|ç›®æ ‡æ–‡ä»¶å¤¹|åŒ¹é…ç†ç”±

å…¶ä¸­åŒ¹é…ç†ç”±æ ¼å¼ä¸ºï¼š
å†…å®¹åŒ¹é…ï¼š{{ç”¨ä¸å¤šäº20ä¸ªå­—æè¿°åŒ¹é…ç†ç”±}}
æˆ–è€…
æ— æ³•è¯»å–å†…å®¹ï¼Œé‡‡ç”¨æ–‡ä»¶å¤¹ååŒ¹é…ï¼š{{ç”¨ä¸å¤šäº20ä¸ªå­—æè¿°åŒ¹é…ç†ç”±}}

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ ¼å¼è¾“å‡ºï¼Œåªè¾“å‡ºä¸€è¡Œç»“æœã€‚
"""
        return prompt
    
    def _extract_file_metadata(self, file_path: str) -> Dict[str, Any]:
        """
        æå–æ–‡ä»¶å…ƒæ•°æ®ï¼ŒåŒ…æ‹¬åˆ›å»ºæ—¶é—´ã€ä¿®æ”¹æ—¶é—´ç­‰
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            åŒ…å«æ–‡ä»¶å…ƒæ•°æ®çš„å­—å…¸
        """
        try:
            file_path = Path(file_path)
            if not file_path.exists():
                raise Exception(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            
            stat = file_path.stat()
            
            metadata = {
                'file_name': file_path.name,
                'file_extension': file_path.suffix.lower(),
                'file_size': stat.st_size,
                'created_time': datetime.fromtimestamp(stat.st_ctime).isoformat(),
                'modified_time': datetime.fromtimestamp(stat.st_mtime).isoformat(),
                'full_path': str(file_path.absolute()),
                'relative_path': str(file_path)
            }
            
            return metadata
            
        except Exception as e:
            logging.error(f"æå–æ–‡ä»¶å…ƒæ•°æ®å¤±è´¥: {e}")
            return {
                'file_name': Path(file_path).name,
                'file_extension': Path(file_path).suffix.lower(),
                'file_size': 0,
                'created_time': datetime.now().isoformat(),
                'modified_time': datetime.now().isoformat(),
                'full_path': str(Path(file_path).absolute()),
                'relative_path': str(file_path),
                'error': str(e)
            }
    
    def _extract_file_content(self, file_path: str, max_length: int = 3000) -> str:
        """
        æå–æ–‡ä»¶å†…å®¹ï¼ˆä»file_reader.pyå¤åˆ¶çš„å®Œæ•´å®ç°ï¼‰
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            max_length: æœ€å¤§æå–é•¿åº¦
            
        Returns:
            æå–çš„æ–‡ä»¶å†…å®¹
        """
        try:
            file_path = Path(file_path)
            if not file_path.exists():
                raise Exception(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            
            file_extension = file_path.suffix.lower()
            logging.info(f"æ­£åœ¨æå–æ–‡ä»¶å†…å®¹: {file_path.name} (ç±»å‹: {file_extension})")
            
            # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©ä¸åŒçš„æå–æ–¹æ³•
            if file_extension == '.pdf':
                return self._extract_pdf_content(file_path, max_length)
            elif file_extension in ['.docx', '.doc']:
                return self._extract_docx_content(file_path, max_length)
            elif file_extension in ['.txt', '.md', '.py', '.js', '.html', '.css', '.json', '.xml', '.csv']:
                return self._extract_text_content(file_path, max_length)
            elif file_extension in ['.jpg', '.jpeg', '.png', '.bmp', '.gif']:
                return self._extract_image_info(file_path)
            else:
                # å°è¯•ä½œä¸ºæ–‡æœ¬æ–‡ä»¶è¯»å–
                return self._extract_text_content(file_path, max_length)
                
        except Exception as e:
            logging.error(f"æå–æ–‡ä»¶å†…å®¹å¤±è´¥: {e}")
            return f"æ— æ³•æå–æ–‡ä»¶å†…å®¹: {str(e)}"
    
    def _extract_pdf_content(self, file_path: Path, max_length: int) -> str:
        """
        æå–PDFæ–‡ä»¶å†…å®¹ï¼ˆä»file_reader.pyå¤åˆ¶çš„å®Œæ•´å®ç°ï¼‰
        """
        try:
            with open(file_path, 'rb') as file:
                pdf_reader = PdfReader(file)
                content = ""
                
                # è¯»å–å‰å‡ é¡µå†…å®¹
                for page_num in range(min(3, len(pdf_reader.pages))):
                    page = pdf_reader.pages[page_num]
                    content += page.extract_text() + "\n"
                    
                    if len(content) >= max_length:
                        break
                
                return content[:max_length] if content else "PDFæ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–æ— æ³•æå–"
                
        except Exception as e:
            return f"PDFæ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}"
    
    def _extract_docx_content(self, file_path: Path, max_length: int) -> str:
        """
        æå–Wordæ–‡æ¡£å†…å®¹ï¼ˆä»file_reader.pyå¤åˆ¶çš„å®Œæ•´å®ç°ï¼‰
        """
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”å¯è¯»
            if not file_path.exists():
                return "æ–‡ä»¶ä¸å­˜åœ¨"
            
            if not file_path.is_file():
                return "è·¯å¾„ä¸æ˜¯æ–‡ä»¶"
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = file_path.stat().st_size
            if file_size == 0:
                return "æ–‡ä»¶ä¸ºç©º"
            
            # å°è¯•è¯»å–Wordæ–‡æ¡£
            try:
                doc = Document(file_path)
                content = ""
                
                # æå–æ®µè½å†…å®¹
                for paragraph in doc.paragraphs:
                    if paragraph.text.strip():  # è·³è¿‡ç©ºæ®µè½
                        content += paragraph.text.strip() + "\n"
                        if len(content) >= max_length:
                            break
                
                # å¦‚æœæ®µè½å†…å®¹ä¸ºç©ºï¼Œå°è¯•æå–è¡¨æ ¼å†…å®¹
                if not content.strip():
                    for table in doc.tables:
                        for row in table.rows:
                            for cell in row.cells:
                                if cell.text.strip():
                                    content += cell.text.strip() + " "
                        content += "\n"
                        if len(content) >= max_length:
                            break
                
                return content[:max_length].strip() if content.strip() else "Wordæ–‡æ¡£å†…å®¹ä¸ºç©ºæˆ–æ— æ³•æå–"
                
            except Exception as docx_error:
                # å¦‚æœæ˜¯.docæ–‡ä»¶æˆ–æŸåçš„.docxæ–‡ä»¶ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
                if file_path.suffix.lower() == '.doc':
                    return "ä¸æ”¯æŒ.docæ ¼å¼ï¼Œè¯·è½¬æ¢ä¸º.docxæ ¼å¼"
                else:
                    return f"Wordæ–‡æ¡£æ ¼å¼é”™è¯¯æˆ–æ–‡ä»¶æŸå: {str(docx_error)}"
            
        except Exception as e:
            return f"Wordæ–‡æ¡£è¯»å–å¤±è´¥: {str(e)}"
    
    def _extract_text_content(self, file_path: Path, max_length: int) -> str:
        """
        æå–æ–‡æœ¬æ–‡ä»¶å†…å®¹ï¼ˆä»file_reader.pyå¤åˆ¶çš„å®Œæ•´å®ç°ï¼‰
        """
        try:
            # å°è¯•å¤šç§ç¼–ç æ ¼å¼
            encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1']
            
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding, errors='ignore') as f:
                        content = f.read(max_length)
                        
                        # æ£€æŸ¥å†…å®¹æ˜¯å¦å¯è¯»
                        printable_ratio = sum(1 for c in content if c.isprintable() or c.isspace()) / len(content) if content else 0
                        if printable_ratio > 0.7:
                            return content
                        
                except Exception:
                    continue
            
            return "æ–‡ä»¶å†…å®¹ä¸ºäºŒè¿›åˆ¶æ ¼å¼ï¼Œæ— æ³•è¯»å–"
            
        except Exception as e:
            return f"æ–‡æœ¬æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}"
    
    def _extract_image_info(self, file_path: Path) -> str:
        """
        æå–å›¾ç‰‡æ–‡ä»¶ä¿¡æ¯ï¼ˆä»file_reader.pyå¤åˆ¶çš„å®Œæ•´å®ç°ï¼‰
        """
        try:
            from PIL import Image
            # å…¼å®¹æ–°ç‰ˆæœ¬Pillow
            try:
                from PIL import ImageDraw, ImageFont
            except ImportError:
                pass
            with Image.open(file_path) as img:
                info = f"å›¾ç‰‡æ–‡ä»¶ä¿¡æ¯:\n"
                info += f"æ ¼å¼: {img.format}\n"
                info += f"å°ºå¯¸: {img.size[0]} x {img.size[1]}\n"
                info += f"æ¨¡å¼: {img.mode}\n"
                
                # å¦‚æœæœ‰EXIFä¿¡æ¯ï¼Œæå–ä¸€äº›åŸºæœ¬ä¿¡æ¯
                if hasattr(img, '_getexif') and img._getexif():
                    info += "åŒ…å«EXIFä¿¡æ¯\n"
                
                return info
                
        except Exception as e:
            return f"å›¾ç‰‡æ–‡ä»¶ä¿¡æ¯æå–å¤±è´¥: {str(e)}"
    
    def _generate_content_summary(self, content: str, file_name: str, summary_length: int = None) -> str:
        """
        ç”Ÿæˆæ–‡ä»¶å†…å®¹æ‘˜è¦
        
        Args:
            content: æ–‡ä»¶å†…å®¹
            file_name: æ–‡ä»¶å
            summary_length: æ‘˜è¦é•¿åº¦ï¼ˆå­—æ•°ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
        """
        try:
            if not content or content.startswith("æ— æ³•") or content.startswith("æ–‡ä»¶å†…å®¹ä¸ºäºŒè¿›åˆ¶"):
                return f"æ— æ³•ç”Ÿæˆæ‘˜è¦ï¼š{content[:50]}..."
            
            if not hasattr(self, 'ai_manager') or self.ai_manager is None:
                self.initialize_ollama()
            
            # ä½¿ç”¨ä¼ å…¥çš„æ‘˜è¦é•¿åº¦ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
            target_length = summary_length if summary_length is not None else self.ai_parameters['summary_length']
            
            # æ„å»ºæ›´æ˜ç¡®çš„æç¤ºè¯ï¼Œé¿å…æ€è€ƒè¿‡ç¨‹è¾“å‡º
            prompt = f"""è¯·ä¸ºä»¥ä¸‹æ–‡ä»¶å†…å®¹ç”Ÿæˆä¸€ä¸ª{target_length}å­—ä»¥å†…çš„ä¸­æ–‡æ‘˜è¦ã€‚

æ–‡ä»¶åï¼š{file_name}

æ–‡ä»¶å†…å®¹ï¼š
{content}

è¦æ±‚ï¼š
1. æ¦‚æ‹¬æ–‡ä»¶çš„ä¸»è¦å†…å®¹å’Œä¸»é¢˜
2. çªå‡ºå…³é”®ä¿¡æ¯å’Œè¦ç‚¹
3. è¯­è¨€ç®€æ´æ˜äº†
4. å­—æ•°æ§åˆ¶åœ¨{target_length}å­—ä»¥å†…
5. ç›´æ¥è¾“å‡ºæ‘˜è¦å†…å®¹ï¼Œä¸è¦åŒ…å«ä»»ä½•æ€è€ƒè¿‡ç¨‹æˆ–è¯´æ˜æ–‡å­—
6. ä¸è¦ä½¿ç”¨"<think>"æ ‡ç­¾æˆ–ä»»ä½•æ€è€ƒè¿‡ç¨‹æè¿°

æ‘˜è¦ï¼š/no_think"""

            # åœ¨ä¼ é€’ç»™å¤§æ¨¡å‹çš„å®Œæ•´å†…å®¹æœ€å°¾éƒ¨æ·»åŠ /no_thinkæ ‡ç­¾
            final_prompt = prompt

            # ä½¿ç”¨ç³»ç»Ÿæç¤ºè¯æ¥æŠ‘åˆ¶æ€è€ƒè¿‡ç¨‹
            messages = [
                {
                    'role': 'system',
                    'content': 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£æ‘˜è¦åŠ©æ‰‹ã€‚é‡è¦ï¼šä¸è¦è¾“å‡ºä»»ä½•æ¨ç†è¿‡ç¨‹ã€æ€è€ƒæ­¥éª¤æˆ–è§£é‡Šã€‚ç›´æ¥æŒ‰è¦æ±‚è¾“å‡ºç»“æœã€‚åªè¾“å‡ºæ‘˜è¦å†…å®¹ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–ä¿¡æ¯ã€‚'
                },
                {
                    'role': 'user',
                    'content': final_prompt
                }
            ]

            # ä½¿ç”¨ç»Ÿä¸€çš„AIç®¡ç†å™¨
            summary = chat_with_ai(messages)
            
            # æ¸…ç†å¯èƒ½çš„æ€è€ƒè¿‡ç¨‹æ ‡ç­¾å’Œå†…å®¹
            summary = summary.replace('<think>', '').replace('</think>', '').strip()
            
            # ç§»é™¤å¸¸è§çš„æ€è€ƒè¿‡ç¨‹å¼€å¤´
            think_prefixes = [
                'å¥½çš„ï¼Œ', 'å¥½ï¼Œ', 'å—¯ï¼Œ', 'æˆ‘æ¥', 'æˆ‘éœ€è¦', 'é¦–å…ˆï¼Œ', 'è®©æˆ‘', 'ç°åœ¨æˆ‘è¦',
                'ç”¨æˆ·å¸Œæœ›', 'ç”¨æˆ·è¦æ±‚', 'ç”¨æˆ·è®©æˆ‘', 'æ ¹æ®', 'åŸºäº', 'è€ƒè™‘åˆ°', 'è®©æˆ‘å…ˆä»”ç»†çœ‹çœ‹',
                'ç”¨æˆ·ç»™äº†æˆ‘è¿™ä¸ªæŸ¥è¯¢', 'ç”¨æˆ·ç»™äº†æˆ‘è¿™ä¸ªä»»åŠ¡', 'ç”¨æˆ·ç»™äº†ä¸€ä¸ªä»»åŠ¡',
                'é¦–å…ˆï¼Œæˆ‘å¾—çœ‹ä¸€ä¸‹', 'é¦–å…ˆï¼Œæˆ‘è¦ç†è§£', 'é¦–å…ˆï¼Œæˆ‘å¾—ä»”ç»†çœ‹çœ‹',
                'å¥½çš„ï¼Œç”¨æˆ·è®©æˆ‘', 'ç”¨æˆ·è®©æˆ‘ç”Ÿæˆ', 'å†…å®¹æ¥è‡ªæ–‡ä»¶', 'é‡ç‚¹åŒ…æ‹¬', 'é¦–å…ˆï¼Œæˆ‘éœ€è¦ç¡®è®¤'
            ]
            
            # æ›´æ¿€è¿›çš„æ¸…ç†ï¼šç§»é™¤æ‰€æœ‰ä»¥æ€è€ƒè¿‡ç¨‹å¼€å¤´çš„å¥å­
            lines = summary.split('\n')
            cleaned_lines = []
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦ä»¥æ€è€ƒè¿‡ç¨‹å¼€å¤´
                is_think_line = False
                for prefix in think_prefixes:
                    if line.lower().startswith(prefix.lower()):
                        is_think_line = True
                        break
                
                # å¦‚æœä¸æ˜¯æ€è€ƒè¿‡ç¨‹ï¼Œä¿ç•™è¿™ä¸€è¡Œ
                if not is_think_line:
                    cleaned_lines.append(line)
            
            # å¦‚æœæ¸…ç†åæ²¡æœ‰å†…å®¹ï¼Œå°è¯•æ›´ç®€å•çš„æ–¹æ³•
            if not cleaned_lines:
                # æ‰¾åˆ°ç¬¬ä¸€ä¸ªä¸æ˜¯æ€è€ƒè¿‡ç¨‹çš„å¥å­
                sentences = summary.split('ã€‚')
                for sentence in sentences:
                    sentence = sentence.strip()
                    if not sentence:
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦ä»¥æ€è€ƒè¿‡ç¨‹å¼€å¤´
                    is_think_sentence = False
                    for prefix in think_prefixes:
                        if sentence.lower().startswith(prefix.lower()):
                            is_think_sentence = True
                            break
                    
                    if not is_think_sentence:
                        cleaned_lines.append(sentence)
                        break
            
            # å¦‚æœè¿˜æ˜¯æ²¡æœ‰å†…å®¹ï¼Œä½¿ç”¨åŸå§‹æ‘˜è¦çš„æœ€åä¸€éƒ¨åˆ†
            if not cleaned_lines:
                # å–æœ€å100ä¸ªå­—ç¬¦ä½œä¸ºæ‘˜è¦
                summary = summary[-100:] if len(summary) > 100 else summary
            
            # é‡æ–°ç»„åˆæ¸…ç†åçš„å†…å®¹
            if cleaned_lines:
                summary = 'ã€‚'.join(cleaned_lines)
            
            # ç¡®ä¿æ‘˜è¦é•¿åº¦ä¸è¶…è¿‡é™åˆ¶
            if len(summary) > target_length:
                summary = summary[:target_length-3] + "..."
            
            return summary.strip()
            
        except Exception as e:
            logging.error(f"ç”Ÿæˆæ‘˜è¦å¤±è´¥: {e}")
            return f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def _clean_ai_response(self, response: str) -> str:
        """æ¸…ç†AIå“åº”ä¸­çš„æ€è€ƒè¿‡ç¨‹"""
        if not response:
            return response
        
        print(f"ğŸ”§ å¼€å§‹æ¸…ç†AIå“åº”ï¼ŒåŸå§‹é•¿åº¦: {len(response)}")
        print(f"ğŸ”§ åŸå§‹å“åº”: {response}")
        
        # æ¸…ç†å¯èƒ½çš„æ€è€ƒè¿‡ç¨‹æ ‡ç­¾å’Œå†…å®¹
        response = response.replace('<think>', '').replace('</think>', '').strip()
        
        # ç§»é™¤å¸¸è§çš„æ€è€ƒè¿‡ç¨‹å¼€å¤´
        think_prefixes = [
            'å¥½çš„ï¼Œ', 'å¥½ï¼Œ', 'å—¯ï¼Œ', 'æˆ‘æ¥', 'æˆ‘éœ€è¦', 'é¦–å…ˆï¼Œ', 'è®©æˆ‘', 'ç°åœ¨æˆ‘è¦',
            'ç”¨æˆ·å¸Œæœ›', 'ç”¨æˆ·è¦æ±‚', 'ç”¨æˆ·è®©æˆ‘', 'æ ¹æ®', 'åŸºäº', 'è€ƒè™‘åˆ°', 'è®©æˆ‘å…ˆä»”ç»†çœ‹çœ‹',
            'ç”¨æˆ·ç»™äº†æˆ‘è¿™ä¸ªæŸ¥è¯¢', 'ç”¨æˆ·ç»™äº†æˆ‘è¿™ä¸ªä»»åŠ¡', 'ç”¨æˆ·ç»™äº†ä¸€ä¸ªä»»åŠ¡',
            'é¦–å…ˆï¼Œæˆ‘å¾—çœ‹ä¸€ä¸‹', 'é¦–å…ˆï¼Œæˆ‘è¦ç†è§£', 'é¦–å…ˆï¼Œæˆ‘å¾—ä»”ç»†çœ‹çœ‹',
            'å¥½çš„ï¼Œç”¨æˆ·è®©æˆ‘', 'ç”¨æˆ·è®©æˆ‘ç”Ÿæˆ', 'å†…å®¹æ¥è‡ªæ–‡ä»¶', 'é‡ç‚¹åŒ…æ‹¬', 'é¦–å…ˆï¼Œæˆ‘éœ€è¦ç¡®è®¤'
        ]
        
        # æ›´æ¿€è¿›çš„æ¸…ç†ï¼šç§»é™¤æ‰€æœ‰ä»¥æ€è€ƒè¿‡ç¨‹å¼€å¤´çš„å¥å­
        lines = response.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # æ£€æŸ¥æ˜¯å¦ä»¥æ€è€ƒè¿‡ç¨‹å¼€å¤´
            is_think_line = False
            for prefix in think_prefixes:
                if line.lower().startswith(prefix.lower()):
                    is_think_line = True
                    break
            
            # å¦‚æœä¸æ˜¯æ€è€ƒè¿‡ç¨‹ï¼Œä¿ç•™è¿™ä¸€è¡Œ
            if not is_think_line:
                cleaned_lines.append(line)
        
        print(f"ğŸ”§ æ¸…ç†åè¡Œæ•°: {len(cleaned_lines)}")
        
        # å¦‚æœæ¸…ç†åæ²¡æœ‰å†…å®¹ï¼Œå°è¯•æ›´ç®€å•çš„æ–¹æ³•
        if not cleaned_lines:
            print(f"ğŸ”§ æ¸…ç†åæ²¡æœ‰å†…å®¹ï¼Œå°è¯•å¥å­åˆ†å‰²æ–¹æ³•")
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªä¸æ˜¯æ€è€ƒè¿‡ç¨‹çš„å¥å­
            sentences = response.split('ã€‚')
            for sentence in sentences:
                sentence = sentence.strip()
                if not sentence:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦ä»¥æ€è€ƒè¿‡ç¨‹å¼€å¤´
                is_think_sentence = False
                for prefix in think_prefixes:
                    if sentence.lower().startswith(prefix.lower()):
                        is_think_sentence = True
                        break
                
                if not is_think_sentence:
                    cleaned_lines.append(sentence)
                    break
        
        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰å†…å®¹ï¼Œä½¿ç”¨åŸå§‹å“åº”çš„æœ€åä¸€éƒ¨åˆ†
        if not cleaned_lines:
            print(f"ğŸ”§ ä»ç„¶æ²¡æœ‰å†…å®¹ï¼Œä½¿ç”¨æœ€å100ä¸ªå­—ç¬¦")
            # å–æœ€å100ä¸ªå­—ç¬¦
            response = response[-100:] if len(response) > 100 else response
            print(f"ğŸ”§ æˆªæ–­åå“åº”: {response}")
            return response
        
        # é‡æ–°ç»„åˆæ¸…ç†åçš„å†…å®¹ï¼Œä¿æŒæ¢è¡Œç¬¦ä»¥æ”¯æŒå¤šè¡Œæ¨èè·¯å¾„
        if cleaned_lines:
            response = '\n'.join(cleaned_lines)
        
        print(f"ğŸ”§ æœ€ç»ˆæ¸…ç†ç»“æœ: {response}")
        return response.strip()
    
    def _get_level_directories(self, base_directory: str, level: int = 1) -> List[str]:
        """
        è·å–æŒ‡å®šå±‚çº§çš„æ‰€æœ‰ç›®å½•
        
        Args:
            base_directory: åŸºç¡€ç›®å½•
            level: å±‚çº§ï¼ˆ1è¡¨ç¤ºä¸€çº§ç›®å½•ï¼Œ2è¡¨ç¤ºäºŒçº§ç›®å½•ç­‰ï¼‰
            
        Returns:
            è¯¥å±‚çº§çš„æ‰€æœ‰ç›®å½•ååˆ—è¡¨
        """
        try:
            base_path = Path(base_directory)
            if not base_path.exists():
                return []
            
            directories = []
            
            if level == 1:
                # è·å–ä¸€çº§ç›®å½•
                for item in base_path.iterdir():
                    if item.is_dir():
                        directories.append(item.name)
            else:
                # è·å–æŒ‡å®šå±‚çº§ç›®å½•
                for item in base_path.rglob('*'):
                    if item.is_dir():
                        # è®¡ç®—ç›¸å¯¹è·¯å¾„çš„å±‚çº§
                        relative_path = item.relative_to(base_path)
                        path_parts = relative_path.parts
                        if len(path_parts) == level:
                            directories.append(item.name)
            
            return directories
            
        except Exception as e:
            logging.error(f"è·å–{level}çº§ç›®å½•å¤±è´¥: {e}")
            return []

    def _match_level_directory(self, file_info: Dict[str, Any], content: str, summary: str, 
                              base_directory: str, current_path: str, level: int) -> Tuple[str, str]:
        """
        åŒ¹é…æŒ‡å®šå±‚çº§çš„ç›®å½•
        
        Args:
            file_info: æ–‡ä»¶ä¿¡æ¯å­—å…¸
            content: æ–‡ä»¶å†…å®¹
            summary: æ–‡ä»¶æ‘˜è¦
            base_directory: åŸºç¡€ç›®å½•
            current_path: å½“å‰å·²åŒ¹é…çš„è·¯å¾„
            level: å½“å‰åŒ¹é…çš„å±‚çº§
            
        Returns:
            (åŒ¹é…çš„ç›®å½•å, åŒ¹é…ç†ç”±)
        """
        try:
            # è·å–å½“å‰å±‚çº§çš„æ‰€æœ‰ç›®å½•
            level_dirs = self._get_level_directories(base_directory, level)
            if not level_dirs:
                return "", "è¯¥å±‚çº§æ²¡æœ‰å­ç›®å½•"
            
            file_name = file_info['file_name']
            file_extension = file_info['file_extension']
            
            # è·å–ç”¨æˆ·è‡ªå®šä¹‰åˆ†ç±»è§„åˆ™
            custom_rules = self.classification_rules_manager.get_rules_for_prompt(level_dirs)
            
            # æ„å»ºåŒ¹é…æç¤ºè¯
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡ä»¶åˆ†ç±»ä¸“å®¶ã€‚è¯·æ ¹æ®æ–‡ä»¶ä¿¡æ¯åœ¨å½“å‰å±‚çº§ç›®å½•ä¸­é€‰æ‹©æœ€åŒ¹é…çš„ä¸€ä¸ªã€‚

æ–‡ä»¶ä¿¡æ¯ï¼š
- æ–‡ä»¶åï¼š{file_name}
- æ–‡ä»¶æ‰©å±•åï¼š{file_extension}
- å†…å®¹æ‘˜è¦ï¼š{summary[:200] if summary else 'æ— æ‘˜è¦'}
- å½“å‰è·¯å¾„ï¼š{current_path if current_path else 'æ ¹ç›®å½•'}

å½“å‰å±‚çº§å¯é€‰ç›®å½•ï¼ˆå¿…é¡»ä¸¥æ ¼ä»ä»¥ä¸‹åˆ—è¡¨ä¸­é€‰æ‹©ä¸€ä¸ªï¼‰ï¼š
{chr(10).join(f"{i+1}. {dir_name}" for i, dir_name in enumerate(level_dirs))}

{custom_rules}

åŒ¹é…ä¼˜å…ˆçº§ï¼š
1. æœ€é«˜ä¼˜å…ˆçº§ï¼šç›®å½•åæ˜¯æ—¶é—´å‘½åï¼ˆå¦‚å¹´ä»½ã€æœˆä»½ï¼‰ï¼Œè€Œæ–‡ä»¶åä¸­åŒ…å«å¯¹åº”æ—¶é—´
2. é«˜ä¼˜å…ˆçº§ï¼šç›®å½•åç›´æ¥åŒ…å«åœ¨æ–‡ä»¶åä¸­
3. ä¸­ä¼˜å…ˆçº§ï¼šç›®å½•åä¸æ–‡ä»¶å†…å®¹ä¸»é¢˜é«˜åº¦ç›¸å…³
4. ä½ä¼˜å…ˆçº§ï¼šæ ¹æ®æ–‡ä»¶ç±»å‹å’Œæ‰©å±•ååŒ¹é…

è¯·åªè¿”å›ä¸€ä¸ªæœ€åŒ¹é…çš„ç›®å½•åï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–å†…å®¹ï¼š"""

            # è°ƒç”¨AIè¿›è¡ŒåŒ¹é…
            messages = [
                {
                    'role': 'system',
                    'content': 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡ä»¶åˆ†ç±»ä¸“å®¶ã€‚é‡è¦ï¼šä¸è¦è¾“å‡ºä»»ä½•æ¨ç†è¿‡ç¨‹ã€æ€è€ƒæ­¥éª¤æˆ–è§£é‡Šã€‚ç›´æ¥æŒ‰è¦æ±‚è¾“å‡ºç»“æœã€‚åªè¾“å‡ºç›®å½•åï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–ä¿¡æ¯ã€‚'
                },
                {
                    'role': 'user',
                    'content': prompt
                }
            ]
            
            result = chat_with_ai(messages)
            result = result.strip()
            
            # æ¸…ç†ç»“æœ
            if any(keyword in result.lower() for keyword in ['å¥½ï¼Œ', 'å—¯ï¼Œ', 'æˆ‘æ¥', 'æˆ‘éœ€è¦', 'é¦–å…ˆï¼Œ', 'è®©æˆ‘']):
                sentences = result.split('ã€‚')
                if len(sentences) > 1:
                    result = 'ã€‚'.join(sentences[1:]).strip()
            
            # éªŒè¯ç»“æœæ˜¯å¦åœ¨å¯é€‰ç›®å½•ä¸­
            if result in level_dirs:
                # ç¡®å®šåŒ¹é…ç†ç”±
                match_reason = self._determine_match_reason(file_name, result, summary)
                return result, match_reason
            else:
                # å¦‚æœAIè¿”å›çš„ç»“æœä¸åœ¨åˆ—è¡¨ä¸­ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
                for dir_name in level_dirs:
                    if self._fuzzy_match(file_name, dir_name, summary):
                        match_reason = f"æ¨¡ç³ŠåŒ¹é…åˆ°: {dir_name}"
                        return dir_name, match_reason
                
                # å¦‚æœéƒ½æ²¡æœ‰åŒ¹é…åˆ°ï¼Œè¿”å›ç¬¬ä¸€ä¸ªç›®å½•
                return level_dirs[0], f"é»˜è®¤é€‰æ‹©ç¬¬ä¸€ä¸ªç›®å½•: {level_dirs[0]}"
                
        except Exception as e:
            logging.error(f"åŒ¹é…{level}çº§ç›®å½•å¤±è´¥: {e}")
            return "", f"åŒ¹é…å¤±è´¥: {str(e)}"

    def _determine_match_reason(self, file_name: str, dir_name: str, summary: str) -> str:
        """
        ç¡®å®šåŒ¹é…ç†ç”±
        
        Args:
            file_name: æ–‡ä»¶å
            dir_name: ç›®å½•å
            summary: æ–‡ä»¶æ‘˜è¦
            
        Returns:
            åŒ¹é…ç†ç”±
        """
        # æ£€æŸ¥æ—¶é—´åŒ¹é…
        if self._is_time_match(file_name, dir_name):
            return f"æ—¶é—´åŒ¹é…: æ–‡ä»¶ååŒ…å«æ—¶é—´ä¿¡æ¯ï¼ŒåŒ¹é…åˆ°æ—¶é—´ç›®å½• {dir_name}"
        
        # æ£€æŸ¥æ–‡ä»¶ååŒ…å«ç›®å½•å
        if dir_name.lower() in file_name.lower():
            return f"æ–‡ä»¶ååŒ¹é…: æ–‡ä»¶ååŒ…å«ç›®å½•å {dir_name}"
        
        # æ£€æŸ¥å†…å®¹ä¸»é¢˜åŒ¹é…
        if summary and any(keyword in summary.lower() for keyword in dir_name.lower().split()):
            return f"å†…å®¹ä¸»é¢˜åŒ¹é…: æ–‡ä»¶å†…å®¹ä¸ç›®å½• {dir_name} ä¸»é¢˜ç›¸å…³"
        
        return f"AIæ™ºèƒ½åˆ†ç±»: æ ¹æ®æ–‡ä»¶å†…å®¹åŒ¹é…åˆ° {dir_name}"

    def _is_time_match(self, file_name: str, dir_name: str) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦ä¸ºæ—¶é—´åŒ¹é…
        
        Args:
            file_name: æ–‡ä»¶å
            dir_name: ç›®å½•å
            
        Returns:
            æ˜¯å¦ä¸ºæ—¶é—´åŒ¹é…
        """
        # æå–æ–‡ä»¶åä¸­çš„å¹´ä»½
        year_pattern = r'\b(19|20)\d{2}\b'
        file_years = re.findall(year_pattern, file_name)
        
        # æ£€æŸ¥ç›®å½•åæ˜¯å¦åŒ…å«å¹´ä»½
        dir_years = re.findall(year_pattern, dir_name)
        
        if file_years and dir_years:
            return any(fy in dy for fy in file_years for dy in dir_years)
        
        return False

    def _fuzzy_match(self, file_name: str, dir_name: str, summary: str) -> bool:
        """
        æ¨¡ç³ŠåŒ¹é…
        
        Args:
            file_name: æ–‡ä»¶å
            dir_name: ç›®å½•å
            summary: æ–‡ä»¶æ‘˜è¦
            
        Returns:
            æ˜¯å¦æ¨¡ç³ŠåŒ¹é…
        """
        # ç®€å•çš„å…³é”®è¯åŒ¹é…
        file_keywords = set(file_name.lower().split())
        dir_keywords = set(dir_name.lower().split())
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å…±åŒå…³é”®è¯
        common_keywords = file_keywords.intersection(dir_keywords)
        if len(common_keywords) > 0:
            return True
        
        # æ£€æŸ¥æ‘˜è¦ä¸­çš„å…³é”®è¯
        if summary:
            summary_keywords = set(summary.lower().split())
            summary_dir_common = summary_keywords.intersection(dir_keywords)
            if len(summary_dir_common) > 0:
                return True
        
        return False

    def _recommend_target_folder_recursive(self, file_path: str, content: str, summary: str, 
                                         target_directory: str, retry_count: int = 0) -> Tuple[str, List[str], str]:
        """
        é€’å½’é€å±‚åŒ¹é…ç›®æ ‡æ–‡ä»¶å¤¹
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            content: æ–‡ä»¶å†…å®¹
            summary: æ–‡ä»¶æ‘˜è¦
            target_directory: ç›®æ ‡ç›®å½•
            retry_count: é‡è¯•æ¬¡æ•°
            
        Returns:
            (å®Œæ•´åŒ¹é…è·¯å¾„, å„çº§æ ‡ç­¾åˆ—è¡¨, åŒ¹é…ç†ç”±)
        """
        try:
            file_info = self.file_cache.get(file_path, {})
            if not file_info:
                file_info = self._extract_file_metadata(file_path)
                self.file_cache[file_path] = file_info
            
            current_path = ""
            full_path = ""
            level_tags = []
            match_reasons = []
            
            level = 1
            max_levels = 10  # é˜²æ­¢æ— é™é€’å½’
            
            while level <= max_levels:
                # æ„å»ºå½“å‰å±‚çº§çš„å®Œæ•´è·¯å¾„
                if current_path:
                    current_full_path = os.path.join(target_directory, current_path)
                else:
                    current_full_path = target_directory
                
                # åŒ¹é…å½“å‰å±‚çº§
                matched_dir, match_reason = self._match_level_directory(
                    file_info, content, summary, current_full_path, current_path, level
                )
                
                if not matched_dir:
                    break
                
                # æ›´æ–°è·¯å¾„å’Œæ ‡ç­¾
                if current_path:
                    current_path = os.path.join(current_path, matched_dir)
                else:
                    current_path = matched_dir
                
                full_path = current_path
                level_tags.append(matched_dir)
                match_reasons.append(f"ç¬¬{level}çº§: {match_reason}")
                
                # æ£€æŸ¥ä¸‹ä¸€çº§æ˜¯å¦æœ‰å­ç›®å½•
                next_level_path = os.path.join(current_full_path, matched_dir)
                next_level_dirs = self._get_level_directories(next_level_path, 1)
                
                if not next_level_dirs:
                    break
                
                level += 1
            
            # ç¼“å­˜æ ‡ç­¾ä¿¡æ¯
            self.level_tags[file_path] = level_tags
            
            # åˆå¹¶åŒ¹é…ç†ç”±
            combined_reason = "; ".join(match_reasons)
            
            return full_path, level_tags, combined_reason
            
        except Exception as e:
            logging.error(f"é€’å½’åŒ¹é…ç›®æ ‡æ–‡ä»¶å¤¹å¤±è´¥: {e}")
            return "", [], f"åŒ¹é…å¤±è´¥: {str(e)}"

    def _clear_file_cache(self, file_path: str) -> None:
        """
        æ¸…ç†æŒ‡å®šæ–‡ä»¶çš„ç¼“å­˜ï¼Œä½†ä¿ç•™å…¨å±€ç¼“å­˜
        
        Args:
            file_path: è¦æ¸…ç†ç¼“å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        try:
            # æ¸…ç†æ–‡ä»¶ç¼“å­˜
            if file_path in self.file_cache:
                del self.file_cache[file_path]
            
            # æ¸…ç†æ ‡ç­¾ç¼“å­˜
            if file_path in self.level_tags:
                del self.level_tags[file_path]
                
            logging.debug(f"å·²æ¸…ç†æ–‡ä»¶ç¼“å­˜: {file_path}")
            
        except Exception as e:
            logging.error(f"æ¸…ç†æ–‡ä»¶ç¼“å­˜å¤±è´¥: {e}")

    def _recommend_target_folder(self, file_path: str, content: str, summary: str, target_directory: str, retry_count: int = 0) -> tuple:
        """
        åŸºäºæ–‡ä»¶å†…å®¹å’Œæ‘˜è¦æ¨èæœ€åŒ¹é…çš„ç›®æ ‡æ–‡ä»¶å¤¹
        ä½¿ç”¨æ–°çš„é€’å½’é€å±‚åŒ¹é…é€»è¾‘
        """
        try:
            file_name = Path(file_path).name
            file_full_path = str(Path(file_path).absolute())
            
            print(f"ğŸ“„ æºæ–‡ä»¶å®Œæ•´è·¯å¾„: {file_full_path}")
            if retry_count > 0:
                print(f"ğŸ”„ ç¬¬ {retry_count} æ¬¡é‡è¯•AIåˆ†ç±»")
            
            # å­˜å‚¨æºæ–‡ä»¶å®Œæ•´è·¯å¾„
            self.source_file_path = file_full_path
            
            # ä½¿ç”¨æ–°çš„é€’å½’åŒ¹é…é€»è¾‘
            recommended_folder, level_tags, match_reason = self._recommend_target_folder_recursive(
                file_path, content, summary, target_directory, retry_count
            )
            
            # æ£€æŸ¥åˆ†ç±»è´¨é‡ï¼šå¦‚æœæ¨èçš„æ˜¯è¿‡äºå®½æ³›çš„åˆ†ç±»ï¼Œå°è¯•é‡æ–°åˆ†ç±»
            if recommended_folder and retry_count < 2:
                # æ£€æŸ¥è·¯å¾„æ·±åº¦ï¼Œå¦‚æœå¤ªæµ…å¯èƒ½æ˜¯è¿‡äºå®½æ³›çš„åˆ†ç±»
                path_depth = len([p for p in recommended_folder.split('\\') + recommended_folder.split('/') if p.strip()])
                if path_depth <= 1:  # è·¯å¾„æ·±åº¦å°äºç­‰äº1å¯èƒ½æ˜¯è¿‡äºå®½æ³›
                    print(f"âš ï¸  AIæ¨èäº†è¿‡äºå®½æ³›çš„åˆ†ç±»: {recommended_folder}ï¼Œå‡†å¤‡ç¬¬ {retry_count + 1} æ¬¡é‡è¯•...")
                    logging.warning(f"AIæ¨èäº†è¿‡äºå®½æ³›çš„åˆ†ç±»: {recommended_folder}ï¼Œå‡†å¤‡ç¬¬ {retry_count + 1} æ¬¡é‡è¯•")
                    
                    # åœ¨é‡è¯•æ—¶å¼ºè°ƒè¦é€‰æ‹©æ›´å…·ä½“çš„åˆ†ç±»
                    return self._recommend_target_folder(file_path, content, summary, target_directory, retry_count + 1)
            
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…å¤±è´¥ï¼Œå¦‚æœæ˜¯ä¸”æœªè¶…è¿‡é‡è¯•æ¬¡æ•°ï¼Œåˆ™é‡è¯•
            if not recommended_folder and retry_count < 2:  # æœ€å¤šé‡è¯•2æ¬¡
                print(f"âš ï¸  AIåˆ†ç±»å¤±è´¥ï¼Œå‡†å¤‡ç¬¬ {retry_count + 1} æ¬¡é‡è¯•...")
                logging.warning(f"AIåˆ†ç±»å¤±è´¥ï¼Œå‡†å¤‡ç¬¬ {retry_count + 1} æ¬¡é‡è¯•")
                
                # é€’å½’è°ƒç”¨è‡ªèº«è¿›è¡Œé‡è¯•
                return self._recommend_target_folder(file_path, content, summary, target_directory, retry_count + 1)
            
            return recommended_folder, match_reason
            
        except Exception as e:
            logging.error(f"æ¨èç›®æ ‡æ–‡ä»¶å¤¹å¤±è´¥: {e}")
            return None, f"æ¨èå¤±è´¥: {str(e)}"
    def _extract_recommended_paths(self, ai_result: str) -> List[str]:
        """
        ä»AIè¿”å›ç»“æœä¸­æå–ä¸‰ä¸ªæ¨èè·¯å¾„
        
        Args:
            ai_result: AIè¿”å›çš„åŸå§‹ç»“æœ
            
        Returns:
            List[str]: æå–åˆ°çš„æ¨èè·¯å¾„åˆ—è¡¨
        """
        recommended_paths = []
        
        try:
            lines = ai_result.strip().split('\n')
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                    
                # æŸ¥æ‰¾åŒ…å«æ¨èè·¯å¾„çš„è¡Œ
                if any(keyword in line for keyword in ['ç¬¬ä¸€æ¨èï¼š', 'ç¬¬äºŒæ¨èï¼š', 'ç¬¬ä¸‰æ¨èï¼š', 'æ¨èï¼š']):
                    # æå–å†’å·åçš„è·¯å¾„
                    if 'ï¼š' in line:
                        path = line.split('ï¼š', 1)[1].strip()
                        if path and path not in recommended_paths:
                            recommended_paths.append(path)
                elif line.startswith('ã€') and 'ã€‘' in line:
                    # ç›´æ¥è¯†åˆ«ä»¥ã€å¼€å¤´çš„è·¯å¾„æ ¼å¼
                    if line not in recommended_paths:
                        recommended_paths.append(line)
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ¼å¼åŒ–çš„æ¨èï¼Œå°è¯•æŒ‰è¡Œæå–å‰ä¸‰ä¸ªæœ‰æ•ˆè·¯å¾„
            if not recommended_paths:
                for line in lines:
                    line = line.strip()
                    if line and line.startswith('ã€') and 'ã€‘' in line:
                        if line not in recommended_paths:
                            recommended_paths.append(line)
                        if len(recommended_paths) >= 3:
                            break
            
            # é™åˆ¶æœ€å¤šè¿”å›3ä¸ªè·¯å¾„
            return recommended_paths[:3]
            
        except Exception as e:
            logging.error(f"æå–æ¨èè·¯å¾„æ—¶å‡ºé”™: {e}")
            return []
    
    def _parse_classification_result(self, result: str, target_directory: str, file_content: str = None, file_summary: str = None) -> tuple:
        """
        è§£æAIåˆ†ç±»ç»“æœï¼Œæ”¯æŒç›¸å…³æ€§è¯„åˆ†æ’åºå’Œä¸‰ä¸ªæ¨èè·¯å¾„çš„ä¾æ¬¡éªŒè¯
        
        Args:
            result: AIè¿”å›çš„åˆ†ç±»ç»“æœ
            target_directory: ç›®æ ‡ç›®å½•è·¯å¾„
            file_content: æ–‡ä»¶å†…å®¹ï¼ˆç”¨äºç›¸å…³æ€§è®¡ç®—ï¼‰
            file_summary: æ–‡ä»¶æ‘˜è¦ï¼ˆç”¨äºç›¸å…³æ€§è®¡ç®—ï¼‰
            
        Returns:
            tuple: (æ¨èæ–‡ä»¶å¤¹è·¯å¾„, åŒ¹é…ç†ç”±)
        """
        try:
            result = result.strip()
            logging.info(f"æ­£åœ¨è§£æAIåˆ†ç±»ç»“æœ: {result[:100]}...")
            
            print(f"ğŸ” è§£æAIåˆ†ç±»ç»“æœ: {result}")
            
            # æ¸…ç†AIè¿”å›ç»“æœä¸­çš„æ€è€ƒè¿‡ç¨‹æ ‡ç­¾å’Œå†…å®¹
            result_clean = self._clean_ai_response(result)
            logging.info(f"æ¸…ç†åçš„åˆ†ç±»ç»“æœ: {result_clean[:100]}...")
            print(f"ğŸ§¹ æ¸…ç†åçš„ç»“æœ: {result_clean}")
            
            # æå–ä¸‰ä¸ªæ¨èè·¯å¾„
            recommended_paths = self._extract_recommended_paths(result_clean)
            
            if not recommended_paths:
                logging.warning(f"æ— æ³•ä»AIç»“æœä¸­æå–æ¨èè·¯å¾„")
                print(f"âš ï¸ æ— æ³•æå–æ¨èè·¯å¾„")
                return None, f"AIæ™ºèƒ½åˆ†ç±»å¤±è´¥ï¼Œæ— æ³•è§£ææ¨èè·¯å¾„: {result_clean[:50]}..."
            
            print(f"ğŸ“‹ æå–åˆ° {len(recommended_paths)} ä¸ªæ¨èè·¯å¾„: {recommended_paths}")
            
            # å¦‚æœæœ‰æ–‡ä»¶å†…å®¹å’Œæ‘˜è¦ï¼Œè¿›è¡Œç›¸å…³æ€§åˆ†æå’Œæ’åº
            if file_content and file_summary:
                print(f"ğŸ” å¼€å§‹ç›¸å…³æ€§åˆ†æå’Œæ’åº...")
                sorted_paths = self._filter_irrelevant_folders(file_content, file_summary, recommended_paths, target_directory)
                
                if sorted_paths:
                    recommended_paths = sorted_paths
                    print(f"âœ… ç›¸å…³æ€§æ’åºå®Œæˆï¼ŒæŒ‰è¯„åˆ†æ’åºåçš„è·¯å¾„: {sorted_paths}")
                else:
                    print(f"âš ï¸ ç›¸å…³æ€§æ’åºåæ— ç›¸å…³è·¯å¾„ï¼Œä½¿ç”¨åŸå§‹æ¨è")
            
            # ä¾æ¬¡éªŒè¯æ¯ä¸ªæ¨èè·¯å¾„ï¼ˆç°åœ¨æŒ‰ç›¸å…³æ€§è¯„åˆ†æ’åºï¼‰
            for i, recommended_path in enumerate(recommended_paths, 1):
                target_path = Path(target_directory) / recommended_path
                
                print(f"ğŸ” éªŒè¯ç¬¬{i}ä¸ªæ¨èè·¯å¾„: {recommended_path}")
                print(f"ğŸ¯ å®Œæ•´ç›®æ ‡è·¯å¾„: {target_path}")
                
                # éªŒè¯è·¯å¾„æ˜¯å¦å­˜åœ¨ä¸”ä¸ºç›®å½•
                if target_path.exists() and target_path.is_dir():
                    logging.info(f"æ‰¾åˆ°æœ‰æ•ˆçš„ç›®æ ‡æ–‡ä»¶å¤¹(ç¬¬{i}é€‰æ‹©): {recommended_path}")
                    print(f"âœ… æ‰¾åˆ°æœ‰æ•ˆç›®æ ‡æ–‡ä»¶å¤¹(ç¬¬{i}é€‰æ‹©): {recommended_path}")
                    
                    # å­˜å‚¨æ¨èçš„å®Œæ•´è·¯å¾„
                    self.recommended_folder_path = recommended_path
                    print(f"ğŸ’¾ å­˜å‚¨æ¨èè·¯å¾„: {self.recommended_folder_path}")
                    
                    # ç”Ÿæˆæœ‰æ„ä¹‰çš„åŒ¹é…ç†ç”±
                    match_reason = f"AIæ™ºèƒ½åˆ†ç±»(ç¬¬{i}é€‰æ‹©)ï¼šæ ¹æ®æ–‡ä»¶å†…å®¹åŒ¹é…åˆ° {recommended_path}"
                    
                    return recommended_path, match_reason
                else:
                    logging.warning(f"ç¬¬{i}ä¸ªæ¨èè·¯å¾„ä¸å­˜åœ¨: {target_path}")
                    print(f"âŒ ç¬¬{i}ä¸ªæ¨èè·¯å¾„ä¸å­˜åœ¨: {recommended_path}")
            
            # å¦‚æœæ‰€æœ‰æ¨èè·¯å¾„éƒ½æ— æ•ˆ
            logging.error(f"æ‰€æœ‰æ¨èè·¯å¾„éƒ½æ— æ•ˆ: {recommended_paths}")
            print(f"ğŸ’¥ æ‰€æœ‰æ¨èè·¯å¾„éƒ½æ— æ•ˆ")
            
            return None, f"AIæ™ºèƒ½åˆ†ç±»å¤±è´¥ï¼Œä¸‰ä¸ªæ¨èè·¯å¾„éƒ½ä¸å­˜åœ¨: {', '.join(recommended_paths[:3])}"
            
        except Exception as e:
            logging.error(f"è§£æåˆ†ç±»ç»“æœå¤±è´¥: {e}, åŸå§‹ç»“æœ: {result}")
            return None, f"è§£æåˆ†ç±»ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
    def scan_source_files(self, source_directory: str) -> List[str]:
        try:
            source_path = Path(source_directory)
            if not source_path.exists():
                raise FileOrganizerError(f"æºç›®å½•ä¸å­˜åœ¨: {source_directory}")
            files = []
            for item in source_path.rglob('*'):
                if item.is_file():
                    files.append(str(item))
            logging.info(f"æ‰«æåˆ° {len(files)} ä¸ªå¾…æ•´ç†æ–‡ä»¶")
            return files
        except Exception as e:
            raise FileOrganizerError(f"æ‰«ææºæ–‡ä»¶å¤±è´¥: {e}")
    def scan_files(self, source_directory: str) -> List[Dict[str, object]]:
        try:
            source_path = Path(source_directory)
            if not source_path.exists():
                raise FileOrganizerError(f"æºç›®å½•ä¸å­˜åœ¨: {source_directory}")
            files = []
            for item in source_path.rglob('*'):
                if item.is_file():
                    files.append({
                        'name': item.name,
                        'path': str(item),
                        'size': item.stat().st_size,
                        'extension': item.suffix.lower()
                    })
            logging.info(f"æ‰«æåˆ° {len(files)} ä¸ªå¾…æ•´ç†æ–‡ä»¶")
            return files
        except Exception as e:
            raise FileOrganizerError(f"æ‰«ææºæ–‡ä»¶å¤±è´¥: {e}")

    def preview_classification(self, source_directory: str, target_directory: str) -> List[Dict[str, object]]:
        try:
            source_files = self.scan_source_files(source_directory)
            if not source_files:
                raise FileOrganizerError("æºç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶")
            preview_results = []
            logging.info(f"å¼€å§‹é¢„è§ˆåˆ†ç±»ï¼Œå…± {len(source_files)} ä¸ªæ–‡ä»¶")
            preview_count = len(source_files)
            for i, file_path in enumerate(source_files, 1):
                file_name = Path(file_path).name
                try:
                    logging.info(f"æ­£åœ¨å¤„ç†æ–‡ä»¶ {i}/{preview_count}: {file_name}")
                    print(f"\n=== å¤„ç†æ–‡ä»¶ {i}/{preview_count}: {file_name} ===")
                    # ä½¿ç”¨æ–°çš„åˆ†ææ–¹æ³•
                    analysis_result = self.analyze_and_classify_file(file_path, target_directory)
                    
                    if analysis_result['success']:
                        timing_info = analysis_result.get('timing_info', {})
                        preview_results.append({
                            'file_path': file_path,
                            'file_name': file_name,
                            'target_folder': analysis_result['recommended_folder'],
                            'match_reason': analysis_result['match_reason'],
                            'classification_method': 'AI_Enhanced',
                            'success': True,
                            'extracted_content': analysis_result['extracted_content'][:200] + "..." if len(analysis_result['extracted_content']) > 200 else analysis_result['extracted_content'],
                            'content_summary': analysis_result['content_summary'],
                            'timing_info': timing_info
                        })
                        total_time = timing_info.get('total_processing_time', 0)
                        extract_time = timing_info.get('content_extraction_time', 0)
                        summary_time = timing_info.get('summary_generation_time', 0)
                        recommend_time = timing_info.get('folder_recommendation_time', 0)
                        
                        logging.info(f"æ–‡ä»¶ {file_name} åˆ†ææˆåŠŸ: {analysis_result['recommended_folder']} ({analysis_result['match_reason']})ï¼Œæ€»è€—æ—¶: {total_time}ç§’")
                        logging.info(f"å†…å®¹æ‘˜è¦: {analysis_result['content_summary']}")
                        logging.info(f"è¯¦ç»†è€—æ—¶ - å†…å®¹æå–: {extract_time}ç§’, æ‘˜è¦ç”Ÿæˆ: {summary_time}ç§’, ç›®å½•æ¨è: {recommend_time}ç§’")
                    else:
                        timing_info = analysis_result.get('timing_info', {})
                        preview_results.append({
                            'file_path': file_path,
                            'file_name': file_name,
                            'target_folder': None,
                            'match_reason': analysis_result['match_reason'],
                            'classification_method': 'Failed',
                            'success': False,
                            'error': analysis_result.get('error', analysis_result['match_reason']),
                            'extracted_content': analysis_result.get('extracted_content', ''),
                            'content_summary': analysis_result.get('content_summary', ''),
                            'timing_info': timing_info
                        })
                        total_time = timing_info.get('total_processing_time', 0)
                        logging.warning(f"æ–‡ä»¶ {file_name} åˆ†æå¤±è´¥: {analysis_result['match_reason']}ï¼Œæ€»è€—æ—¶: {total_time}ç§’")
                except Exception as e:
                    error_msg = f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}"
                    logging.error(f"æ–‡ä»¶ {file_name} å¤„ç†å¼‚å¸¸: {e}")
                    preview_results.append({
                        'file_path': file_path,
                        'file_name': file_name,
                        'target_folder': None,
                        'match_reason': error_msg,
                        'classification_method': 'Error',
                        'success': False,
                        'error': error_msg
                    })
            success_count = sum(1 for result in preview_results if result['success'])
            logging.info(f"é¢„è§ˆåˆ†ç±»å®Œæˆï¼ŒæˆåŠŸåˆ†ç±» {success_count}/{len(preview_results)} ä¸ªæ–‡ä»¶")
            
            # ç”Ÿæˆé¢„è§ˆç»“æœJSONæ–‡ä»¶ï¼ˆæ ¼å¼ä¸preview_ai_result.jsonä¿æŒä¸€è‡´ï¼‰
            try:
                ai_preview_results = []
                for result in preview_results:
                    if result['success']:
                        ai_preview_result = {
                            "æºæ–‡ä»¶è·¯å¾„": result['file_path'],
                            "æ–‡ä»¶æ‘˜è¦": result['content_summary'],
                            "æœ€åŒ¹é…çš„ç›®æ ‡ç›®å½•": result['target_folder'],
                            "åŒ¹é…ç†ç”±": result['match_reason'],
                            "å¤„ç†è€—æ—¶ä¿¡æ¯": {
                                "æ€»è€—æ—¶(ç§’)": result['timing_info'].get('total_processing_time', 0),
                                "å†…å®¹æå–è€—æ—¶(ç§’)": result['timing_info'].get('content_extraction_time', 0),
                                "æ‘˜è¦ç”Ÿæˆè€—æ—¶(ç§’)": result['timing_info'].get('summary_generation_time', 0),
                                "ç›®å½•æ¨èè€—æ—¶(ç§’)": result['timing_info'].get('folder_recommendation_time', 0)
                            }
                        }
                        ai_preview_results.append(ai_preview_result)
                
                # ä¿å­˜é¢„è§ˆç»“æœåˆ°JSONæ–‡ä»¶
                preview_result_file = 'preview_ai_result.json'
                with open(preview_result_file, 'w', encoding='utf-8') as f:
                    json.dump(ai_preview_results, f, ensure_ascii=False, indent=2)
                logging.info(f"AIé¢„è§ˆç»“æœå·²ä¿å­˜åˆ°: {preview_result_file}")
                
            except Exception as e:
                logging.warning(f"ç”Ÿæˆé¢„è§ˆç»“æœæ–‡ä»¶å¤±è´¥: {e}")
            
            return preview_results
        except Exception as e:
            raise FileOrganizerError(f"é¢„è§ˆåˆ†ç±»å¤±è´¥: {e}")
    def organize_file(self, file_path: str, target_directory: str) -> Tuple[bool, str]:
        try:
            if not hasattr(self, 'ai_manager') or self.ai_manager is None:
                self.initialize_ollama()
            
            file_path_obj = Path(file_path)
            filename = file_path_obj.name
            source_file_full_path = str(file_path_obj.absolute())
            
            print(f"ğŸ“„ æºæ–‡ä»¶å®Œæ•´è·¯å¾„: {source_file_full_path}")
            
            # ä½¿ç”¨AIåˆ†ç±»è·å–æ¨èæ–‡ä»¶å¤¹
            target_folder, match_reason, success = self.classify_file(str(file_path_obj), target_directory)
            if not success or not target_folder:
                return False, "æ— æ³•ç¡®å®šç›®æ ‡æ–‡ä»¶å¤¹"
            
            # ä½¿ç”¨å­˜å‚¨çš„æ¨èè·¯å¾„å˜é‡
            if hasattr(self, 'recommended_folder_path') and self.recommended_folder_path:
                target_folder_full_path = Path(target_directory) / self.recommended_folder_path
                print(f"ğŸ”§ ä½¿ç”¨å­˜å‚¨çš„æ¨èè·¯å¾„: {self.recommended_folder_path}")
                print(f"ğŸ¯ ç›®æ ‡æ–‡ä»¶å¤¹å®Œæ•´è·¯å¾„: {target_folder_full_path}")
            else:
                target_folder_full_path = Path(target_directory) / target_folder
                print(f"âš ï¸  ä½¿ç”¨é»˜è®¤è·¯å¾„: {target_folder}")
                print(f"ğŸ¯ ç›®æ ‡æ–‡ä»¶å¤¹å®Œæ•´è·¯å¾„: {target_folder_full_path}")
            
            # æ„å»ºå®Œæ•´çš„è¿ç§»å‘½ä»¤ä¿¡æ¯
            migration_info = {
                'source_file': source_file_full_path,
                'target_folder': str(target_folder_full_path),
                'filename': filename,
                'match_reason': match_reason
            }
            
            print(f"ğŸ“‹ è¿ç§»ä¿¡æ¯: {migration_info}")
            
            # æ£€æŸ¥å¹¶åˆ›å»ºå¹´ä»½å­æ–‡ä»¶å¤¹
            final_target_folder = self._check_and_create_year_folder(target_folder_full_path, filename)
            
            # ç¡®ä¿ç›®æ ‡æ–‡ä»¶å¤¹å­˜åœ¨
            final_target_folder.mkdir(parents=True, exist_ok=True)
            
            # æ„å»ºç›®æ ‡æ–‡ä»¶è·¯å¾„
            target_file_path = final_target_folder / filename
            if target_file_path.exists():
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                name_parts = filename.rsplit('.', 1)
                if len(name_parts) == 2:
                    new_filename = f"{name_parts[0]}_{timestamp}.{name_parts[1]}"
                else:
                    new_filename = f"{filename}_{timestamp}"
                target_file_path = final_target_folder / new_filename
                print(f"âš ï¸  æ–‡ä»¶åå†²çªï¼Œé‡å‘½åä¸º: {new_filename}")
            
            # æ‰§è¡Œæ–‡ä»¶å¤åˆ¶
            shutil.copy2(source_file_full_path, str(target_file_path))
            
            # è®°å½•è¿ç§»æ—¥å¿—
            migration_log = f"æ–‡ä»¶è¿ç§»å®Œæˆ: {source_file_full_path} -> {target_file_path}"
            logging.info(migration_log)
            print(f"âœ… {migration_log}")
            
            return True, str(target_file_path)
        except Exception as e:
            error_msg = f"æ•´ç†æ–‡ä»¶å¤±è´¥: {e}"
            logging.error(error_msg)
            return False, error_msg
    def organize_files(self, files=None, target_base_dir=None, copy_mode=True, source_directory=None, target_directory=None, dry_run=False, progress_callback=None) -> Dict[str, object]:
        try:
            if source_directory and target_directory:
                files = self.scan_files(source_directory)
                target_base_dir = target_directory
                copy_mode = True
            if not files or not target_base_dir:
                raise FileOrganizerError("ç¼ºå°‘å¿…è¦å‚æ•°")
            log_session_name = None
            if self.enable_transfer_log and self.transfer_log_manager and not dry_run:
                try:
                    session_name = f"organize_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                    log_session_name = self.transfer_log_manager.start_transfer_session(session_name)
                    logging.info(f"å¼€å§‹è½¬ç§»æ—¥å¿—ä¼šè¯: {session_name}")
                except Exception as e:
                    logging.warning(f"å¯åŠ¨è½¬ç§»æ—¥å¿—ä¼šè¯å¤±è´¥: {e}")
            if not files:
                raise FileOrganizerError("æ²¡æœ‰æ–‡ä»¶éœ€è¦æ•´ç†")
            results = {
                'total_files': len(files),
                'processed_files': 0,
                'successful_moves': 0,
                'failed_moves': 0,
                'skipped_files': 0,
                'success': [],
                'failed': [],
                'errors': [],
                'move_details': [],
                'ai_responses': [],
                'start_time': datetime.now(),
                'end_time': None,
                'transfer_log_file': log_session_name,
                'dry_run': dry_run
            }
            
            # åˆå§‹åŒ–AIç»“æœæ–‡ä»¶
            ai_result_file = 'ai_organize_result.json'
            # ä¸å†åœ¨å†…å­˜ä¸­ä¿å­˜å®Œæ•´çš„ai_resultsåˆ—è¡¨ï¼Œåªä¿å­˜å¿…è¦çš„è¿ç§»ä¿¡æ¯
            migration_queue = []  # åªä¿å­˜æºè·¯å¾„å’Œç›®æ ‡è·¯å¾„çš„ç®€å•ä¿¡æ¯
            
            # ç¡®ä¿AIç»“æœæ–‡ä»¶å­˜åœ¨ï¼Œä½†ä¸æ¸…ç©ºç°æœ‰å†…å®¹
            if not os.path.exists(ai_result_file):
                with open(ai_result_file, 'w', encoding='utf-8') as f:
                    json.dump([], f, ensure_ascii=False, indent=2)
                logging.info(f"åˆ›å»ºæ–°çš„AIç»“æœæ–‡ä»¶: {ai_result_file}")
            else:
                logging.info(f"AIç»“æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œå°†è¿½åŠ æ–°è®°å½•: {ai_result_file}")
            logging.info(f"å¼€å§‹å®‰å…¨æ–‡ä»¶æ•´ç†ï¼Œå…± {len(files)} ä¸ªæ–‡ä»¶")
            print(f"\n=== å¼€å§‹AIæ™ºèƒ½æ–‡ä»¶æ•´ç† ===")
            print(f"æºç›®å½•: {source_directory if source_directory else 'æŒ‡å®šæ–‡ä»¶åˆ—è¡¨'}")
            print(f"ç›®æ ‡ç›®å½•: {target_base_dir}")
            print(f"å¾…å¤„ç†æ–‡ä»¶æ€»æ•°: {len(files)}")
            print(f"æ“ä½œæ¨¡å¼: {'å¤åˆ¶' if copy_mode else 'ç§»åŠ¨'}")
            print("=" * 50)
            
            for i, file_info in enumerate(files, 1):
                file_path = str(file_info['path'])
                filename = str(file_info['name'])
                
                # è°ƒç”¨è¿›åº¦å›è°ƒ
                if progress_callback:
                    progress_callback(i, len(files), filename)
                
                # æ§åˆ¶å°è¾“å‡ºå½“å‰å¤„ç†è¿›åº¦
                print(f"\n[{i}/{len(files)}] æ­£åœ¨å¤„ç†: {filename}")
                
                try:
                    logging.info(f"æ­£åœ¨å¤„ç†æ–‡ä»¶ {i}/{len(files)}: {filename}")
                    print(f"  ğŸ” æ­£åœ¨åˆ†ææ–‡ä»¶å†…å®¹...", end="", flush=True)
                    
                    # è·å–è¯¦ç»†çš„åˆ†æç»“æœï¼ˆåªè°ƒç”¨ä¸€æ¬¡ï¼‰
                    analysis_result = self.analyze_and_classify_file(file_path, target_base_dir)
                    target_folder = analysis_result.get('recommended_folder')
                    match_reason = analysis_result.get('match_reason', '')
                    success = analysis_result.get('success', False)
                    
                    results['ai_responses'].append({
                        'file_name': filename,
                        'target_folder': target_folder,
                        'match_reason': match_reason,
                        'success': success
                    })
                    
                    # æ„å»ºAIç»“æœé¡¹ï¼ˆåŸºç¡€ä¿¡æ¯ï¼‰
                    level_tags = analysis_result.get('level_tags', [])
                    file_metadata = analysis_result.get('file_metadata', {})
                    
                    ai_result_item = {
                        "å¤„ç†æ—¶é—´": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        "æ–‡ä»¶å": filename,
                        "æºæ–‡ä»¶è·¯å¾„": file_path,
                        "æ–‡ä»¶æ‘˜è¦": analysis_result.get('content_summary', ''),
                        "æœ€åŒ¹é…çš„ç›®æ ‡ç›®å½•": analysis_result.get('recommended_folder', ''),
                        "åŒ¹é…ç†ç”±": analysis_result.get('match_reason', ''),
                        "å¤„ç†è€—æ—¶ä¿¡æ¯": {
                            "æ€»è€—æ—¶(ç§’)": analysis_result.get('timing_info', {}).get('total_processing_time', 0),
                            "å…ƒæ•°æ®æå–è€—æ—¶(ç§’)": analysis_result.get('timing_info', {}).get('metadata_extraction_time', 0),
                            "å†…å®¹æå–è€—æ—¶(ç§’)": analysis_result.get('timing_info', {}).get('content_extraction_time', 0),
                            "æ‘˜è¦ç”Ÿæˆè€—æ—¶(ç§’)": analysis_result.get('timing_info', {}).get('summary_generation_time', 0),
                            "ç›®å½•æ¨èè€—æ—¶(ç§’)": analysis_result.get('timing_info', {}).get('folder_recommendation_time', 0)
                        },
                        "æ–‡ä»¶å…ƒæ•°æ®": {
                            "åˆ›å»ºæ—¶é—´": file_metadata.get('created_time', ''),
                            "ä¿®æ”¹æ—¶é—´": file_metadata.get('modified_time', ''),
                            "æ–‡ä»¶å¤§å°": file_metadata.get('file_size', 0)
                        }
                    }
                    
                    # æ·»åŠ å„çº§æ ‡ç­¾
                    for i, tag in enumerate(level_tags, 1):
                        ai_result_item[f"{i}çº§æ ‡ç­¾"] = tag
                    
                    # ä¿å­˜åˆ°è¿ç§»é˜Ÿåˆ—ï¼Œç­‰å¾…è¿ç§»æˆåŠŸåå†™å…¥å®Œæ•´ä¿¡æ¯
                    migration_queue.append({
                        'source_path': file_path,
                        'target_folder': target_folder,
                        'filename': filename,
                        'match_reason': match_reason,
                        'ai_result_item': ai_result_item
                    })
                    
                    if not success or not target_folder:
                        error_msg = f"æ–‡ä»¶ {filename} åˆ†ç±»å¤±è´¥: {match_reason}ï¼Œå·²è·³è¿‡ï¼Œæœªåšä»»ä½•å¤„ç†"
                        print(f"\r  âŒ åˆ†ç±»å¤±è´¥: {match_reason}")
                        logging.warning(error_msg)
                        results['errors'].append(error_msg)
                        results['skipped_files'] += 1
                        results['failed'].append({
                            'source_path': file_path,
                            'error': error_msg
                        })
                        
                        # æ›´æ–°è¿ç§»é˜Ÿåˆ—ä¸­æœ€åä¸€é¡¹çš„å¤±è´¥çŠ¶æ€å¹¶ç«‹å³å†™å…¥
                        if migration_queue:
                            migration_queue[-1]['ai_result_item'].update({
                                "å¤„ç†çŠ¶æ€": "åˆ†ç±»å¤±è´¥",
                                "é”™è¯¯ä¿¡æ¯": match_reason
                            })
                            # ç«‹å³å†™å…¥å¤±è´¥è®°å½•
                            self._append_ai_result_to_file(ai_result_file, migration_queue[-1]['ai_result_item'])
                        
                        # æ¸…ç†å½“å‰æ–‡ä»¶çš„ç¼“å­˜
                        self._clear_file_cache(file_path)
                        
                        continue
                    
                    print(f"\r  âœ… æ¨èç›®å½•: {target_folder}")
                    print(f"     ç†ç”±: {match_reason}")
                    target_folder_path = Path(target_base_dir) / target_folder
                    if not target_folder_path.exists():
                        error_msg = f"ç›®æ ‡æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {target_folder}"
                        logging.error(error_msg)
                        results['errors'].append(error_msg)
                        results['failed_moves'] += 1
                        results['failed'].append({
                            'source_path': file_path,
                            'error': error_msg
                        })
                        continue
                    original_target_path = target_folder_path / filename
                    target_file_path = original_target_path
                    if target_file_path.exists():
                        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                        name_parts = filename.rsplit('.', 1)
                        if len(name_parts) == 2:
                            new_filename = f"{name_parts[0]}_{timestamp}.{name_parts[1]}"
                        else:
                            new_filename = f"{filename}_{timestamp}"
                        target_file_path = target_folder_path / new_filename
                        logging.info(f"æ–‡ä»¶åå†²çªï¼Œé‡å‘½åä¸º: {target_file_path.name}")
                    if not dry_run:
                        try:
                            if not Path(file_path).exists():
                                error_msg = f"æºæ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
                                logging.error(error_msg)
                                results['errors'].append(error_msg)
                                results['failed_moves'] += 1
                                results['failed'].append({
                                    'source_path': file_path,
                                    'error': error_msg
                                })
                                continue
                            if copy_mode:
                                shutil.copy2(file_path, str(target_file_path))
                                operation = "copy"
                                operation_cn = "å¤åˆ¶"
                            else:
                                shutil.move(file_path, str(target_file_path))
                                operation = "move"
                                operation_cn = "ç§»åŠ¨"
                            if target_file_path.exists():
                                if not copy_mode and Path(file_path).exists():
                                    error_msg = f"æ–‡ä»¶ç§»åŠ¨éªŒè¯å¤±è´¥: {filename}"
                                    logging.error(error_msg)
                                    results['errors'].append(error_msg)
                                    results['failed_moves'] += 1
                                    results['failed'].append({
                                        'source_path': file_path,
                                        'error': error_msg
                                    })
                                    continue
                                print(f"     âœ… {operation_cn}æˆåŠŸ: {filename} -> {target_folder}")
                                logging.info(f"æ–‡ä»¶å®‰å…¨{operation_cn}æˆåŠŸ: {filename} -> {target_folder} ({match_reason})")
                                results['successful_moves'] += 1
                                
                                # æ›´æ–°è¿ç§»é˜Ÿåˆ—ä¸­å¯¹åº”é¡¹çš„æœ€ç»ˆè·¯å¾„å¹¶ç«‹å³å†™å…¥
                                for queue_item in migration_queue:
                                    if queue_item['source_path'] == file_path:
                                        queue_item['ai_result_item'].update({
                                            "æœ€ç»ˆç›®æ ‡è·¯å¾„": str(target_file_path),
                                            "æ“ä½œç±»å‹": operation_cn,
                                            "å¤„ç†çŠ¶æ€": "æˆåŠŸ"
                                        })
                                        # ç«‹å³å†™å…¥æˆåŠŸè®°å½•
                                        self._append_ai_result_to_file(ai_result_file, queue_item['ai_result_item'])
                                        break
                                
                                # æ¸…ç†å½“å‰æ–‡ä»¶çš„ç¼“å­˜
                                self._clear_file_cache(file_path)
                            else:
                                error_msg = f"æ–‡ä»¶{operation_cn}éªŒè¯å¤±è´¥: {filename}"
                                logging.error(error_msg)
                                results['errors'].append(error_msg)
                                results['failed_moves'] += 1
                                results['failed'].append({
                                    'source_path': file_path,
                                    'error': error_msg
                                })
                                continue
                        except Exception as move_error:
                            error_msg = f"{operation_cn}æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {move_error}"
                            logging.error(error_msg)
                            results['errors'].append(error_msg)
                            results['failed_moves'] += 1
                            results['failed'].append({
                                'source_path': file_path,
                                'error': error_msg
                            })
                            continue
                    else:
                        operation = "copy" if copy_mode else "move"
                        operation_cn = "å¤åˆ¶" if copy_mode else "ç§»åŠ¨"
                        logging.info(f"[è¯•è¿è¡Œ] æ–‡ä»¶å°†{operation_cn}: {filename} -> {target_folder} ({match_reason})")
                        results['successful_moves'] += 1
                        
                        # è¯•è¿è¡Œæ¨¡å¼ä¸‹æ›´æ–°è¿ç§»é˜Ÿåˆ—ä¸­å¯¹åº”é¡¹å¹¶ç«‹å³å†™å…¥
                        for queue_item in migration_queue:
                            if queue_item['source_path'] == file_path:
                                queue_item['ai_result_item'].update({
                                    "æœ€ç»ˆç›®æ ‡è·¯å¾„": str(target_file_path),
                                    "æ“ä½œç±»å‹": operation_cn,
                                    "å¤„ç†çŠ¶æ€": "è¯•è¿è¡ŒæˆåŠŸ"
                                })
                                # ç«‹å³å†™å…¥è¯•è¿è¡Œè®°å½•
                                self._append_ai_result_to_file(ai_result_file, queue_item['ai_result_item'])
                                break
                        
                        # æ¸…ç†å½“å‰æ–‡ä»¶çš„ç¼“å­˜
                        self._clear_file_cache(file_path)
                    if self.enable_transfer_log and self.transfer_log_manager and not dry_run:
                        try:
                            file_size_raw = file_info.get('size', 0)
                            file_size = int(file_size_raw) if isinstance(file_size_raw, (int, float, str)) and str(file_size_raw).isdigit() else 0
                            self.transfer_log_manager.log_transfer_operation(
                                source_path=file_path,
                                target_path=str(target_file_path),
                                operation_type=operation,
                                target_folder=target_folder,
                                success=True,
                                file_size=file_size
                            )
                        except Exception as e:
                            logging.warning(f"è®°å½•è½¬ç§»æ—¥å¿—å¤±è´¥: {e}")
                    results['success'].append({
                        'source_path': file_path,
                        'target_path': str(target_file_path),
                        'target_folder': target_folder,
                        'operation': operation
                    })
                    results['move_details'].append({
                        'source_file': file_path,
                        'file_name': filename,
                        'target_folder': target_folder,
                        'target_path': str(target_file_path),
                        'match_reason': match_reason,
                        'classification_method': 'AI',
                        'renamed': str(target_file_path) != str(original_target_path),
                        'operation': operation
                    })
                except Exception as e:
                    error_msg = f"å¤„ç†æ–‡ä»¶ {filename} æ—¶å‡ºç°å¼‚å¸¸: {e}"
                    logging.error(error_msg)
                    results['errors'].append(error_msg)
                    results['failed_moves'] += 1
                    if self.enable_transfer_log and self.transfer_log_manager and not dry_run:
                        try:
                            operation_type = "copy" if copy_mode else "move"
                            self.transfer_log_manager.log_transfer_operation(
                                source_path=file_path,
                                target_path="",
                                operation_type=operation_type,
                                target_folder="",
                                success=False,
                                error_message=error_msg
                            )
                        except Exception as log_e:
                            logging.warning(f"è®°å½•å¤±è´¥è½¬ç§»æ—¥å¿—å¤±è´¥: {log_e}")
                    results['failed'].append({
                        'source_path': file_path,
                        'error': error_msg
                    })
                finally:
                    results['processed_files'] += 1
            results['end_time'] = datetime.now()
            
            # è¾“å‡ºå¤„ç†å®Œæˆæ€»ç»“
            print(f"\n=== æ–‡ä»¶æ•´ç†å®Œæˆ ===")
            print(f"æ€»æ–‡ä»¶æ•°: {results['total_files']}")
            print(f"æˆåŠŸå¤„ç†: {results['successful_moves']} ä¸ª")
            print(f"å¤„ç†å¤±è´¥: {results['failed_moves']} ä¸ª")
            print(f"è·³è¿‡æ–‡ä»¶: {results['skipped_files']} ä¸ª")
            duration = (results['end_time'] - results['start_time']).total_seconds()
            print(f"æ€»è€—æ—¶: {duration:.1f} ç§’")
            print("=" * 50)
            
            # AIç»“æœå·²åœ¨å¤„ç†è¿‡ç¨‹ä¸­å®æ—¶å†™å…¥
            results['ai_result_file'] = ai_result_file
            logging.info(f"AIåˆ†æç»“æœå·²å®æ—¶å†™å…¥: {ai_result_file}")
            
            if self.enable_transfer_log and self.transfer_log_manager and not dry_run:
                try:
                    session_summary = self.transfer_log_manager.end_transfer_session()
                    logging.info(f"è½¬ç§»æ—¥å¿—ä¼šè¯ç»“æŸ: {session_summary}")
                except Exception as e:
                    logging.warning(f"ç»“æŸè½¬ç§»æ—¥å¿—ä¼šè¯å¤±è´¥: {e}")
            
            logging.info(f"å®‰å…¨æ–‡ä»¶æ•´ç†å®Œæˆ: æˆåŠŸ {results['successful_moves']}, å¤±è´¥ {results['failed_moves']}, è·³è¿‡ {results['skipped_files']}")
            
            # åˆ é™¤æºæ–‡ä»¶åŠŸèƒ½å·²ç§»è‡³GUIç•Œé¢ï¼Œä¸å†åœ¨æ§åˆ¶å°è¯¢é—®
            if not dry_run and results['successful_moves'] > 0:
                logging.info(f"æˆåŠŸå¤„ç† {results['successful_moves']} ä¸ªæ–‡ä»¶ï¼Œåˆ é™¤æºæ–‡ä»¶åŠŸèƒ½è¯·åœ¨GUIç•Œé¢ä½¿ç”¨")
            
            return results
        except Exception as e:
            raise FileOrganizerError(f"æ‰¹é‡æ•´ç†æ–‡ä»¶å¤±è´¥: {e}")
    def get_transfer_logs(self) -> List[str]:
        if not self.enable_transfer_log or not self.transfer_log_manager:
            raise FileOrganizerError("è½¬ç§»æ—¥å¿—åŠŸèƒ½æœªå¯ç”¨")
        return self.transfer_log_manager.get_transfer_logs()
    def get_transfer_log_summary(self, log_file_path: str) -> Dict:
        if not self.enable_transfer_log or not self.transfer_log_manager:
            raise FileOrganizerError("è½¬ç§»æ—¥å¿—åŠŸèƒ½æœªå¯ç”¨")
        return self.transfer_log_manager.get_session_summary(log_file_path)
    def restore_files_from_log(self, log_file_path: str, operation_ids: Optional[List[int]] = None, dry_run: bool = True) -> Dict:
        if not self.enable_transfer_log or not self.transfer_log_manager:
            raise FileOrganizerError("è½¬ç§»æ—¥å¿—åŠŸèƒ½æœªå¯ç”¨")
        try:
            restore_results = self.transfer_log_manager.restore_from_log(
                log_file_path=log_file_path,
                operation_ids=operation_ids if operation_ids is not None else [],
                dry_run=dry_run
            )
            logging.info(f"æ–‡ä»¶æ¢å¤å®Œæˆ: æˆåŠŸ {restore_results['successful_restores']}, å¤±è´¥ {restore_results['failed_restores']}, è·³è¿‡ {restore_results['skipped_operations']}")
            return restore_results
        except Exception as e:
            raise FileOrganizerError(f"æ–‡ä»¶æ¢å¤å¤±è´¥: {e}")
    def cleanup_old_transfer_logs(self, days_to_keep: int = 30) -> int:
        if not self.enable_transfer_log or not self.transfer_log_manager:
            raise FileOrganizerError("è½¬ç§»æ—¥å¿—åŠŸèƒ½æœªå¯ç”¨")
        return self.transfer_log_manager.cleanup_old_logs(days_to_keep)
    def _append_ai_result_to_file(self, ai_result_file: str, ai_result_item: dict) -> None:
        """è¿½åŠ AIç»“æœåˆ°æ–‡ä»¶ï¼Œé¿å…åœ¨å†…å­˜ä¸­ä¿å­˜å¤§é‡æ•°æ®"""
        try:
            # è¯»å–ç°æœ‰æ•°æ®
            existing_data = []
            if os.path.exists(ai_result_file):
                try:
                    with open(ai_result_file, 'r', encoding='utf-8') as f:
                        content = f.read().strip()
                        if content:  # åªæœ‰å½“æ–‡ä»¶ä¸ä¸ºç©ºæ—¶æ‰å°è¯•è§£æJSON
                            existing_data = json.loads(content)
                        else:
                            existing_data = []  # ç©ºæ–‡ä»¶æ—¶åˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨
                except json.JSONDecodeError:
                    # å¦‚æœJSONè§£æå¤±è´¥ï¼Œåˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨
                    existing_data = []
                    logging.warning(f"AIç»“æœæ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œé‡æ–°åˆå§‹åŒ–: {ai_result_file}")
            
            # è¿½åŠ æ–°æ•°æ®
            existing_data.append(ai_result_item)
            
            # å†™å›æ–‡ä»¶
            with open(ai_result_file, 'w', encoding='utf-8') as f:
                json.dump(existing_data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            logging.error(f"è¿½åŠ AIç»“æœåˆ°æ–‡ä»¶å¤±è´¥: {e}")
    
    def _update_last_ai_result(self, ai_result_file: str, updates: dict) -> None:
        """æ›´æ–°æ–‡ä»¶ä¸­æœ€åä¸€æ¡AIç»“æœè®°å½•"""
        try:
            # è¯»å–ç°æœ‰æ•°æ®
            existing_data = []
            if os.path.exists(ai_result_file):
                try:
                    with open(ai_result_file, 'r', encoding='utf-8') as f:
                        content = f.read().strip()
                        if content:  # åªæœ‰å½“æ–‡ä»¶ä¸ä¸ºç©ºæ—¶æ‰å°è¯•è§£æJSON
                            existing_data = json.loads(content)
                        else:
                            existing_data = []  # ç©ºæ–‡ä»¶æ—¶åˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨
                except json.JSONDecodeError:
                    # å¦‚æœJSONè§£æå¤±è´¥ï¼Œåˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨
                    existing_data = []
                    logging.warning(f"AIç»“æœæ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œé‡æ–°åˆå§‹åŒ–: {ai_result_file}")
            
            # æ›´æ–°æœ€åä¸€æ¡è®°å½•
            if existing_data:
                existing_data[-1].update(updates)
                
                # å†™å›æ–‡ä»¶
                with open(ai_result_file, 'w', encoding='utf-8') as f:
                    json.dump(existing_data, f, ensure_ascii=False, indent=2)
                    
        except Exception as e:
            logging.error(f"æ›´æ–°AIç»“æœæ–‡ä»¶å¤±è´¥: {e}")

    # def _ask_delete_source_files(self, successful_moves: List[Dict]) -> None:
    #     """è¯¢é—®ç”¨æˆ·æ˜¯å¦åˆ é™¤æºæ–‡ä»¶ - å·²ç§»è‡³GUIç•Œé¢ï¼Œæ­¤æ–¹æ³•ä¸å†ä½¿ç”¨"""
    #     # åˆ é™¤æºæ–‡ä»¶åŠŸèƒ½å·²ç§»è‡³GUIç•Œé¢ï¼Œä¸å†åœ¨æ§åˆ¶å°è¯¢é—®
    #     pass

    def get_file_summary(self, file_path: str, max_length: int = 50, max_pages: int = 2, max_seconds: int = 10) -> str:
        """
        è‡ªåŠ¨é€‚é… txt/pdf/docx æ ¼å¼ï¼Œè¿”å›å‰max_lengthå­—æ‘˜è¦ï¼ŒPDF/Wordåªå–å‰max_pagesé¡µï¼Œå•æ–‡ä»¶å¤„ç†è¶…æ—¶max_secondsç§’ã€‚
        """
        ext = Path(file_path).suffix.lower()
        start_time = time.time()
        try:
            if ext == '.txt':
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    return f.read(max_length)
            elif ext == '.pdf':
                with open(file_path, 'rb') as f:
                    reader = PdfReader(f)
                    text = ''
                    for i, page in enumerate(reader.pages):
                        if i >= max_pages or (time.time() - start_time) > max_seconds:
                            break
                        page_text = page.extract_text() or ''
                        text += page_text
                        if len(text) >= max_length:
                            break
                    if (time.time() - start_time) > max_seconds:
                        return 'æå–è¶…æ—¶ï¼Œå·²è·³è¿‡'
                    return text[:max_length] if text else 'æœªèƒ½æå–æ­£æ–‡'
            elif ext == '.docx':
                doc = Document(file_path)
                text = ''
                for i, para in enumerate(doc.paragraphs):
                    if (time.time() - start_time) > max_seconds:
                        break
                    text += para.text
                    if len(text) >= max_length:
                        break
                if (time.time() - start_time) > max_seconds:
                    return 'æå–è¶…æ—¶ï¼Œå·²è·³è¿‡'
                return text[:max_length] if text else 'æœªèƒ½æå–æ­£æ–‡'
            else:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    return f.read(max_length)
        except Exception as e:
            if (time.time() - start_time) > max_seconds:
                return 'æå–è¶…æ—¶ï¼Œå·²è·³è¿‡'
            return f'æ‘˜è¦è·å–å¤±è´¥: {e}'
    
    def batch_read_documents(self, folder_path: str, progress_callback=None, summary_length: int = 200) -> Dict[str, Any]:
        """
        æ‰¹é‡è§£è¯»æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰æ–‡æ¡£ï¼Œç”Ÿæˆæ‘˜è¦å¹¶ä¿å­˜åˆ°AIç»“æœæ–‡ä»¶
        
        Args:
            folder_path: è¦è§£è¯»çš„æ–‡ä»¶å¤¹è·¯å¾„
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶ (current, total, filename) å‚æ•°
            summary_length: æ‘˜è¦é•¿åº¦ï¼ˆå­—æ•°ï¼‰ï¼Œé»˜è®¤200å­—
            
        Returns:
            åŒ…å«å¤„ç†ç»“æœçš„å­—å…¸
        """
        start_time = time.time()
        timing_info = {}
        
        try:
            if not hasattr(self, 'ai_manager') or self.ai_manager is None:
                init_start = time.time()
                self.initialize_ollama()
                timing_info['ollama_init_time'] = round(time.time() - init_start, 3)
            
            folder_path = Path(folder_path)
            if not folder_path.exists() or not folder_path.is_dir():
                raise FileOrganizerError(f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨æˆ–ä¸æ˜¯æœ‰æ•ˆç›®å½•: {folder_path}")
            
            # æ”¯æŒçš„æ–‡æ¡£æ ¼å¼
            supported_extensions = {'.txt', '.pdf', '.docx', '.doc', '.md', '.py', '.js', '.html', '.css', '.json', '.xml', '.csv'}
            
            # æ‰«ææ–‡ä»¶å¤¹è·å–æ‰€æœ‰æ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶
            document_files = []
            for file_path in folder_path.rglob('*'):
                if file_path.is_file() and file_path.suffix.lower() in supported_extensions:
                    document_files.append(file_path)
            
            if not document_files:
                return {
                    'total_files': 0,
                    'processed_files': 0,
                    'successful_reads': 0,
                    'failed_reads': 0,
                    'errors': [],
                    'timing_info': timing_info,
                    'start_time': datetime.now(),
                    'end_time': datetime.now()
                }
            
            # åˆå§‹åŒ–ç»“æœç»Ÿè®¡
            results = {
                'total_files': len(document_files),
                'processed_files': 0,
                'successful_reads': 0,
                'failed_reads': 0,
                'errors': [],
                'timing_info': timing_info,
                'start_time': datetime.now(),
                'end_time': None
            }
            
            # åˆå§‹åŒ–AIç»“æœæ–‡ä»¶
            ai_result_file = 'ai_organize_result.json'
            if not os.path.exists(ai_result_file):
                with open(ai_result_file, 'w', encoding='utf-8') as f:
                    json.dump([], f, ensure_ascii=False, indent=2)
                logging.info(f"åˆ›å»ºæ–°çš„AIç»“æœæ–‡ä»¶: {ai_result_file}")
            else:
                logging.info(f"AIç»“æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œå°†è¿½åŠ æ–°è®°å½•: {ai_result_file}")
            

            
            logging.info(f"å¼€å§‹æ‰¹é‡æ–‡æ¡£è§£è¯»ï¼Œå…± {len(document_files)} ä¸ªæ–‡ä»¶")
            print(f"\n=== å¼€å§‹æ‰¹é‡æ–‡æ¡£è§£è¯» ===")
            print(f"æºæ–‡ä»¶å¤¹: {folder_path}")
            print(f"å¾…å¤„ç†æ–‡ä»¶æ€»æ•°: {len(document_files)}")
            print("=" * 50)
            
            # é€ä¸ªå¤„ç†æ–‡æ¡£æ–‡ä»¶
            for i, file_path in enumerate(document_files, 1):
                filename = file_path.name
                
                # è°ƒç”¨è¿›åº¦å›è°ƒ
                if progress_callback:
                    progress_callback(i, len(document_files), filename)
                
                # æ§åˆ¶å°è¾“å‡ºå½“å‰å¤„ç†è¿›åº¦
                print(f"\n[{i}/{len(document_files)}] æ­£åœ¨è§£è¯»: {filename}")
                
                try:
                    logging.info(f"æ­£åœ¨è§£è¯»æ–‡ä»¶ {i}/{len(document_files)}: {filename}")
                    print(f"  ğŸ“– æ­£åœ¨æå–æ–‡ä»¶å†…å®¹...", end="", flush=True)
                    
                    # æå–æ–‡ä»¶å†…å®¹
                    extract_start = time.time()
                    extracted_content = self._extract_file_content(str(file_path))
                    extract_time = round(time.time() - extract_start, 3)
                    
                    # æ ¹æ®è®¾ç½®æˆªå–å†…å®¹ç”¨äºAIå¤„ç†
                    truncate_length = self.ai_parameters['content_extraction_length'] if self.ai_parameters['content_extraction_length'] < 2000 else len(extracted_content)
                    content_for_ai = extracted_content[:truncate_length] if extracted_content else ""
                    
                    print(f"\r  ğŸ“– æ­£åœ¨ç”Ÿæˆæ‘˜è¦...", end="", flush=True)
                    
                    # ç”Ÿæˆæ‘˜è¦
                    summary_start = time.time()
                    summary = self._generate_content_summary(content_for_ai, filename, summary_length)
                    summary_time = round(time.time() - summary_start, 3)
                    
                    print(f"\r  âœ… è§£è¯»å®Œæˆ")
                    print(f"     æ‘˜è¦: {summary[:100]}{'...' if len(summary) > 100 else ''}")
                    
                    # æ„å»ºAIç»“æœé¡¹ï¼ˆä¸è¿ç§»åŠŸèƒ½æ ¼å¼å…¼å®¹ï¼‰
                    ai_result_item = {
                        "å¤„ç†æ—¶é—´": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        "æ–‡ä»¶å": filename,
                        "æºæ–‡ä»¶è·¯å¾„": str(file_path),  # è¿™é‡Œä½œä¸ºç›®æ ‡è·¯å¾„ï¼Œå› ä¸ºæ–‡ä»¶å·²åœ¨ç›®æ ‡ä½ç½®
                        "æ–‡ä»¶æ‘˜è¦": summary,
                        "æœ€åŒ¹é…çš„ç›®æ ‡ç›®å½•": str(file_path.parent),  # å½“å‰æ–‡ä»¶å¤¹ä½œä¸ºç›®æ ‡ç›®å½•
                        "åŒ¹é…ç†ç”±": "æ‰¹é‡æ–‡æ¡£è§£è¯»",
                        "å¤„ç†è€—æ—¶ä¿¡æ¯": {
                            "æ€»è€—æ—¶(ç§’)": round(extract_time + summary_time, 3),
                            "å†…å®¹æå–è€—æ—¶(ç§’)": extract_time,
                            "æ‘˜è¦ç”Ÿæˆè€—æ—¶(ç§’)": summary_time,
                            "ç›®å½•æ¨èè€—æ—¶(ç§’)": 0
                        },
                        "æœ€ç»ˆç›®æ ‡è·¯å¾„": str(file_path),  # æ–‡ä»¶å½“å‰ä½ç½®å°±æ˜¯æœ€ç»ˆä½ç½®
                        "æ“ä½œç±»å‹": "æ–‡æ¡£è§£è¯»",
                        "å¤„ç†çŠ¶æ€": "è§£è¯»æˆåŠŸ"
                    }
                    
                    # ä¿å­˜åˆ°AIç»“æœæ–‡ä»¶
                    self._append_ai_result_to_file(ai_result_file, ai_result_item)
                    
                    results['successful_reads'] += 1
                    logging.info(f"æ–‡æ¡£è§£è¯»æˆåŠŸ: {filename}ï¼Œæ‘˜è¦é•¿åº¦: {len(summary)} å­—ç¬¦")
                    
                except Exception as e:
                    error_msg = f"è§£è¯»æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {str(e)}"
                    print(f"\r  âŒ è§£è¯»å¤±è´¥: {str(e)}")
                    logging.error(error_msg)
                    results['errors'].append(error_msg)
                    results['failed_reads'] += 1
                    
                    # ä¿å­˜å¤±è´¥è®°å½•
                    try:
                        ai_result_item = {
                            "å¤„ç†æ—¶é—´": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                            "æ–‡ä»¶å": filename,
                            "æºæ–‡ä»¶è·¯å¾„": str(file_path),
                            "æ–‡ä»¶æ‘˜è¦": "",
                            "æœ€åŒ¹é…çš„ç›®æ ‡ç›®å½•": str(file_path.parent),
                            "åŒ¹é…ç†ç”±": "æ‰¹é‡æ–‡æ¡£è§£è¯»",
                            "å¤„ç†è€—æ—¶ä¿¡æ¯": {
                                "æ€»è€—æ—¶(ç§’)": 0,
                                "å†…å®¹æå–è€—æ—¶(ç§’)": 0,
                                "æ‘˜è¦ç”Ÿæˆè€—æ—¶(ç§’)": 0,
                                "ç›®å½•æ¨èè€—æ—¶(ç§’)": 0
                            },
                            "æœ€ç»ˆç›®æ ‡è·¯å¾„": str(file_path),
                            "æ“ä½œç±»å‹": "æ–‡æ¡£è§£è¯»",
                            "å¤„ç†çŠ¶æ€": "è§£è¯»å¤±è´¥",
                            "é”™è¯¯ä¿¡æ¯": str(e)
                        }
                        self._append_ai_result_to_file(ai_result_file, ai_result_item)
                    except Exception as save_error:
                        logging.warning(f"ä¿å­˜å¤±è´¥è®°å½•æ—¶å‡ºé”™: {save_error}")
                
                finally:
                    results['processed_files'] += 1
            
            results['end_time'] = datetime.now()
            
            # è¾“å‡ºå¤„ç†å®Œæˆæ€»ç»“
            print(f"\n=== æ‰¹é‡æ–‡æ¡£è§£è¯»å®Œæˆ ===")
            print(f"æ€»æ–‡ä»¶æ•°: {results['total_files']}")
            print(f"æˆåŠŸè§£è¯»: {results['successful_reads']} ä¸ª")
            print(f"è§£è¯»å¤±è´¥: {results['failed_reads']} ä¸ª")
            duration = (results['end_time'] - results['start_time']).total_seconds()
            print(f"æ€»è€—æ—¶: {duration:.1f} ç§’")
            print("=" * 50)
            
            logging.info(f"æ‰¹é‡æ–‡æ¡£è§£è¯»å®Œæˆ: æˆåŠŸ {results['successful_reads']}, å¤±è´¥ {results['failed_reads']}")
            logging.info(f"è§£è¯»ç»“æœå·²ä¿å­˜åˆ°: {ai_result_file}")
            
            return results
            
        except Exception as e:
            raise FileOrganizerError(f"æ‰¹é‡æ–‡æ¡£è§£è¯»å¤±è´¥: {e}")

    def _extract_folder_keywords(self, folder_path: str) -> List[str]:
        """
        ä»æ–‡ä»¶å¤¹è·¯å¾„ä¸­æå–æœ‰æ„ä¹‰çš„å…³é”®è¯
        
        Args:
            folder_path: æ–‡ä»¶å¤¹è·¯å¾„
            
        Returns:
            List[str]: å…³é”®è¯åˆ—è¡¨
        """
        try:
            # æå–è·¯å¾„ä¸­çš„æ–‡ä»¶å¤¹åç§°
            path_parts = folder_path.split('\\') + folder_path.split('/')
            keywords = []
            
            for part in path_parts:
                part = part.strip()
                if not part:
                    continue
                
                # ç§»é™¤ç¼–å·å‰ç¼€ï¼Œå¦‚"ã€7-4-5ã€‘"
                if 'ã€' in part and 'ã€‘' in part:
                    part = part.split('ã€‘', 1)[1] if 'ã€‘' in part else part
                
                # è¿‡æ»¤æ‰æ•°å­—å’Œå•ä¸ªå­—ç¬¦
                if len(part) <= 1 or part.isdigit():
                    continue
                
                # è¿‡æ»¤æ‰åŒ…å«è¿å­—ç¬¦çš„æ•°å­—ä»£ç ï¼Œå¦‚"7-4-5"
                if '-' in part and all(segment.isdigit() for segment in part.split('-')):
                    continue
                
                # è¿‡æ»¤æ‰çº¯è‹±æ–‡ç¼©å†™
                if part.isupper() and len(part) <= 3:
                    continue
                
                # é¿å…é‡å¤æ·»åŠ 
                if part not in keywords:
                    keywords.append(part)
                
                # å¯¹äºå¤åˆè¯æ±‡ï¼Œä¹Ÿæå–å…¶ä¸­çš„å…³é”®è¯
                if '\\' in part or '/' in part:
                    sub_parts = part.replace('\\', ' ').replace('/', ' ').split()
                    for sub_part in sub_parts:
                        sub_part = sub_part.strip()
                        if len(sub_part) > 1 and sub_part not in keywords:
                            keywords.append(sub_part)
            
            return keywords
        except Exception as e:
            logging.error(f"æå–æ–‡ä»¶å¤¹å…³é”®è¯å¤±è´¥: {e}")
            return []

    def _calculate_folder_relevance(self, file_content: str, file_summary: str, folder_path: str) -> float:
        """
        è®¡ç®—æ–‡ä»¶å†…å®¹ä¸æ–‡ä»¶å¤¹è·¯å¾„çš„ç›¸å…³æ€§è¯„åˆ†
        
        Args:
            file_content: æ–‡ä»¶å†…å®¹
            file_summary: æ–‡ä»¶æ‘˜è¦
            folder_path: æ–‡ä»¶å¤¹è·¯å¾„
            
        Returns:
            float: ç›¸å…³æ€§è¯„åˆ† (0.0-1.0)
        """
        try:
            # æå–æ–‡ä»¶å¤¹è·¯å¾„ä¸­çš„å…³é”®è¯
            folder_keywords = self._extract_folder_keywords(folder_path)
            if not folder_keywords:
                return 0.0
            
            # åˆå¹¶æ–‡ä»¶å†…å®¹å’Œæ‘˜è¦è¿›è¡Œåˆ†æ
            analysis_text = f"{file_content} {file_summary}".lower()
            
            # å®šä¹‰é€šç”¨çš„åŒä¹‰è¯å’Œç›¸å…³è¯æ±‡æ˜ å°„ï¼ˆåŸºäºå¸¸è§è¯æ±‡ï¼‰
            synonym_mapping = {
                # é€šç”¨ä¸šåŠ¡è¯æ±‡
                'ç»¼åˆ': ['ç»¼åˆ', 'ç­–ç•¥', 'æŠ•èµ„ç­–ç•¥', 'ç­–ç•¥æŠ¥å‘Š', 'ç»¼åˆåˆ†æ', 'ç»¼åˆç®¡ç†'],
                'ç ”ç©¶': ['ç ”ç©¶', 'åˆ†æ', 'è°ƒç ”', 'è°ƒæŸ¥', 'æŠ¥å‘Š', 'ä¸“é¢˜', 'ç ”ç©¶æŠ¥å‘Š'],
                'æŠ•èµ„': ['æŠ•èµ„', 'æŠ•èµ„ç­–ç•¥', 'æŠ•èµ„åˆ†æ', 'æŠ•èµ„ç ”ç©¶', 'æŠ•èµ„ç†å¿µ'],
                'è¡Œä¸š': ['è¡Œä¸š', 'äº§ä¸š', 'é¢†åŸŸ', 'æ¿å—', 'ç»†åˆ†', 'è¡Œä¸šç ”ç©¶'],
                'å…¬å¸': ['å…¬å¸', 'ä¼ä¸š', 'æœºæ„', 'é›†å›¢', 'ä¸»ä½“', 'å…¬å¸ç ”ç©¶'],
                'ç»æµ': ['ç»æµ', 'å®è§‚ç»æµ', 'ç»æµåˆ†æ', 'ç»æµç ”ç©¶', 'ç»æµæ¨¡å‹'],
                'çŸ¥è¯†': ['çŸ¥è¯†', 'ç™¾ç§‘', 'ç§‘æ™®', 'æ•™è‚²', 'å­¦ä¹ '],
                'é¡¹ç›®': ['é¡¹ç›®', 'åˆä½œ', 'åˆä½œé¡¹ç›®', 'é¡¹ç›®åˆä½œ'],
                'èµ„æ–™': ['èµ„æ–™', 'æ–‡æ¡£', 'æ–‡ä»¶', 'ææ–™', 'ä¿¡æ¯'],
                'æŠ¥å‘Š': ['æŠ¥å‘Š', 'ç ”ç©¶æŠ¥å‘Š', 'åˆ†ææŠ¥å‘Š', 'ç­–ç•¥æŠ¥å‘Š', 'ä¸“é¢˜æŠ¥å‘Š'],
                'é›†åˆ': ['é›†åˆ', 'æ±‡æ€»', 'æ•´ç†', 'åˆ†ç±»', 'é›†åˆ'],
                'åˆ†ç±»': ['åˆ†ç±»', 'åˆ†ç±»ç ”ç©¶', 'è¡Œä¸šåˆ†ç±»', 'ç ”ç©¶åˆ†ç±»'],
                'æƒç›Š': ['æƒç›Š', 'æƒç›ŠæŠ•èµ„', 'è‚¡ç¥¨', 'è‚¡æƒ', 'æƒç›Šç±»'],
                'ä¼°å€¼': ['ä¼°å€¼', 'ä¼°å€¼æ–¹æ³•', 'ä¼°å€¼ç ”ç©¶', 'ä»·å€¼è¯„ä¼°'],
                'æ–¹æ³•': ['æ–¹æ³•', 'ç ”ç©¶æ–¹æ³•', 'åˆ†ææ–¹æ³•', 'ä¼°å€¼æ–¹æ³•'],
            }
            
            # ç®€åŒ–çš„ç›¸å…³æ€§è®¡ç®—ï¼šä¸»è¦åŸºäºå…³é”®è¯åŒ¹é…å’Œè·¯å¾„é•¿åº¦æƒé‡
            # åˆ é™¤å¤æ‚çš„ç›¸æ–¥æ€§æ£€æŸ¥ï¼Œè®©AIè‡ªå·±åˆ¤æ–­
            
            # è®¡ç®—ç›¸å…³æ€§è¯„åˆ†
            total_score = 0.0
            matched_keywords = []
            
            for keyword in folder_keywords:
                keyword_lower = keyword.lower()
                
                # ç›´æ¥åŒ¹é…ï¼ˆå®Œå…¨åŒ¹é…ï¼‰
                if keyword_lower in analysis_text:
                    total_score += 1.0
                    matched_keywords.append(keyword)
                    continue
                
                # éƒ¨åˆ†åŒ¹é…ï¼ˆå…³é”®è¯æ˜¯æ–‡ä»¶å†…å®¹ä¸­æŸä¸ªè¯æ±‡çš„å­ä¸²ï¼‰
                if any(keyword_lower in word.lower() for word in analysis_text.split()):
                    total_score += 0.9
                    matched_keywords.append(f"{keyword}(éƒ¨åˆ†åŒ¹é…)")
                    continue
                
                # åŒä¹‰è¯åŒ¹é…
                if keyword in synonym_mapping:
                    synonyms = synonym_mapping[keyword]
                    for synonym in synonyms:
                        if synonym.lower() in analysis_text:
                            total_score += 0.8  # åŒä¹‰è¯åŒ¹é…æƒé‡ç¨ä½
                            matched_keywords.append(f"{keyword}(åŒä¹‰è¯:{synonym})")
                            break
            
            # è®¡ç®—åŸºç¡€ç›¸å…³æ€§è¯„åˆ†
            if folder_keywords:
                base_relevance_score = total_score / len(folder_keywords)
                
                # è·¯å¾„é•¿åº¦æƒé‡ï¼šè·¯å¾„è¶Šé•¿ï¼ˆåˆ†ç±»è¶Šç»†è‡´ï¼‰ï¼Œæƒé‡è¶Šé«˜
                path_depth = len([p for p in folder_path.split('\\') + folder_path.split('/') if p.strip()])
                path_length_bonus = min(0.3, path_depth * 0.05)  # æœ€å¤šå¢åŠ 0.3åˆ†
                
                # æœ€ç»ˆç›¸å…³æ€§è¯„åˆ† = åŸºç¡€è¯„åˆ† + è·¯å¾„é•¿åº¦å¥–åŠ±
                relevance_score = base_relevance_score + path_length_bonus
                
                print(f"ğŸ“Š ç›¸å…³æ€§è¯„åˆ†: {relevance_score:.3f} (åŸºç¡€: {base_relevance_score:.3f}, è·¯å¾„å¥–åŠ±: {path_length_bonus:.3f}, åŒ¹é…å…³é”®è¯: {matched_keywords})")
                return relevance_score
            
            return 0.0
            
        except Exception as e:
            logging.error(f"è®¡ç®—æ–‡ä»¶å¤¹ç›¸å…³æ€§å¤±è´¥: {e}")
            return 0.0

    def _filter_irrelevant_folders(self, file_content: str, file_summary: str, recommended_paths: List[str], target_directory: str) -> List[str]:
        """
        è¿‡æ»¤æ‰ä¸æ–‡ä»¶å†…å®¹ä¸ç›¸å…³çš„æ–‡ä»¶å¤¹è·¯å¾„ï¼Œå¹¶æŒ‰ç›¸å…³æ€§è¯„åˆ†æ’åº
        
        Args:
            file_content: æ–‡ä»¶å†…å®¹
            file_summary: æ–‡ä»¶æ‘˜è¦
            recommended_paths: AIæ¨èçš„æ–‡ä»¶å¤¹è·¯å¾„åˆ—è¡¨
            target_directory: ç›®æ ‡ç›®å½•
            
        Returns:
            List[str]: æŒ‰ç›¸å…³æ€§è¯„åˆ†æ’åºçš„æ–‡ä»¶å¤¹è·¯å¾„åˆ—è¡¨
        """
        try:
            print(f"ğŸ” å¼€å§‹ç›¸å…³æ€§åˆ†æå’Œæ’åº...")
            print(f"ğŸ“‹ åŸå§‹æ¨èè·¯å¾„: {recommended_paths}")
            
            relevance_scores = {}
            
            for path in recommended_paths:
                # è®¡ç®—ç›¸å…³æ€§è¯„åˆ†
                relevance_score = self._calculate_folder_relevance(file_content, file_summary, path)
                relevance_scores[path] = relevance_score
                
                print(f"ğŸ“Š {path}: ç›¸å…³æ€§è¯„åˆ† {relevance_score:.3f}")
            
            # ç®€åŒ–è¿‡æ»¤é€»è¾‘ï¼šåªè¿‡æ»¤æ‰ç›¸å…³æ€§è¯„åˆ†ä¸º0çš„è·¯å¾„ï¼Œä¿ç•™æ‰€æœ‰æœ‰è¯„åˆ†çš„è·¯å¾„
            filtered_paths = [path for path, score in relevance_scores.items() if score > 0]
            
            if not filtered_paths:
                print(f"âš ï¸ æ‰€æœ‰è·¯å¾„ç›¸å…³æ€§è¯„åˆ†éƒ½ä¸º0ï¼Œè¿”å›åŸå§‹æ¨è")
                return recommended_paths  # å›é€€åˆ°åŸå§‹æ¨èï¼Œè€Œä¸æ˜¯æ‹’ç»åˆ†ç±»
            
            # æŒ‰ç›¸å…³æ€§è¯„åˆ†ä»é«˜åˆ°ä½æ’åº
            sorted_paths = sorted(filtered_paths, key=lambda x: relevance_scores[x], reverse=True)
            
            print(f"âœ… ç›¸å…³æ€§æ’åºå®Œæˆ:")
            for i, path in enumerate(sorted_paths, 1):
                print(f"  {i}. {path} (è¯„åˆ†: {relevance_scores[path]:.3f})")
            
            return sorted_paths
            
        except Exception as e:
            logging.error(f"ç›¸å…³æ€§è¿‡æ»¤å¤±è´¥: {e}")
            print(f"âŒ ç›¸å…³æ€§è¿‡æ»¤å¤±è´¥: {e}")
            return recommended_paths

    def _extract_year_from_filename(self, filename: str) -> Optional[int]:
        """
        ä»æ–‡ä»¶åä¸­æå–å¹´ä»½ä¿¡æ¯
        æ”¯æŒå¤šç§å¹´ä»½æ ¼å¼ï¼š2024ã€2025ã€24ã€25ç­‰
        """
        import re
        
        # æå–4ä½å¹´ä»½ (2024, 2025ç­‰)
        year_patterns = [
            r'20\d{2}',  # 2020-2099
            r'19\d{2}',  # 1900-1999
        ]
        
        for pattern in year_patterns:
            matches = re.findall(pattern, filename)
            if matches:
                # è¿”å›æ‰¾åˆ°çš„ç¬¬ä¸€ä¸ªå¹´ä»½
                return int(matches[0])
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°4ä½å¹´ä»½ï¼Œå°è¯•2ä½å¹´ä»½ (24, 25ç­‰)
        short_year_pattern = r'\b(2[0-9]|1[0-9])\b'  # 10-29
        matches = re.findall(short_year_pattern, filename)
        if matches:
            year = int(matches[0])
            # å‡è®¾20xxå¹´
            if year >= 20:
                return 2000 + year
            else:
                return 1900 + year
        
        return None

    def _check_and_create_year_folder(self, target_folder_path: Path, filename: str) -> Path:
        """
        æ£€æŸ¥ç›®æ ‡æ–‡ä»¶å¤¹ä¸­æ˜¯å¦åŒ…å«æ–‡ä»¶å¹´ä»½ï¼Œå¦‚æœæ²¡æœ‰åˆ™åˆ›å»ºå¹´ä»½å­æ–‡ä»¶å¤¹
        å¹¶å‚è€ƒåŸç›®å½•ç»“æ„åˆ›å»ºåˆé€‚çš„å­ç›®å½•
        """
        # æå–æ–‡ä»¶åä¸­çš„å¹´ä»½
        file_year = self._extract_year_from_filename(filename)
        
        if not file_year:
            print(f"ğŸ“… æ— æ³•ä»æ–‡ä»¶åä¸­æå–å¹´ä»½: {filename}")
            return target_folder_path
        
        print(f"ğŸ“… ä»æ–‡ä»¶åæå–åˆ°å¹´ä»½: {file_year}")
        
        # æ£€æŸ¥ç›®æ ‡æ–‡ä»¶å¤¹ä¸­æ˜¯å¦å·²æœ‰è¯¥å¹´ä»½çš„å­æ–‡ä»¶å¤¹
        year_folder_name = str(file_year)
        
        # æ£€æŸ¥ç›®æ ‡æ–‡ä»¶å¤¹åŠå…¶å­æ–‡ä»¶å¤¹ä¸­æ˜¯å¦åŒ…å«è¯¥å¹´ä»½
        existing_year_folders = []
        if target_folder_path.exists():
            for item in target_folder_path.iterdir():
                if item.is_dir():
                    # æ£€æŸ¥æ–‡ä»¶å¤¹åæ˜¯å¦åŒ…å«å¹´ä»½
                    if year_folder_name in item.name:
                        existing_year_folders.append(item)
                    # é€’å½’æ£€æŸ¥å­æ–‡ä»¶å¤¹
                    for subitem in item.rglob("*"):
                        if subitem.is_dir() and year_folder_name in subitem.name:
                            existing_year_folders.append(subitem)
        
        if existing_year_folders:
            print(f"ğŸ“… æ‰¾åˆ°ç°æœ‰å¹´ä»½æ–‡ä»¶å¤¹: {[str(f) for f in existing_year_folders]}")
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„å¹´ä»½æ–‡ä»¶å¤¹
            return existing_year_folders[0]
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¹´ä»½æ–‡ä»¶å¤¹ï¼Œéœ€è¦åˆ›å»ºæ–°çš„å¹´ä»½æ–‡ä»¶å¤¹ç»“æ„
        # åˆ†æåŸç›®å½•ç»“æ„ï¼Œåˆ›å»ºåˆé€‚çš„å¹´ä»½ç›®å½•
        
        # è·å–ç›®æ ‡ç›®å½•çš„æ ¹ç›®å½•ï¼ˆé€šå¸¸æ˜¯ç›®æ ‡åŸºç¡€ç›®å½•ï¼‰
        # ä¾‹å¦‚ï¼šå¦‚æœç›®æ ‡è·¯å¾„æ˜¯ "F:\æµ©æ€»å‚é˜…èµ„æ–™\ã€01ã€‘ç­–ç•¥æŠ¥å‘Šé›†åˆ\2021\2021å¹´æŠ•èµ„ç­–ç•¥\ä¸­ä¿¡å»ºæŠ•\è¡Œä¸š"
        # æˆ‘ä»¬éœ€è¦æ‰¾åˆ° "F:\æµ©æ€»å‚é˜…èµ„æ–™\ã€01ã€‘ç­–ç•¥æŠ¥å‘Šé›†åˆ" ä½œä¸ºæ ¹ç›®å½•
        
        # æŸ¥æ‰¾åŒ…å«å¹´ä»½çš„ç›®å½•å±‚çº§
        target_parts = list(target_folder_path.parts)
        
        # ä»åå¾€å‰æŸ¥æ‰¾ç¬¬ä¸€ä¸ªåŒ…å«å¹´ä»½çš„ç›®å½•
        year_index = -1
        for i, part in enumerate(target_parts):
            if part.isdigit() and len(part) == 4 and part.startswith('20'):
                year_index = i
                break
        
        if year_index != -1:
            # æ‰¾åˆ°äº†å¹´ä»½ç›®å½•ï¼Œæ›¿æ¢å®ƒ
            new_parts = target_parts.copy()
            new_parts[year_index] = str(file_year)
            new_year_folder_path = Path(*new_parts)
        else:
            # æ²¡æœ‰æ‰¾åˆ°å¹´ä»½ç›®å½•ï¼Œåœ¨ç›®æ ‡ç›®å½•ä¸‹åˆ›å»ºå¹´ä»½æ–‡ä»¶å¤¹
            new_year_folder_path = target_folder_path / year_folder_name
        
        print(f"ğŸ“… åˆ›å»ºæ–°çš„å¹´ä»½ç›®å½•ç»“æ„: {new_year_folder_path}")
        new_year_folder_path.mkdir(parents=True, exist_ok=True)
        
        return new_year_folder_path