#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIæ™ºèƒ½åˆ†ç±»ä¸“ç”¨æ–‡ä»¶æ•´ç†æ ¸å¿ƒæ¨¡å—
"""
import os
import shutil
import logging
import json
from datetime import datetime
from typing import List, Dict, Tuple, Optional, Any
from pathlib import Path
import ollama
from transfer_log_manager import TransferLogManager
import PyPDF2
import docx
import time

class FileOrganizerError(Exception):
    pass

class OllamaClient:
    def __init__(self, model_name: str = None, host: str = "http://localhost:11434"):
        self.model_name = model_name
        self.host = host
        self.client = ollama.Client(host=host)
        self._validate_connection()
    
    def _validate_connection(self) -> None:
        try:
            models_response = self.client.list()
            if hasattr(models_response, 'models'):
                models_list = models_response.models
            elif isinstance(models_response, dict) and 'models' in models_response:
                models_list = models_response['models']
            else:
                models_list = models_response if isinstance(models_response, list) else []
            self.available_models = []
            for model in models_list:
                if isinstance(model, dict):
                    if 'name' in model:
                        model_name = model['name']
                        # ç¡®ä¿æ¨¡å‹åç§°æ˜¯å­—ç¬¦ä¸²æ ¼å¼
                        if isinstance(model_name, str):
                            self.available_models.append(model_name)
                        else:
                            self.available_models.append(str(model_name))
                    elif 'model' in model:
                        model_name = model['model']
                        if isinstance(model_name, str):
                            self.available_models.append(model_name)
                        else:
                            self.available_models.append(str(model_name))
                elif isinstance(model, str):
                    self.available_models.append(model)
                else:
                    # å¦‚æœæ˜¯å…¶ä»–ç±»å‹ï¼Œå°è¯•æå–modelæˆ–nameå±æ€§
                    model_name = None
                    if hasattr(model, 'model'):
                        model_name = getattr(model, 'model')
                    elif hasattr(model, 'name'):
                        model_name = getattr(model, 'name')
                    
                    if model_name:
                        if isinstance(model_name, str):
                            self.available_models.append(model_name)
                        else:
                            self.available_models.append(str(model_name))
                    else:
                        # æœ€åå°è¯•ç›´æ¥è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                        model_str = str(model)
                        # å¦‚æœå­—ç¬¦ä¸²åŒ…å«æ¨¡å‹ä¿¡æ¯ï¼Œå°è¯•æå–
                        if "model='" in model_str:
                            import re
                            match = re.search(r"model='([^']+)'", model_str)
                            if match:
                                self.available_models.append(match.group(1))
                            else:
                                self.available_models.append(model_str)
                        else:
                            self.available_models.append(model_str)
            
            # é€‰æ‹©æ¨¡å‹ - ä¼˜å…ˆä½¿ç”¨qwen3ç³»åˆ—æ¨¡å‹
            if not self.available_models:
                raise FileOrganizerError("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹ï¼Œè¯·å…ˆæ‹‰å–æ¨¡å‹")
            
            if self.model_name and self.model_name in self.available_models:
                logging.info(f"ä½¿ç”¨æŒ‡å®šæ¨¡å‹: {self.model_name}")
            else:
                # è‡ªåŠ¨é€‰æ‹©æ¨¡å‹ï¼šä¼˜å…ˆqwen3ç³»åˆ—ï¼Œå…¶æ¬¡deepseekç³»åˆ—ï¼Œæœ€åå…¶ä»–æ¨¡å‹
                preferred_models = []
                
                # æŸ¥æ‰¾qwen3ç³»åˆ—æ¨¡å‹
                qwen3_models = [m for m in self.available_models if 'qwen3' in m.lower()]
                if qwen3_models:
                    preferred_models.extend(qwen3_models)
                    logging.info(f"æ‰¾åˆ°qwen3ç³»åˆ—æ¨¡å‹: {qwen3_models}")
                
                # æŸ¥æ‰¾deepseekç³»åˆ—æ¨¡å‹
                deepseek_models = [m for m in self.available_models if 'deepseek' in m.lower()]
                if deepseek_models:
                    preferred_models.extend(deepseek_models)
                    logging.info(f"æ‰¾åˆ°deepseekç³»åˆ—æ¨¡å‹: {deepseek_models}")
                
                # æ·»åŠ å…¶ä»–å¯ç”¨æ¨¡å‹
                other_models = [m for m in self.available_models if 'qwen3' not in m.lower() and 'deepseek' not in m.lower()]
                preferred_models.extend(other_models)
                
                if preferred_models:
                    self.model_name = preferred_models[0]
                    if self.model_name is not None:
                        logging.warning(f"æ¨¡å‹ {self.model_name} ä¸å¯ç”¨ï¼Œè‡ªåŠ¨é€‰æ‹©: {self.model_name}")
                    logging.info(f"è‡ªåŠ¨é€‰æ‹©æ¨¡å‹: {self.model_name}")
                else:
                    self.model_name = self.available_models[0]
                    logging.info(f"ä½¿ç”¨é»˜è®¤æ¨¡å‹: {self.model_name}")
            
            logging.info(f"æˆåŠŸè¿æ¥åˆ° Ollamaï¼Œä½¿ç”¨æ¨¡å‹: {self.model_name}")
            logging.info(f"å¯ç”¨æ¨¡å‹åˆ—è¡¨: {self.available_models}")
        except Exception as e:
            raise FileOrganizerError(f"è¿æ¥ Ollama å¤±è´¥: {e}")
    
    def chat_with_retry(self, messages: List[Dict], max_retries: Optional[int] = None) -> str:
        if max_retries is None:
            max_retries = len(self.available_models)
        last_error = None
        models_to_try = [self.model_name] + [m for m in self.available_models if m != self.model_name]
        for attempt, model_name in enumerate(models_to_try[:max_retries]):
            try:
                client = ollama.Client(host=self.host)
                if not isinstance(model_name, str) or not model_name:
                    raise FileOrganizerError("æ¨¡å‹åæ— æ•ˆï¼Œæ— æ³•è°ƒç”¨ chat")
                
                # åŒæ—¶ä½¿ç”¨ä¸‰ç§ç­–ç•¥æŠ‘åˆ¶æ€è€ƒè¿‡ç¨‹
                chat_params = {
                    'model': model_name,
                    'messages': messages,
                    'options': {'enable_thinking': False}  # ç­–ç•¥2ï¼šä¼ é€’enable_thinkingå‚æ•°
                }
                
                response = client.chat(**chat_params)
                
                if model_name != self.model_name:
                    logging.info(f"æ¨¡å‹åˆ‡æ¢: {self.model_name} -> {model_name}")
                    self.model_name = model_name
                return response['message']['content'].strip()
            except Exception as e:
                last_error = e
                logging.warning(f"æ¨¡å‹ {model_name} å“åº”å¤±è´¥: {e}")
                if attempt < max_retries - 1:
                    continue
        raise FileOrganizerError(f"æ‰€æœ‰å¯ç”¨æ¨¡å‹éƒ½å“åº”å¤±è´¥ï¼Œæœ€åé”™è¯¯: {last_error}")

class FileOrganizer:
    def __init__(self, model_name: str = None, enable_transfer_log: bool = True):
        self.model_name = model_name
        self.ollama_client = None
        self.enable_transfer_log = enable_transfer_log
        self.transfer_log_manager = None
        self.setup_logging()
        
        if self.enable_transfer_log:
            self.transfer_log_manager = TransferLogManager()
        
        # åˆå§‹åŒ–AIå‚æ•°
        self.ai_parameters = {
            'similarity_threshold': 0.7,
            'max_retries': 3,
            'content_extraction_length': 3000,
            'summary_length': 200,
            'classification_prompt_template': None
        }
    def setup_logging(self) -> None:
        """è®¾ç½®æ—¥å¿—é…ç½®ï¼Œä»…è¾“å‡ºåˆ°æ§åˆ¶å°"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler()
            ]
        )
        logging.info("æ–‡ä»¶æ•´ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    def initialize_ollama(self) -> None:
        try:
            self.ollama_client = OllamaClient(self.model_name)
            logging.info("Ollama å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            raise FileOrganizerError(f"åˆå§‹åŒ– Ollama å®¢æˆ·ç«¯å¤±è´¥: {e}")
    def scan_target_folders(self, target_directory: str) -> List[str]:
        """æ‰«æç›®æ ‡æ–‡ä»¶å¤¹ï¼Œè¿”å›ç›¸å¯¹è·¯å¾„åˆ—è¡¨ï¼ˆå†…éƒ¨ä½¿ç”¨ï¼Œä¸è¾“å‡ºæ—¥å¿—ï¼‰"""
        try:
            target_path = Path(target_directory)
            if not target_path.exists():
                raise FileOrganizerError(f"ç›®æ ‡ç›®å½•ä¸å­˜åœ¨: {target_directory}")
            folders = []
            for item in target_path.rglob('*'):
                if item.is_dir() and item != target_path:  # æ’é™¤ç›®æ ‡ç›®å½•æœ¬èº«
                    relative_path = item.relative_to(target_path)
                    folders.append(str(relative_path))
            return folders
        except Exception as e:
            raise FileOrganizerError(f"æ‰«æç›®æ ‡æ–‡ä»¶å¤¹å¤±è´¥: {e}")
    def get_directory_tree_structure(self, target_directory: str) -> str:
        """ç”Ÿæˆç›®æ ‡ç›®å½•ç»“æ„ï¼Œè¿”å›å®Œæ•´è·¯å¾„åˆ—è¡¨"""
        try:
            target_path = Path(target_directory)
            if not target_path.exists():
                raise FileOrganizerError(f"ç›®æ ‡ç›®å½•ä¸å­˜åœ¨: {target_directory}")
            
            # ç›´æ¥æ‰«æå¹¶æ„å»ºç›®å½•ç»“æ„
            folders = []
            for item in target_path.rglob('*'):
                if item.is_dir() and item != target_path:  # æ’é™¤ç›®æ ‡ç›®å½•æœ¬èº«
                    relative_path = item.relative_to(target_path)
                    folders.append(str(relative_path))
            
            # æ„å»ºæ¸…æ™°çš„è·¯å¾„åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªå®Œæ•´è·¯å¾„
            path_lines = []
            for i, folder_path in enumerate(folders, 1):
                path_lines.append(f"{i}. {folder_path}")
            
            tree_structure = "\n".join(path_lines)
            logging.info(f"ç”Ÿæˆç›®å½•è·¯å¾„ç»“æ„ï¼Œå…± {len(path_lines)} è¡Œ")
            return tree_structure
        except Exception as e:
            raise FileOrganizerError(f"ç”Ÿæˆç›®å½•æ ‘ç»“æ„å¤±è´¥: {e}")
    def analyze_and_classify_file(self, file_path: str, target_directory: str) -> Dict[str, Any]:
        """
        æ–°çš„æ–‡ä»¶åˆ†æå’Œåˆ†ç±»æ–¹æ³•ï¼šå…ˆè§£æå†…å®¹ï¼Œç”Ÿæˆæ‘˜è¦ï¼Œå†æ¨èç›®å½•
        è¿”å›åŒ…å«æå–å†…å®¹ã€æ‘˜è¦å’Œæ¨èç›®å½•çš„å®Œæ•´ç»“æœ
        """
        start_time = time.time()
        timing_info = {}
        
        try:
            if not self.ollama_client:
                init_start = time.time()
                self.initialize_ollama()
                timing_info['ollama_init_time'] = round(time.time() - init_start, 3)
            
            file_name = Path(file_path).name
            logging.info(f"å¼€å§‹åˆ†ææ–‡ä»¶: {file_name}")
            
            # ç¬¬ä¸€æ­¥ï¼šè§£ææ–‡ä»¶å†…å®¹
            extract_start = time.time()
            extracted_content = self._extract_file_content(file_path)
            extract_time = round(time.time() - extract_start, 3)
            timing_info['content_extraction_time'] = extract_time
            logging.info(f"æ–‡ä»¶å†…å®¹æå–å®Œæˆï¼Œé•¿åº¦: {len(extracted_content)} å­—ç¬¦ï¼Œè€—æ—¶: {extract_time}ç§’")
            
            # æ ¹æ®è®¾ç½®æˆªå–å†…å®¹ç”¨äºåç»­AIå¤„ç†ï¼Œæé«˜å¤„ç†æ•ˆç‡
            truncate_length = self.ai_parameters['content_extraction_length'] if self.ai_parameters['content_extraction_length'] < 2000 else len(extracted_content)
            content_for_ai = extracted_content[:truncate_length] if extracted_content else ""
            if len(extracted_content) > truncate_length:
                logging.info(f"å†…å®¹å·²æˆªå–è‡³å‰{truncate_length}å­—ç¬¦ç”¨äºAIå¤„ç†ï¼ˆåŸé•¿åº¦: {len(extracted_content)} å­—ç¬¦ï¼‰")
            
            # ç¬¬äºŒæ­¥ï¼šç”Ÿæˆ100å­—æ‘˜è¦ï¼ˆä½¿ç”¨æˆªå–åçš„å†…å®¹ï¼‰
            summary_start = time.time()
            summary = self._generate_content_summary(content_for_ai, file_name)
            summary_time = round(time.time() - summary_start, 3)
            timing_info['summary_generation_time'] = summary_time
            logging.info(f"å†…å®¹æ‘˜è¦ç”Ÿæˆå®Œæˆ: {summary[:50]}...ï¼Œè€—æ—¶: {summary_time}ç§’")
            
            # ç¬¬ä¸‰æ­¥ï¼šæ¨èæœ€åŒ¹é…çš„å­˜æ”¾ç›®å½•ï¼ˆä½¿ç”¨æˆªå–åçš„å†…å®¹ï¼‰
            recommend_start = time.time()
            recommended_folder, match_reason = self._recommend_target_folder(
                file_path, content_for_ai, summary, target_directory
            )
            recommend_time = round(time.time() - recommend_start, 3)
            timing_info['folder_recommendation_time'] = recommend_time
            
            total_time = round(time.time() - start_time, 3)
            timing_info['total_processing_time'] = total_time
            
            result = {
                'file_path': file_path,
                'file_name': file_name,
                'extracted_content': extracted_content,
                'content_summary': summary,
                'recommended_folder': recommended_folder,
                'match_reason': match_reason,
                'success': recommended_folder is not None,
                'timing_info': timing_info
            }
            
            if recommended_folder:
                logging.info(f"æ–‡ä»¶åˆ†æå®Œæˆ: {file_name} -> {recommended_folder}ï¼Œæ€»è€—æ—¶: {total_time}ç§’")
                logging.info(f"æ‘˜è¦: {summary}")
                logging.info(f"æ¨èç†ç”±: {match_reason}")
                logging.info(f"è¯¦ç»†è€—æ—¶ - å†…å®¹æå–: {extract_time}ç§’, æ‘˜è¦ç”Ÿæˆ: {summary_time}ç§’, ç›®å½•æ¨è: {recommend_time}ç§’")
            else:
                logging.warning(f"æ–‡ä»¶åˆ†æå¤±è´¥: {file_name}ï¼Œæ€»è€—æ—¶: {total_time}ç§’")
            
            return result
            
        except Exception as e:
            total_time = round(time.time() - start_time, 3)
            timing_info['total_processing_time'] = total_time
            logging.error(f"æ–‡ä»¶åˆ†æå¤±è´¥: {e}ï¼Œæ€»è€—æ—¶: {total_time}ç§’")
            return {
                'file_path': file_path,
                'file_name': Path(file_path).name,
                'extracted_content': '',
                'content_summary': '',
                'recommended_folder': None,
                'match_reason': f"åˆ†æå¤±è´¥: {str(e)}",
                'success': False,
                'error': str(e),
                'timing_info': timing_info
            }
    
    def classify_file(self, file_path: str, target_directory: str) -> tuple:
        """
        ä¿æŒåŸæœ‰çš„åˆ†ç±»æ–¹æ³•å…¼å®¹æ€§
        """
        result = self.analyze_and_classify_file(file_path, target_directory)
        return result['recommended_folder'], result['match_reason'], result['success']
    def _build_classification_prompt(self, file_path: str, target_directory: str) -> str:
        file_name = Path(file_path).name
        file_extension = Path(file_path).suffix.lower()
        file_content = ""
        content_readable = False
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                file_content = f.read()[:2000]
                printable_ratio = sum(1 for c in file_content if c.isprintable() or c.isspace()) / len(file_content) if file_content else 0
                if printable_ratio > 0.7:
                    content_readable = True
                else:
                    file_content = "æ–‡ä»¶å†…å®¹ä¸ºäºŒè¿›åˆ¶æ ¼å¼ï¼Œæ— æ³•è¯»å–"
        except Exception:
            for encoding in ['gbk', 'gb2312', 'latin-1']:
                try:
                    with open(file_path, 'r', encoding=encoding, errors='ignore') as f:
                        file_content = f.read()[:2000]
                        printable_ratio = sum(1 for c in file_content if c.isprintable() or c.isspace()) / len(file_content) if file_content else 0
                        if printable_ratio > 0.7:
                            content_readable = True
                            break
                        else:
                            file_content = "æ–‡ä»¶å†…å®¹ä¸ºäºŒè¿›åˆ¶æ ¼å¼ï¼Œæ— æ³•è¯»å–"
                except Exception:
                    continue
            if not content_readable:
                file_content = "æ— æ³•è¯»å–æ–‡ä»¶å†…å®¹"
        directory_structure = self.get_directory_tree_structure(target_directory)
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡ä»¶åˆ†ç±»åŠ©æ‰‹ã€‚è¯·é˜…è¯»åŸæ–‡ä»¶çš„å‰500ä¸ªå­—ï¼Œæˆ–è€…è¯†åˆ«ç¬¬ä¸€é¡µå†…å®¹ï¼Œæ ¹æ®æ–‡ä»¶å†…å®¹åˆ¤æ–­æ–‡ä»¶åº”è¯¥å½’ç±»åˆ°ä¸‹åˆ—å“ªä¸ªæ–‡ä»¶å¤¹ä¸­ï¼Œæ³¨æ„ï¼Œæ¯ä¸ªæ–‡ä»¶æ ¹æ®æ–‡ä»¶å†…å®¹åªåŒ¹é…ä¸€ä¸ªæ–‡ä»¶å¤¹ï¼ŒåŒ¹é…åŸåˆ™æ˜¯ï¼š

- ä¼˜å…ˆåŒ¹é…ï¼šæ–‡ä»¶å†…å®¹ä¸»é¢˜å’Œæ–‡ä»¶å¤¹ååŠæ–‡ä»¶è·¯å¾„åæœ€ç›¸ç¬¦
- é™çº§åŒ¹é…ï¼šå¦‚æœè¯»å–ä¸åˆ°æ–‡ä»¶å†…å®¹ï¼Œåˆ™ç”¨åŸæ–‡ä»¶åå’Œç›®æ ‡æ–‡ä»¶å¤¹ååŠæ–‡ä»¶å¤¹è·¯å¾„åæ¥åŒ¹é…

æ–‡ä»¶ä¿¡æ¯ï¼š
- åŸæ–‡ä»¶åï¼š{file_name}
- æ–‡ä»¶æ‰©å±•åï¼š{file_extension}
- æ–‡ä»¶å†…å®¹ï¼š{file_content if content_readable else "æ— æ³•è¯»å–å†…å®¹"}

ç›®æ ‡ç›®å½•æ–‡ä»¶å¤¹æ¸…å•å¦‚ä¸‹ï¼š
{directory_structure}

è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
åŸæ–‡ä»¶å|ç›®æ ‡æ–‡ä»¶å¤¹|åŒ¹é…ç†ç”±

å…¶ä¸­åŒ¹é…ç†ç”±æ ¼å¼ä¸ºï¼š
å†…å®¹åŒ¹é…ï¼š{{ç”¨ä¸å¤šäº20ä¸ªå­—æè¿°åŒ¹é…ç†ç”±}}
æˆ–è€…
æ— æ³•è¯»å–å†…å®¹ï¼Œé‡‡ç”¨æ–‡ä»¶å¤¹ååŒ¹é…ï¼š{{ç”¨ä¸å¤šäº20ä¸ªå­—æè¿°åŒ¹é…ç†ç”±}}

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ ¼å¼è¾“å‡ºï¼Œåªè¾“å‡ºä¸€è¡Œç»“æœã€‚
"""
        return prompt
    
    def _extract_file_content(self, file_path: str, max_length: int = 3000) -> str:
        """
        æå–æ–‡ä»¶å†…å®¹ï¼ˆä»file_reader.pyå¤åˆ¶çš„å®Œæ•´å®ç°ï¼‰
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            max_length: æœ€å¤§æå–é•¿åº¦
            
        Returns:
            æå–çš„æ–‡ä»¶å†…å®¹
        """
        try:
            file_path = Path(file_path)
            if not file_path.exists():
                raise Exception(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            
            file_extension = file_path.suffix.lower()
            logging.info(f"æ­£åœ¨æå–æ–‡ä»¶å†…å®¹: {file_path.name} (ç±»å‹: {file_extension})")
            
            # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©ä¸åŒçš„æå–æ–¹æ³•
            if file_extension == '.pdf':
                return self._extract_pdf_content(file_path, max_length)
            elif file_extension in ['.docx', '.doc']:
                return self._extract_docx_content(file_path, max_length)
            elif file_extension in ['.txt', '.md', '.py', '.js', '.html', '.css', '.json', '.xml', '.csv']:
                return self._extract_text_content(file_path, max_length)
            elif file_extension in ['.jpg', '.jpeg', '.png', '.bmp', '.gif']:
                return self._extract_image_info(file_path)
            else:
                # å°è¯•ä½œä¸ºæ–‡æœ¬æ–‡ä»¶è¯»å–
                return self._extract_text_content(file_path, max_length)
                
        except Exception as e:
            logging.error(f"æå–æ–‡ä»¶å†…å®¹å¤±è´¥: {e}")
            return f"æ— æ³•æå–æ–‡ä»¶å†…å®¹: {str(e)}"
    
    def _extract_pdf_content(self, file_path: Path, max_length: int) -> str:
        """
        æå–PDFæ–‡ä»¶å†…å®¹ï¼ˆä»file_reader.pyå¤åˆ¶çš„å®Œæ•´å®ç°ï¼‰
        """
        try:
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                content = ""
                
                # è¯»å–å‰å‡ é¡µå†…å®¹
                for page_num in range(min(3, len(pdf_reader.pages))):
                    page = pdf_reader.pages[page_num]
                    content += page.extract_text() + "\n"
                    
                    if len(content) >= max_length:
                        break
                
                return content[:max_length] if content else "PDFæ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–æ— æ³•æå–"
                
        except Exception as e:
            return f"PDFæ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}"
    
    def _extract_docx_content(self, file_path: Path, max_length: int) -> str:
        """
        æå–Wordæ–‡æ¡£å†…å®¹ï¼ˆä»file_reader.pyå¤åˆ¶çš„å®Œæ•´å®ç°ï¼‰
        """
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”å¯è¯»
            if not file_path.exists():
                return "æ–‡ä»¶ä¸å­˜åœ¨"
            
            if not file_path.is_file():
                return "è·¯å¾„ä¸æ˜¯æ–‡ä»¶"
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = file_path.stat().st_size
            if file_size == 0:
                return "æ–‡ä»¶ä¸ºç©º"
            
            # å°è¯•è¯»å–Wordæ–‡æ¡£
            try:
                doc = docx.Document(file_path)
                content = ""
                
                # æå–æ®µè½å†…å®¹
                for paragraph in doc.paragraphs:
                    if paragraph.text.strip():  # è·³è¿‡ç©ºæ®µè½
                        content += paragraph.text.strip() + "\n"
                        if len(content) >= max_length:
                            break
                
                # å¦‚æœæ®µè½å†…å®¹ä¸ºç©ºï¼Œå°è¯•æå–è¡¨æ ¼å†…å®¹
                if not content.strip():
                    for table in doc.tables:
                        for row in table.rows:
                            for cell in row.cells:
                                if cell.text.strip():
                                    content += cell.text.strip() + " "
                        content += "\n"
                        if len(content) >= max_length:
                            break
                
                return content[:max_length].strip() if content.strip() else "Wordæ–‡æ¡£å†…å®¹ä¸ºç©ºæˆ–æ— æ³•æå–"
                
            except Exception as docx_error:
                # å¦‚æœæ˜¯.docæ–‡ä»¶æˆ–æŸåçš„.docxæ–‡ä»¶ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
                if file_path.suffix.lower() == '.doc':
                    return "ä¸æ”¯æŒ.docæ ¼å¼ï¼Œè¯·è½¬æ¢ä¸º.docxæ ¼å¼"
                else:
                    return f"Wordæ–‡æ¡£æ ¼å¼é”™è¯¯æˆ–æ–‡ä»¶æŸå: {str(docx_error)}"
            
        except Exception as e:
            return f"Wordæ–‡æ¡£è¯»å–å¤±è´¥: {str(e)}"
    
    def _extract_text_content(self, file_path: Path, max_length: int) -> str:
        """
        æå–æ–‡æœ¬æ–‡ä»¶å†…å®¹ï¼ˆä»file_reader.pyå¤åˆ¶çš„å®Œæ•´å®ç°ï¼‰
        """
        try:
            # å°è¯•å¤šç§ç¼–ç æ ¼å¼
            encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1']
            
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding, errors='ignore') as f:
                        content = f.read(max_length)
                        
                        # æ£€æŸ¥å†…å®¹æ˜¯å¦å¯è¯»
                        printable_ratio = sum(1 for c in content if c.isprintable() or c.isspace()) / len(content) if content else 0
                        if printable_ratio > 0.7:
                            return content
                        
                except Exception:
                    continue
            
            return "æ–‡ä»¶å†…å®¹ä¸ºäºŒè¿›åˆ¶æ ¼å¼ï¼Œæ— æ³•è¯»å–"
            
        except Exception as e:
            return f"æ–‡æœ¬æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}"
    
    def _extract_image_info(self, file_path: Path) -> str:
        """
        æå–å›¾ç‰‡æ–‡ä»¶ä¿¡æ¯ï¼ˆä»file_reader.pyå¤åˆ¶çš„å®Œæ•´å®ç°ï¼‰
        """
        try:
            from PIL import Image
            with Image.open(file_path) as img:
                info = f"å›¾ç‰‡æ–‡ä»¶ä¿¡æ¯:\n"
                info += f"æ ¼å¼: {img.format}\n"
                info += f"å°ºå¯¸: {img.size[0]} x {img.size[1]}\n"
                info += f"æ¨¡å¼: {img.mode}\n"
                
                # å¦‚æœæœ‰EXIFä¿¡æ¯ï¼Œæå–ä¸€äº›åŸºæœ¬ä¿¡æ¯
                if hasattr(img, '_getexif') and img._getexif():
                    info += "åŒ…å«EXIFä¿¡æ¯\n"
                
                return info
                
        except Exception as e:
            return f"å›¾ç‰‡æ–‡ä»¶ä¿¡æ¯æå–å¤±è´¥: {str(e)}"
    
    def _generate_content_summary(self, content: str, file_name: str, summary_length: int = None) -> str:
        """
        ç”Ÿæˆæ–‡ä»¶å†…å®¹æ‘˜è¦
        
        Args:
            content: æ–‡ä»¶å†…å®¹
            file_name: æ–‡ä»¶å
            summary_length: æ‘˜è¦é•¿åº¦ï¼ˆå­—æ•°ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
        """
        try:
            if not content or content.startswith("æ— æ³•") or content.startswith("æ–‡ä»¶å†…å®¹ä¸ºäºŒè¿›åˆ¶"):
                return f"æ— æ³•ç”Ÿæˆæ‘˜è¦ï¼š{content[:50]}..."
            
            if not self.ollama_client:
                self.initialize_ollama()
            
            # ä½¿ç”¨ä¼ å…¥çš„æ‘˜è¦é•¿åº¦ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
            target_length = summary_length if summary_length is not None else self.ai_parameters['summary_length']
            
            # æ„å»ºæ›´æ˜ç¡®çš„æç¤ºè¯ï¼Œé¿å…æ€è€ƒè¿‡ç¨‹è¾“å‡º
            prompt = f"""è¯·ä¸ºä»¥ä¸‹æ–‡ä»¶å†…å®¹ç”Ÿæˆä¸€ä¸ª{target_length}å­—ä»¥å†…çš„ä¸­æ–‡æ‘˜è¦ã€‚

æ–‡ä»¶åï¼š{file_name}

æ–‡ä»¶å†…å®¹ï¼š
{content}

è¦æ±‚ï¼š
1. æ¦‚æ‹¬æ–‡ä»¶çš„ä¸»è¦å†…å®¹å’Œä¸»é¢˜
2. çªå‡ºå…³é”®ä¿¡æ¯å’Œè¦ç‚¹
3. è¯­è¨€ç®€æ´æ˜äº†
4. å­—æ•°æ§åˆ¶åœ¨{target_length}å­—ä»¥å†…
5. ç›´æ¥è¾“å‡ºæ‘˜è¦å†…å®¹ï¼Œä¸è¦åŒ…å«ä»»ä½•æ€è€ƒè¿‡ç¨‹æˆ–è¯´æ˜æ–‡å­—
6. ä¸è¦ä½¿ç”¨"<think>"æ ‡ç­¾æˆ–ä»»ä½•æ€è€ƒè¿‡ç¨‹æè¿°

æ‘˜è¦ï¼š/no_think"""

            # åœ¨ä¼ é€’ç»™å¤§æ¨¡å‹çš„å®Œæ•´å†…å®¹æœ€å°¾éƒ¨æ·»åŠ /no_thinkæ ‡ç­¾
            final_prompt = prompt

            # ä½¿ç”¨ç³»ç»Ÿæç¤ºè¯æ¥æŠ‘åˆ¶æ€è€ƒè¿‡ç¨‹
            messages = [
                {
                    'role': 'system',
                    'content': 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£æ‘˜è¦åŠ©æ‰‹ã€‚é‡è¦ï¼šä¸è¦è¾“å‡ºä»»ä½•æ¨ç†è¿‡ç¨‹ã€æ€è€ƒæ­¥éª¤æˆ–è§£é‡Šã€‚ç›´æ¥æŒ‰è¦æ±‚è¾“å‡ºç»“æœã€‚åªè¾“å‡ºæ‘˜è¦å†…å®¹ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–ä¿¡æ¯ã€‚'
                },
                {
                    'role': 'user',
                    'content': final_prompt
                }
            ]

            summary = self.ollama_client.chat_with_retry(messages)
            
            # æ¸…ç†å¯èƒ½çš„æ€è€ƒè¿‡ç¨‹æ ‡ç­¾å’Œå†…å®¹
            summary = summary.replace('<think>', '').replace('</think>', '').strip()
            
            # ç§»é™¤å¸¸è§çš„æ€è€ƒè¿‡ç¨‹å¼€å¤´
            think_prefixes = [
                'å¥½çš„ï¼Œ', 'å¥½ï¼Œ', 'å—¯ï¼Œ', 'æˆ‘æ¥', 'æˆ‘éœ€è¦', 'é¦–å…ˆï¼Œ', 'è®©æˆ‘', 'ç°åœ¨æˆ‘è¦',
                'ç”¨æˆ·å¸Œæœ›', 'ç”¨æˆ·è¦æ±‚', 'ç”¨æˆ·è®©æˆ‘', 'æ ¹æ®', 'åŸºäº', 'è€ƒè™‘åˆ°', 'è®©æˆ‘å…ˆä»”ç»†çœ‹çœ‹',
                'ç”¨æˆ·ç»™äº†æˆ‘è¿™ä¸ªæŸ¥è¯¢', 'ç”¨æˆ·ç»™äº†æˆ‘è¿™ä¸ªä»»åŠ¡', 'ç”¨æˆ·ç»™äº†ä¸€ä¸ªä»»åŠ¡',
                'é¦–å…ˆï¼Œæˆ‘å¾—çœ‹ä¸€ä¸‹', 'é¦–å…ˆï¼Œæˆ‘è¦ç†è§£', 'é¦–å…ˆï¼Œæˆ‘å¾—ä»”ç»†çœ‹çœ‹',
                'å¥½çš„ï¼Œç”¨æˆ·è®©æˆ‘', 'ç”¨æˆ·è®©æˆ‘ç”Ÿæˆ', 'å†…å®¹æ¥è‡ªæ–‡ä»¶', 'é‡ç‚¹åŒ…æ‹¬', 'é¦–å…ˆï¼Œæˆ‘éœ€è¦ç¡®è®¤'
            ]
            
            # æ›´æ¿€è¿›çš„æ¸…ç†ï¼šç§»é™¤æ‰€æœ‰ä»¥æ€è€ƒè¿‡ç¨‹å¼€å¤´çš„å¥å­
            lines = summary.split('\n')
            cleaned_lines = []
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦ä»¥æ€è€ƒè¿‡ç¨‹å¼€å¤´
                is_think_line = False
                for prefix in think_prefixes:
                    if line.lower().startswith(prefix.lower()):
                        is_think_line = True
                        break
                
                # å¦‚æœä¸æ˜¯æ€è€ƒè¿‡ç¨‹ï¼Œä¿ç•™è¿™ä¸€è¡Œ
                if not is_think_line:
                    cleaned_lines.append(line)
            
            # å¦‚æœæ¸…ç†åæ²¡æœ‰å†…å®¹ï¼Œå°è¯•æ›´ç®€å•çš„æ–¹æ³•
            if not cleaned_lines:
                # æ‰¾åˆ°ç¬¬ä¸€ä¸ªä¸æ˜¯æ€è€ƒè¿‡ç¨‹çš„å¥å­
                sentences = summary.split('ã€‚')
                for sentence in sentences:
                    sentence = sentence.strip()
                    if not sentence:
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦ä»¥æ€è€ƒè¿‡ç¨‹å¼€å¤´
                    is_think_sentence = False
                    for prefix in think_prefixes:
                        if sentence.lower().startswith(prefix.lower()):
                            is_think_sentence = True
                            break
                    
                    if not is_think_sentence:
                        cleaned_lines.append(sentence)
                        break
            
            # å¦‚æœè¿˜æ˜¯æ²¡æœ‰å†…å®¹ï¼Œä½¿ç”¨åŸå§‹æ‘˜è¦çš„æœ€åä¸€éƒ¨åˆ†
            if not cleaned_lines:
                # å–æœ€å100ä¸ªå­—ç¬¦ä½œä¸ºæ‘˜è¦
                summary = summary[-100:] if len(summary) > 100 else summary
            
            # é‡æ–°ç»„åˆæ¸…ç†åçš„å†…å®¹
            if cleaned_lines:
                summary = 'ã€‚'.join(cleaned_lines)
            
            # ç¡®ä¿æ‘˜è¦é•¿åº¦ä¸è¶…è¿‡é™åˆ¶
            if len(summary) > target_length:
                summary = summary[:target_length-3] + "..."
            
            return summary.strip()
            
        except Exception as e:
            logging.error(f"ç”Ÿæˆæ‘˜è¦å¤±è´¥: {e}")
            return f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def _clean_ai_response(self, response: str) -> str:
        """æ¸…ç†AIå“åº”ä¸­çš„æ€è€ƒè¿‡ç¨‹"""
        if not response:
            return response
        
        print(f"ğŸ”§ å¼€å§‹æ¸…ç†AIå“åº”ï¼ŒåŸå§‹é•¿åº¦: {len(response)}")
        print(f"ğŸ”§ åŸå§‹å“åº”: {response}")
        
        # æ¸…ç†å¯èƒ½çš„æ€è€ƒè¿‡ç¨‹æ ‡ç­¾å’Œå†…å®¹
        response = response.replace('<think>', '').replace('</think>', '').strip()
        
        # ç§»é™¤å¸¸è§çš„æ€è€ƒè¿‡ç¨‹å¼€å¤´
        think_prefixes = [
            'å¥½çš„ï¼Œ', 'å¥½ï¼Œ', 'å—¯ï¼Œ', 'æˆ‘æ¥', 'æˆ‘éœ€è¦', 'é¦–å…ˆï¼Œ', 'è®©æˆ‘', 'ç°åœ¨æˆ‘è¦',
            'ç”¨æˆ·å¸Œæœ›', 'ç”¨æˆ·è¦æ±‚', 'ç”¨æˆ·è®©æˆ‘', 'æ ¹æ®', 'åŸºäº', 'è€ƒè™‘åˆ°', 'è®©æˆ‘å…ˆä»”ç»†çœ‹çœ‹',
            'ç”¨æˆ·ç»™äº†æˆ‘è¿™ä¸ªæŸ¥è¯¢', 'ç”¨æˆ·ç»™äº†æˆ‘è¿™ä¸ªä»»åŠ¡', 'ç”¨æˆ·ç»™äº†ä¸€ä¸ªä»»åŠ¡',
            'é¦–å…ˆï¼Œæˆ‘å¾—çœ‹ä¸€ä¸‹', 'é¦–å…ˆï¼Œæˆ‘è¦ç†è§£', 'é¦–å…ˆï¼Œæˆ‘å¾—ä»”ç»†çœ‹çœ‹',
            'å¥½çš„ï¼Œç”¨æˆ·è®©æˆ‘', 'ç”¨æˆ·è®©æˆ‘ç”Ÿæˆ', 'å†…å®¹æ¥è‡ªæ–‡ä»¶', 'é‡ç‚¹åŒ…æ‹¬', 'é¦–å…ˆï¼Œæˆ‘éœ€è¦ç¡®è®¤'
        ]
        
        # æ›´æ¿€è¿›çš„æ¸…ç†ï¼šç§»é™¤æ‰€æœ‰ä»¥æ€è€ƒè¿‡ç¨‹å¼€å¤´çš„å¥å­
        lines = response.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # æ£€æŸ¥æ˜¯å¦ä»¥æ€è€ƒè¿‡ç¨‹å¼€å¤´
            is_think_line = False
            for prefix in think_prefixes:
                if line.lower().startswith(prefix.lower()):
                    is_think_line = True
                    break
            
            # å¦‚æœä¸æ˜¯æ€è€ƒè¿‡ç¨‹ï¼Œä¿ç•™è¿™ä¸€è¡Œ
            if not is_think_line:
                cleaned_lines.append(line)
        
        print(f"ğŸ”§ æ¸…ç†åè¡Œæ•°: {len(cleaned_lines)}")
        
        # å¦‚æœæ¸…ç†åæ²¡æœ‰å†…å®¹ï¼Œå°è¯•æ›´ç®€å•çš„æ–¹æ³•
        if not cleaned_lines:
            print(f"ğŸ”§ æ¸…ç†åæ²¡æœ‰å†…å®¹ï¼Œå°è¯•å¥å­åˆ†å‰²æ–¹æ³•")
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªä¸æ˜¯æ€è€ƒè¿‡ç¨‹çš„å¥å­
            sentences = response.split('ã€‚')
            for sentence in sentences:
                sentence = sentence.strip()
                if not sentence:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦ä»¥æ€è€ƒè¿‡ç¨‹å¼€å¤´
                is_think_sentence = False
                for prefix in think_prefixes:
                    if sentence.lower().startswith(prefix.lower()):
                        is_think_sentence = True
                        break
                
                if not is_think_sentence:
                    cleaned_lines.append(sentence)
                    break
        
        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰å†…å®¹ï¼Œä½¿ç”¨åŸå§‹å“åº”çš„æœ€åä¸€éƒ¨åˆ†
        if not cleaned_lines:
            print(f"ğŸ”§ ä»ç„¶æ²¡æœ‰å†…å®¹ï¼Œä½¿ç”¨æœ€å100ä¸ªå­—ç¬¦")
            # å–æœ€å100ä¸ªå­—ç¬¦
            response = response[-100:] if len(response) > 100 else response
            print(f"ğŸ”§ æˆªæ–­åå“åº”: {response}")
            return response
        
        # é‡æ–°ç»„åˆæ¸…ç†åçš„å†…å®¹ï¼Œä¿æŒæ¢è¡Œç¬¦ä»¥æ”¯æŒå¤šè¡Œæ¨èè·¯å¾„
        if cleaned_lines:
            response = '\n'.join(cleaned_lines)
        
        print(f"ğŸ”§ æœ€ç»ˆæ¸…ç†ç»“æœ: {response}")
        return response.strip()
    
    def _recommend_target_folder(self, file_path: str, content: str, summary: str, target_directory: str, retry_count: int = 0) -> tuple:
        """
        åŸºäºæ–‡ä»¶å†…å®¹å’Œæ‘˜è¦æ¨èæœ€åŒ¹é…çš„ç›®æ ‡æ–‡ä»¶å¤¹
        æ”¯æŒé‡è¯•æœºåˆ¶ï¼Œå½“AIè¿”å›ä¸å­˜åœ¨çš„æ–‡ä»¶å¤¹æ—¶è‡ªåŠ¨é‡è¯•
        """
        try:
            if not self.ollama_client:
                self.initialize_ollama()
            
            file_name = Path(file_path).name
            file_full_path = str(Path(file_path).absolute())
            
            directory_structure = self.get_directory_tree_structure(target_directory)
            
            print(f"ğŸ“„ æºæ–‡ä»¶å®Œæ•´è·¯å¾„: {file_full_path}")
            print(f"ğŸ“‹ ç›®å½•ç»“æ„:\n{directory_structure}")
            if retry_count > 0:
                print(f"ğŸ”„ ç¬¬ {retry_count} æ¬¡é‡è¯•AIåˆ†ç±»")
            
            # å­˜å‚¨AIæ¨èçš„å®Œæ•´è·¯å¾„
            self.recommended_folder_path = None
            # å­˜å‚¨æºæ–‡ä»¶å®Œæ•´è·¯å¾„
            self.source_file_path = file_full_path
            
            # åˆ¤æ–­æ˜¯å¦æœ‰æœ‰æ•ˆçš„å†…å®¹å’Œæ‘˜è¦
            has_valid_content = content and not content.startswith("æ— æ³•") and not content.startswith("æ–‡ä»¶å†…å®¹ä¸ºäºŒè¿›åˆ¶")
            has_valid_summary = summary and not summary.startswith("æ— æ³•") and not summary.startswith("æ‘˜è¦ç”Ÿæˆå¤±è´¥")
            
            print(f"ğŸ“„ æ–‡ä»¶å†…å®¹æœ‰æ•ˆ: {has_valid_content}")
            print(f"ğŸ“ æ‘˜è¦æœ‰æ•ˆ: {has_valid_summary}")
            if has_valid_summary:
                print(f"ğŸ“ æ‘˜è¦å†…å®¹: {summary[:100]}...")
            
            # æ„å»ºæ›´æ˜ç¡®çš„åˆ†ç±»æç¤ºè¯ï¼Œè¦æ±‚è¿”å›å‰ä¸‰ä¸ªåŒ¹é…åº¦æœ€é«˜çš„è·¯å¾„
            if has_valid_content and has_valid_summary:
                prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡ä»¶åˆ†ç±»åŠ©æ‰‹ã€‚è¯·æ ¹æ®æ–‡ä»¶ä¿¡æ¯æ¨èåŒ¹é…åº¦å‰ä¸‰çš„ç›®æ ‡æ–‡ä»¶å¤¹è·¯å¾„ã€‚

æ–‡ä»¶ä¿¡æ¯ï¼š
- æºæ–‡ä»¶å®Œæ•´è·¯å¾„ï¼š{file_full_path}
- æ–‡ä»¶åï¼š{file_name}
- å†…å®¹æ‘˜è¦ï¼š{summary}

å¯é€‰çš„ç›®æ ‡æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆå¿…é¡»ä¸¥æ ¼ä»ä»¥ä¸‹åˆ—è¡¨ä¸­é€‰æ‹©ï¼Œä¸èƒ½ä¿®æ”¹è·¯å¾„ï¼‰ï¼š
{directory_structure}

åˆ†ç±»è¦æ±‚ï¼š
1. å¿…é¡»ä¸¥æ ¼ä»ä¸Šè¿°æ–‡ä»¶å¤¹è·¯å¾„åˆ—è¡¨ä¸­å¤åˆ¶å®Œæ•´çš„è·¯å¾„
2. ä¸èƒ½ä¿®æ”¹ã€ç¼©å†™æˆ–æ·»åŠ ä»»ä½•å†…å®¹åˆ°è·¯å¾„
3. ä¸èƒ½åˆ›å»ºæˆ–æƒ³è±¡ä¸å­˜åœ¨çš„æ–‡ä»¶å¤¹
4. ä¼˜å…ˆæ ¹æ®æ–‡ä»¶å†…å®¹ä¸»é¢˜åŒ¹é…æ–‡ä»¶å¤¹
5. æŒ‰åŒ¹é…åº¦ä»é«˜åˆ°ä½è¿”å›å‰ä¸‰ä¸ªè·¯å¾„
6. æ¯è¡Œä¸€ä¸ªè·¯å¾„ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–å†…å®¹
7. ä¸è¦ä½¿ç”¨"<think>"æ ‡ç­¾æˆ–ä»»ä½•æ€è€ƒè¿‡ç¨‹æè¿°
8. å¦‚æœåŒ¹é…åº¦ç›¸è¿‘ï¼Œä¼˜å…ˆé€‰æ‹©æ›´å…·ä½“çš„æ–‡ä»¶å¤¹

è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼æŒ‰æ­¤æ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªå®Œæ•´è·¯å¾„ï¼‰ï¼š
ç¬¬ä¸€æ¨èï¼š[å®Œæ•´è·¯å¾„1]
ç¬¬äºŒæ¨èï¼š[å®Œæ•´è·¯å¾„2]
ç¬¬ä¸‰æ¨èï¼š[å®Œæ•´è·¯å¾„3]

ç¤ºä¾‹è¾“å‡ºï¼š
ç¬¬ä¸€æ¨èï¼šã€7-4-10ã€‘æ–°å…´ä¸šæ€/ã€7-4-9-10ã€‘åŒ»ç–—ä¿é™©
ç¬¬äºŒæ¨èï¼šã€7-4ã€‘ä¿é™©/å¥åº·é™©
ç¬¬ä¸‰æ¨èï¼šã€7-4-5ã€‘äººèº«é™©

è¯·å¼€å§‹æ¨èï¼š"""
            else:
                # æ— æ³•è·å–æœ‰æ•ˆå†…å®¹æ—¶ï¼Œä½¿ç”¨æ–‡ä»¶åè¿›è¡Œåˆ†ç±»ï¼ŒåŒæ ·è¿”å›å‰ä¸‰ä¸ªæ¨è
                file_extension = Path(file_name).suffix.lower()
                
                prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡ä»¶åˆ†ç±»åŠ©æ‰‹ã€‚ç”±äºæ— æ³•è¯»å–æ–‡ä»¶å†…å®¹ï¼Œè¯·æ ¹æ®æ–‡ä»¶åå’Œæ‰©å±•åæ¨èåŒ¹é…åº¦å‰ä¸‰çš„ç›®æ ‡æ–‡ä»¶å¤¹è·¯å¾„ã€‚

æ–‡ä»¶ä¿¡æ¯ï¼š
- æºæ–‡ä»¶å®Œæ•´è·¯å¾„ï¼š{file_full_path}
- æ–‡ä»¶åï¼š{file_name}
- æ–‡ä»¶æ‰©å±•åï¼š{file_extension}

å¯é€‰çš„ç›®æ ‡æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆå¿…é¡»ä¸¥æ ¼ä»ä»¥ä¸‹åˆ—è¡¨ä¸­é€‰æ‹©ï¼Œä¸èƒ½ä¿®æ”¹è·¯å¾„ï¼‰ï¼š
{directory_structure}

åˆ†ç±»è¦æ±‚ï¼š
1. å¿…é¡»ä¸¥æ ¼ä»ä¸Šè¿°æ–‡ä»¶å¤¹è·¯å¾„åˆ—è¡¨ä¸­å¤åˆ¶å®Œæ•´çš„è·¯å¾„
2. ä¸èƒ½ä¿®æ”¹ã€ç¼©å†™æˆ–æ·»åŠ ä»»ä½•å†…å®¹åˆ°è·¯å¾„
3. ä¸èƒ½åˆ›å»ºæˆ–æƒ³è±¡ä¸å­˜åœ¨çš„æ–‡ä»¶å¤¹
4. æ ¹æ®æ–‡ä»¶æ‰©å±•åå’Œæ–‡ä»¶åå…³é”®è¯åŒ¹é…æ–‡ä»¶å¤¹
5. æŒ‰åŒ¹é…åº¦ä»é«˜åˆ°ä½è¿”å›å‰ä¸‰ä¸ªè·¯å¾„
6. æ¯è¡Œä¸€ä¸ªè·¯å¾„ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–å†…å®¹
7. ä¸è¦ä½¿ç”¨"<think>"æ ‡ç­¾æˆ–ä»»ä½•æ€è€ƒè¿‡ç¨‹æè¿°
8. å¦‚æœåŒ¹é…åº¦ç›¸è¿‘ï¼Œä¼˜å…ˆé€‰æ‹©æ›´å…·ä½“çš„æ–‡ä»¶å¤¹

è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼æŒ‰æ­¤æ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªå®Œæ•´è·¯å¾„ï¼‰ï¼š
ç¬¬ä¸€æ¨èï¼š[å®Œæ•´è·¯å¾„1]
ç¬¬äºŒæ¨èï¼š[å®Œæ•´è·¯å¾„2]
ç¬¬ä¸‰æ¨èï¼š[å®Œæ•´è·¯å¾„3]

ç¤ºä¾‹è¾“å‡ºï¼š
ç¬¬ä¸€æ¨èï¼šã€7-4-10ã€‘æ–°å…´ä¸šæ€/ã€7-4-9-10ã€‘åŒ»ç–—ä¿é™©
ç¬¬äºŒæ¨èï¼šã€7-4ã€‘ä¿é™©/å¥åº·é™©
ç¬¬ä¸‰æ¨èï¼šã€7-4-5ã€‘äººèº«é™©

è¯·å¼€å§‹æ¨èï¼š"""
            
            # åœ¨ä¼ é€’ç»™å¤§æ¨¡å‹çš„å®Œæ•´å†…å®¹æœ€å°¾éƒ¨æ·»åŠ /no_thinkæ ‡ç­¾
            final_prompt = prompt + "\n\n/no_think"
            
            print(f"ğŸ¤– å‘é€ç»™AIçš„æç¤ºè¯:\n{final_prompt}")
            
            # ä½¿ç”¨ç³»ç»Ÿæç¤ºè¯æ¥æŠ‘åˆ¶æ€è€ƒè¿‡ç¨‹
            messages = [
                {
                    'role': 'system',
                    'content': 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡ä»¶åˆ†ç±»åŠ©æ‰‹ã€‚é‡è¦ï¼šä¸è¦è¾“å‡ºä»»ä½•æ¨ç†è¿‡ç¨‹ã€æ€è€ƒæ­¥éª¤æˆ–è§£é‡Šã€‚ç›´æ¥æŒ‰è¦æ±‚è¾“å‡ºç»“æœã€‚åªè¾“å‡ºå®Œæ•´è·¯å¾„ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–ä¿¡æ¯ã€‚'
                },
                {
                    'role': 'user',
                    'content': final_prompt
                }
            ]
            
            result = self.ollama_client.chat_with_retry(messages)
            print(f"ğŸ¤– AIåŸå§‹è¿”å›ç»“æœ: {result}")
            
            # æ¸…ç†ç»“æœä¸­çš„æ€è€ƒè¿‡ç¨‹ï¼ˆç°åœ¨åœ¨_parse_classification_resultä¸­ç»Ÿä¸€å¤„ç†ï¼‰
            result = result.strip()
            
            # å¦‚æœç»“æœä»ç„¶ä»¥æ€è€ƒè¿‡ç¨‹å¼€å¤´ï¼Œå°è¯•æ›´æ¿€è¿›çš„æ¸…ç†
            if any(keyword in result.lower() for keyword in ['å¥½ï¼Œ', 'å—¯ï¼Œ', 'æˆ‘æ¥', 'æˆ‘éœ€è¦', 'é¦–å…ˆï¼Œ', 'è®©æˆ‘']):
                # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå®Œæ•´çš„å¥å­
                sentences = result.split('ã€‚')
                if len(sentences) > 1:
                    # è·³è¿‡ç¬¬ä¸€ä¸ªå¥å­ï¼ˆé€šå¸¸æ˜¯æ€è€ƒè¿‡ç¨‹ï¼‰ï¼Œä½¿ç”¨ç¬¬äºŒä¸ªå¥å­å¼€å§‹
                    result = 'ã€‚'.join(sentences[1:]).strip()
            
            # è§£æAIåˆ†ç±»ç»“æœ
            recommended_folder, match_reason = self._parse_classification_result(result, target_directory)
            
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…å¤±è´¥ï¼Œå¦‚æœæ˜¯ä¸”æœªè¶…è¿‡é‡è¯•æ¬¡æ•°ï¼Œåˆ™é‡è¯•
            if recommended_folder is None and retry_count < 2:  # æœ€å¤šé‡è¯•2æ¬¡
                print(f"âš ï¸  AIåˆ†ç±»å¤±è´¥ï¼Œå‡†å¤‡ç¬¬ {retry_count + 1} æ¬¡é‡è¯•...")
                logging.warning(f"AIåˆ†ç±»å¤±è´¥ï¼Œå‡†å¤‡ç¬¬ {retry_count + 1} æ¬¡é‡è¯•")
                
                # é€’å½’è°ƒç”¨è‡ªèº«è¿›è¡Œé‡è¯•
                return self._recommend_target_folder(file_path, content, summary, target_directory, retry_count + 1)
            
            return recommended_folder, match_reason
            
        except Exception as e:
            logging.error(f"æ¨èç›®æ ‡æ–‡ä»¶å¤¹å¤±è´¥: {e}")
            return None, f"æ¨èå¤±è´¥: {str(e)}"
    def _extract_recommended_paths(self, ai_result: str) -> List[str]:
        """
        ä»AIè¿”å›ç»“æœä¸­æå–ä¸‰ä¸ªæ¨èè·¯å¾„
        
        Args:
            ai_result: AIè¿”å›çš„åŸå§‹ç»“æœ
            
        Returns:
            List[str]: æå–åˆ°çš„æ¨èè·¯å¾„åˆ—è¡¨
        """
        recommended_paths = []
        
        try:
            lines = ai_result.strip().split('\n')
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                    
                # æŸ¥æ‰¾åŒ…å«æ¨èè·¯å¾„çš„è¡Œ
                if any(keyword in line for keyword in ['ç¬¬ä¸€æ¨èï¼š', 'ç¬¬äºŒæ¨èï¼š', 'ç¬¬ä¸‰æ¨èï¼š', 'æ¨èï¼š']):
                    # æå–å†’å·åçš„è·¯å¾„
                    if 'ï¼š' in line:
                        path = line.split('ï¼š', 1)[1].strip()
                        if path and path not in recommended_paths:
                            recommended_paths.append(path)
                elif line.startswith('ã€') and 'ã€‘' in line:
                    # ç›´æ¥è¯†åˆ«ä»¥ã€å¼€å¤´çš„è·¯å¾„æ ¼å¼
                    if line not in recommended_paths:
                        recommended_paths.append(line)
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ¼å¼åŒ–çš„æ¨èï¼Œå°è¯•æŒ‰è¡Œæå–å‰ä¸‰ä¸ªæœ‰æ•ˆè·¯å¾„
            if not recommended_paths:
                for line in lines:
                    line = line.strip()
                    if line and line.startswith('ã€') and 'ã€‘' in line:
                        if line not in recommended_paths:
                            recommended_paths.append(line)
                        if len(recommended_paths) >= 3:
                            break
            
            # é™åˆ¶æœ€å¤šè¿”å›3ä¸ªè·¯å¾„
            return recommended_paths[:3]
            
        except Exception as e:
            logging.error(f"æå–æ¨èè·¯å¾„æ—¶å‡ºé”™: {e}")
            return []
    
    def _parse_classification_result(self, result: str, target_directory: str) -> tuple:
        """
        è§£æAIåˆ†ç±»ç»“æœï¼Œæ”¯æŒä¸‰ä¸ªæ¨èè·¯å¾„çš„ä¾æ¬¡éªŒè¯
        
        Args:
            result: AIè¿”å›çš„åˆ†ç±»ç»“æœ
            target_directory: ç›®æ ‡ç›®å½•è·¯å¾„
            
        Returns:
            tuple: (æ¨èæ–‡ä»¶å¤¹è·¯å¾„, åŒ¹é…ç†ç”±)
        """
        try:
            result = result.strip()
            logging.info(f"æ­£åœ¨è§£æAIåˆ†ç±»ç»“æœ: {result[:100]}...")
            
            print(f"ğŸ” è§£æAIåˆ†ç±»ç»“æœ: {result}")
            
            # æ¸…ç†AIè¿”å›ç»“æœä¸­çš„æ€è€ƒè¿‡ç¨‹æ ‡ç­¾å’Œå†…å®¹
            result_clean = self._clean_ai_response(result)
            logging.info(f"æ¸…ç†åçš„åˆ†ç±»ç»“æœ: {result_clean[:100]}...")
            print(f"ğŸ§¹ æ¸…ç†åçš„ç»“æœ: {result_clean}")
            
            # æå–ä¸‰ä¸ªæ¨èè·¯å¾„
            recommended_paths = self._extract_recommended_paths(result_clean)
            
            if not recommended_paths:
                logging.warning(f"æ— æ³•ä»AIç»“æœä¸­æå–æ¨èè·¯å¾„")
                print(f"âš ï¸ æ— æ³•æå–æ¨èè·¯å¾„")
                return None, f"AIæ™ºèƒ½åˆ†ç±»å¤±è´¥ï¼Œæ— æ³•è§£ææ¨èè·¯å¾„: {result_clean[:50]}..."
            
            print(f"ğŸ“‹ æå–åˆ° {len(recommended_paths)} ä¸ªæ¨èè·¯å¾„: {recommended_paths}")
            
            # ä¾æ¬¡éªŒè¯æ¯ä¸ªæ¨èè·¯å¾„
            for i, recommended_path in enumerate(recommended_paths, 1):
                target_path = Path(target_directory) / recommended_path
                
                print(f"ğŸ” éªŒè¯ç¬¬{i}ä¸ªæ¨èè·¯å¾„: {recommended_path}")
                print(f"ğŸ¯ å®Œæ•´ç›®æ ‡è·¯å¾„: {target_path}")
                
                # éªŒè¯è·¯å¾„æ˜¯å¦å­˜åœ¨ä¸”ä¸ºç›®å½•
                if target_path.exists() and target_path.is_dir():
                    logging.info(f"æ‰¾åˆ°æœ‰æ•ˆçš„ç›®æ ‡æ–‡ä»¶å¤¹(ç¬¬{i}é€‰æ‹©): {recommended_path}")
                    print(f"âœ… æ‰¾åˆ°æœ‰æ•ˆç›®æ ‡æ–‡ä»¶å¤¹(ç¬¬{i}é€‰æ‹©): {recommended_path}")
                    
                    # å­˜å‚¨æ¨èçš„å®Œæ•´è·¯å¾„
                    self.recommended_folder_path = recommended_path
                    print(f"ğŸ’¾ å­˜å‚¨æ¨èè·¯å¾„: {self.recommended_folder_path}")
                    
                    # ç”Ÿæˆæœ‰æ„ä¹‰çš„åŒ¹é…ç†ç”±
                    match_reason = f"AIæ™ºèƒ½åˆ†ç±»(ç¬¬{i}é€‰æ‹©)ï¼šæ ¹æ®æ–‡ä»¶å†…å®¹åŒ¹é…åˆ° {recommended_path}"
                    
                    return recommended_path, match_reason
                else:
                    logging.warning(f"ç¬¬{i}ä¸ªæ¨èè·¯å¾„ä¸å­˜åœ¨: {target_path}")
                    print(f"âŒ ç¬¬{i}ä¸ªæ¨èè·¯å¾„ä¸å­˜åœ¨: {recommended_path}")
            
            # å¦‚æœæ‰€æœ‰æ¨èè·¯å¾„éƒ½æ— æ•ˆ
            logging.error(f"æ‰€æœ‰æ¨èè·¯å¾„éƒ½æ— æ•ˆ: {recommended_paths}")
            print(f"ğŸ’¥ æ‰€æœ‰æ¨èè·¯å¾„éƒ½æ— æ•ˆ")
            
            return None, f"AIæ™ºèƒ½åˆ†ç±»å¤±è´¥ï¼Œä¸‰ä¸ªæ¨èè·¯å¾„éƒ½ä¸å­˜åœ¨: {', '.join(recommended_paths[:3])}"
            
        except Exception as e:
            logging.error(f"è§£æåˆ†ç±»ç»“æœå¤±è´¥: {e}, åŸå§‹ç»“æœ: {result}")
            return None, f"è§£æåˆ†ç±»ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
    def scan_source_files(self, source_directory: str) -> List[str]:
        try:
            source_path = Path(source_directory)
            if not source_path.exists():
                raise FileOrganizerError(f"æºç›®å½•ä¸å­˜åœ¨: {source_directory}")
            files = []
            for item in source_path.rglob('*'):
                if item.is_file():
                    files.append(str(item))
            logging.info(f"æ‰«æåˆ° {len(files)} ä¸ªå¾…æ•´ç†æ–‡ä»¶")
            return files
        except Exception as e:
            raise FileOrganizerError(f"æ‰«ææºæ–‡ä»¶å¤±è´¥: {e}")
    def scan_files(self, source_directory: str) -> List[Dict[str, object]]:
        try:
            source_path = Path(source_directory)
            if not source_path.exists():
                raise FileOrganizerError(f"æºç›®å½•ä¸å­˜åœ¨: {source_directory}")
            files = []
            for item in source_path.rglob('*'):
                if item.is_file():
                    files.append({
                        'name': item.name,
                        'path': str(item),
                        'size': item.stat().st_size,
                        'extension': item.suffix.lower()
                    })
            logging.info(f"æ‰«æåˆ° {len(files)} ä¸ªå¾…æ•´ç†æ–‡ä»¶")
            return files
        except Exception as e:
            raise FileOrganizerError(f"æ‰«ææºæ–‡ä»¶å¤±è´¥: {e}")

    def preview_classification(self, source_directory: str, target_directory: str) -> List[Dict[str, object]]:
        try:
            source_files = self.scan_source_files(source_directory)
            if not source_files:
                raise FileOrganizerError("æºç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶")
            preview_results = []
            logging.info(f"å¼€å§‹é¢„è§ˆåˆ†ç±»ï¼Œå…± {len(source_files)} ä¸ªæ–‡ä»¶")
            preview_count = len(source_files)
            for i, file_path in enumerate(source_files, 1):
                file_name = Path(file_path).name
                try:
                    logging.info(f"æ­£åœ¨å¤„ç†æ–‡ä»¶ {i}/{preview_count}: {file_name}")
                    print(f"\n=== å¤„ç†æ–‡ä»¶ {i}/{preview_count}: {file_name} ===")
                    # ä½¿ç”¨æ–°çš„åˆ†ææ–¹æ³•
                    analysis_result = self.analyze_and_classify_file(file_path, target_directory)
                    
                    if analysis_result['success']:
                        timing_info = analysis_result.get('timing_info', {})
                        preview_results.append({
                            'file_path': file_path,
                            'file_name': file_name,
                            'target_folder': analysis_result['recommended_folder'],
                            'match_reason': analysis_result['match_reason'],
                            'classification_method': 'AI_Enhanced',
                            'success': True,
                            'extracted_content': analysis_result['extracted_content'][:200] + "..." if len(analysis_result['extracted_content']) > 200 else analysis_result['extracted_content'],
                            'content_summary': analysis_result['content_summary'],
                            'timing_info': timing_info
                        })
                        total_time = timing_info.get('total_processing_time', 0)
                        extract_time = timing_info.get('content_extraction_time', 0)
                        summary_time = timing_info.get('summary_generation_time', 0)
                        recommend_time = timing_info.get('folder_recommendation_time', 0)
                        
                        logging.info(f"æ–‡ä»¶ {file_name} åˆ†ææˆåŠŸ: {analysis_result['recommended_folder']} ({analysis_result['match_reason']})ï¼Œæ€»è€—æ—¶: {total_time}ç§’")
                        logging.info(f"å†…å®¹æ‘˜è¦: {analysis_result['content_summary']}")
                        logging.info(f"è¯¦ç»†è€—æ—¶ - å†…å®¹æå–: {extract_time}ç§’, æ‘˜è¦ç”Ÿæˆ: {summary_time}ç§’, ç›®å½•æ¨è: {recommend_time}ç§’")
                    else:
                        timing_info = analysis_result.get('timing_info', {})
                        preview_results.append({
                            'file_path': file_path,
                            'file_name': file_name,
                            'target_folder': None,
                            'match_reason': analysis_result['match_reason'],
                            'classification_method': 'Failed',
                            'success': False,
                            'error': analysis_result.get('error', analysis_result['match_reason']),
                            'extracted_content': analysis_result.get('extracted_content', ''),
                            'content_summary': analysis_result.get('content_summary', ''),
                            'timing_info': timing_info
                        })
                        total_time = timing_info.get('total_processing_time', 0)
                        logging.warning(f"æ–‡ä»¶ {file_name} åˆ†æå¤±è´¥: {analysis_result['match_reason']}ï¼Œæ€»è€—æ—¶: {total_time}ç§’")
                except Exception as e:
                    error_msg = f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}"
                    logging.error(f"æ–‡ä»¶ {file_name} å¤„ç†å¼‚å¸¸: {e}")
                    preview_results.append({
                        'file_path': file_path,
                        'file_name': file_name,
                        'target_folder': None,
                        'match_reason': error_msg,
                        'classification_method': 'Error',
                        'success': False,
                        'error': error_msg
                    })
            success_count = sum(1 for result in preview_results if result['success'])
            logging.info(f"é¢„è§ˆåˆ†ç±»å®Œæˆï¼ŒæˆåŠŸåˆ†ç±» {success_count}/{len(preview_results)} ä¸ªæ–‡ä»¶")
            
            # ç”Ÿæˆé¢„è§ˆç»“æœJSONæ–‡ä»¶ï¼ˆæ ¼å¼ä¸preview_ai_result.jsonä¿æŒä¸€è‡´ï¼‰
            try:
                ai_preview_results = []
                for result in preview_results:
                    if result['success']:
                        ai_preview_result = {
                            "æºæ–‡ä»¶è·¯å¾„": result['file_path'],
                            "æ–‡ä»¶æ‘˜è¦": result['content_summary'],
                            "æœ€åŒ¹é…çš„ç›®æ ‡ç›®å½•": result['target_folder'],
                            "åŒ¹é…ç†ç”±": result['match_reason'],
                            "å¤„ç†è€—æ—¶ä¿¡æ¯": {
                                "æ€»è€—æ—¶(ç§’)": result['timing_info'].get('total_processing_time', 0),
                                "å†…å®¹æå–è€—æ—¶(ç§’)": result['timing_info'].get('content_extraction_time', 0),
                                "æ‘˜è¦ç”Ÿæˆè€—æ—¶(ç§’)": result['timing_info'].get('summary_generation_time', 0),
                                "ç›®å½•æ¨èè€—æ—¶(ç§’)": result['timing_info'].get('folder_recommendation_time', 0)
                            }
                        }
                        ai_preview_results.append(ai_preview_result)
                
                # ä¿å­˜é¢„è§ˆç»“æœåˆ°JSONæ–‡ä»¶
                preview_result_file = 'preview_ai_result.json'
                with open(preview_result_file, 'w', encoding='utf-8') as f:
                    json.dump(ai_preview_results, f, ensure_ascii=False, indent=2)
                logging.info(f"AIé¢„è§ˆç»“æœå·²ä¿å­˜åˆ°: {preview_result_file}")
                
            except Exception as e:
                logging.warning(f"ç”Ÿæˆé¢„è§ˆç»“æœæ–‡ä»¶å¤±è´¥: {e}")
            
            return preview_results
        except Exception as e:
            raise FileOrganizerError(f"é¢„è§ˆåˆ†ç±»å¤±è´¥: {e}")
    def organize_file(self, file_path: str, target_directory: str) -> Tuple[bool, str]:
        try:
            if not self.ollama_client:
                self.initialize_ollama()
            
            file_path_obj = Path(file_path)
            filename = file_path_obj.name
            source_file_full_path = str(file_path_obj.absolute())
            
            print(f"ğŸ“„ æºæ–‡ä»¶å®Œæ•´è·¯å¾„: {source_file_full_path}")
            
            # ä½¿ç”¨AIåˆ†ç±»è·å–æ¨èæ–‡ä»¶å¤¹
            target_folder, match_reason, success = self.classify_file(str(file_path_obj), target_directory)
            if not success or not target_folder:
                return False, "æ— æ³•ç¡®å®šç›®æ ‡æ–‡ä»¶å¤¹"
            
            # ä½¿ç”¨å­˜å‚¨çš„æ¨èè·¯å¾„å˜é‡
            if hasattr(self, 'recommended_folder_path') and self.recommended_folder_path:
                target_folder_full_path = Path(target_directory) / self.recommended_folder_path
                print(f"ğŸ”§ ä½¿ç”¨å­˜å‚¨çš„æ¨èè·¯å¾„: {self.recommended_folder_path}")
                print(f"ğŸ¯ ç›®æ ‡æ–‡ä»¶å¤¹å®Œæ•´è·¯å¾„: {target_folder_full_path}")
            else:
                target_folder_full_path = Path(target_directory) / target_folder
                print(f"âš ï¸  ä½¿ç”¨é»˜è®¤è·¯å¾„: {target_folder}")
                print(f"ğŸ¯ ç›®æ ‡æ–‡ä»¶å¤¹å®Œæ•´è·¯å¾„: {target_folder_full_path}")
            
            # æ„å»ºå®Œæ•´çš„è¿ç§»å‘½ä»¤ä¿¡æ¯
            migration_info = {
                'source_file': source_file_full_path,
                'target_folder': str(target_folder_full_path),
                'filename': filename,
                'match_reason': match_reason
            }
            
            print(f"ğŸ“‹ è¿ç§»ä¿¡æ¯: {migration_info}")
            
            # åˆ›å»ºç›®æ ‡æ–‡ä»¶å¤¹
            target_folder_full_path.mkdir(parents=True, exist_ok=True)
            
            # æ„å»ºç›®æ ‡æ–‡ä»¶è·¯å¾„
            target_file_path = target_folder_full_path / filename
            if target_file_path.exists():
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                name_parts = filename.rsplit('.', 1)
                if len(name_parts) == 2:
                    new_filename = f"{name_parts[0]}_{timestamp}.{name_parts[1]}"
                else:
                    new_filename = f"{filename}_{timestamp}"
                target_file_path = target_folder_full_path / new_filename
                print(f"âš ï¸  æ–‡ä»¶åå†²çªï¼Œé‡å‘½åä¸º: {new_filename}")
            
            # æ‰§è¡Œæ–‡ä»¶å¤åˆ¶
            shutil.copy2(source_file_full_path, str(target_file_path))
            
            # è®°å½•è¿ç§»æ—¥å¿—
            migration_log = f"æ–‡ä»¶è¿ç§»å®Œæˆ: {source_file_full_path} -> {target_file_path}"
            logging.info(migration_log)
            print(f"âœ… {migration_log}")
            
            return True, str(target_file_path)
        except Exception as e:
            error_msg = f"æ•´ç†æ–‡ä»¶å¤±è´¥: {e}"
            logging.error(error_msg)
            return False, error_msg
    def organize_files(self, files=None, target_base_dir=None, copy_mode=True, source_directory=None, target_directory=None, dry_run=False, progress_callback=None) -> Dict[str, object]:
        try:
            if source_directory and target_directory:
                files = self.scan_files(source_directory)
                target_base_dir = target_directory
                copy_mode = True
            if not files or not target_base_dir:
                raise FileOrganizerError("ç¼ºå°‘å¿…è¦å‚æ•°")
            log_session_name = None
            if self.enable_transfer_log and self.transfer_log_manager and not dry_run:
                try:
                    session_name = f"organize_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                    log_session_name = self.transfer_log_manager.start_transfer_session(session_name)
                    logging.info(f"å¼€å§‹è½¬ç§»æ—¥å¿—ä¼šè¯: {session_name}")
                except Exception as e:
                    logging.warning(f"å¯åŠ¨è½¬ç§»æ—¥å¿—ä¼šè¯å¤±è´¥: {e}")
            if not files:
                raise FileOrganizerError("æ²¡æœ‰æ–‡ä»¶éœ€è¦æ•´ç†")
            results = {
                'total_files': len(files),
                'processed_files': 0,
                'successful_moves': 0,
                'failed_moves': 0,
                'skipped_files': 0,
                'success': [],
                'failed': [],
                'errors': [],
                'move_details': [],
                'ai_responses': [],
                'start_time': datetime.now(),
                'end_time': None,
                'transfer_log_file': log_session_name,
                'dry_run': dry_run
            }
            
            # åˆå§‹åŒ–AIç»“æœæ–‡ä»¶
            ai_result_file = 'ai_organize_result.json'
            # ä¸å†åœ¨å†…å­˜ä¸­ä¿å­˜å®Œæ•´çš„ai_resultsåˆ—è¡¨ï¼Œåªä¿å­˜å¿…è¦çš„è¿ç§»ä¿¡æ¯
            migration_queue = []  # åªä¿å­˜æºè·¯å¾„å’Œç›®æ ‡è·¯å¾„çš„ç®€å•ä¿¡æ¯
            
            # ç¡®ä¿AIç»“æœæ–‡ä»¶å­˜åœ¨ï¼Œä½†ä¸æ¸…ç©ºç°æœ‰å†…å®¹
            if not os.path.exists(ai_result_file):
                with open(ai_result_file, 'w', encoding='utf-8') as f:
                    json.dump([], f, ensure_ascii=False, indent=2)
                logging.info(f"åˆ›å»ºæ–°çš„AIç»“æœæ–‡ä»¶: {ai_result_file}")
            else:
                logging.info(f"AIç»“æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œå°†è¿½åŠ æ–°è®°å½•: {ai_result_file}")
            logging.info(f"å¼€å§‹å®‰å…¨æ–‡ä»¶æ•´ç†ï¼Œå…± {len(files)} ä¸ªæ–‡ä»¶")
            print(f"\n=== å¼€å§‹AIæ™ºèƒ½æ–‡ä»¶æ•´ç† ===")
            print(f"æºç›®å½•: {source_directory if source_directory else 'æŒ‡å®šæ–‡ä»¶åˆ—è¡¨'}")
            print(f"ç›®æ ‡ç›®å½•: {target_base_dir}")
            print(f"å¾…å¤„ç†æ–‡ä»¶æ€»æ•°: {len(files)}")
            print(f"æ“ä½œæ¨¡å¼: {'å¤åˆ¶' if copy_mode else 'ç§»åŠ¨'}")
            print("=" * 50)
            
            for i, file_info in enumerate(files, 1):
                file_path = str(file_info['path'])
                filename = str(file_info['name'])
                
                # è°ƒç”¨è¿›åº¦å›è°ƒ
                if progress_callback:
                    progress_callback(i, len(files), filename)
                
                # æ§åˆ¶å°è¾“å‡ºå½“å‰å¤„ç†è¿›åº¦
                print(f"\n[{i}/{len(files)}] æ­£åœ¨å¤„ç†: {filename}")
                
                try:
                    logging.info(f"æ­£åœ¨å¤„ç†æ–‡ä»¶ {i}/{len(files)}: {filename}")
                    print(f"  ğŸ” æ­£åœ¨åˆ†ææ–‡ä»¶å†…å®¹...", end="", flush=True)
                    
                    # è·å–è¯¦ç»†çš„åˆ†æç»“æœï¼ˆåªè°ƒç”¨ä¸€æ¬¡ï¼‰
                    analysis_result = self.analyze_and_classify_file(file_path, target_base_dir)
                    target_folder = analysis_result.get('recommended_folder')
                    match_reason = analysis_result.get('match_reason', '')
                    success = analysis_result.get('success', False)
                    
                    results['ai_responses'].append({
                        'file_name': filename,
                        'target_folder': target_folder,
                        'match_reason': match_reason,
                        'success': success
                    })
                    
                    # æ„å»ºAIç»“æœé¡¹ï¼ˆåŸºç¡€ä¿¡æ¯ï¼‰
                    ai_result_item = {
                        "å¤„ç†æ—¶é—´": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        "æ–‡ä»¶å": filename,
                        "æºæ–‡ä»¶è·¯å¾„": file_path,
                        "æ–‡ä»¶æ‘˜è¦": analysis_result.get('content_summary', ''),
                        "æœ€åŒ¹é…çš„ç›®æ ‡ç›®å½•": analysis_result.get('recommended_folder', ''),
                        "åŒ¹é…ç†ç”±": analysis_result.get('match_reason', ''),
                        "å¤„ç†è€—æ—¶ä¿¡æ¯": {
                            "æ€»è€—æ—¶(ç§’)": analysis_result.get('timing_info', {}).get('total_processing_time', 0),
                            "å†…å®¹æå–è€—æ—¶(ç§’)": analysis_result.get('timing_info', {}).get('content_extraction_time', 0),
                            "æ‘˜è¦ç”Ÿæˆè€—æ—¶(ç§’)": analysis_result.get('timing_info', {}).get('summary_generation_time', 0),
                            "ç›®å½•æ¨èè€—æ—¶(ç§’)": analysis_result.get('timing_info', {}).get('folder_recommendation_time', 0)
                        }
                    }
                    
                    # ä¿å­˜åˆ°è¿ç§»é˜Ÿåˆ—ï¼Œç­‰å¾…è¿ç§»æˆåŠŸåå†™å…¥å®Œæ•´ä¿¡æ¯
                    migration_queue.append({
                        'source_path': file_path,
                        'target_folder': target_folder,
                        'filename': filename,
                        'match_reason': match_reason,
                        'ai_result_item': ai_result_item
                    })
                    
                    if not success or not target_folder:
                        error_msg = f"æ–‡ä»¶ {filename} åˆ†ç±»å¤±è´¥: {match_reason}ï¼Œå·²è·³è¿‡ï¼Œæœªåšä»»ä½•å¤„ç†"
                        print(f"\r  âŒ åˆ†ç±»å¤±è´¥: {match_reason}")
                        logging.warning(error_msg)
                        results['errors'].append(error_msg)
                        results['skipped_files'] += 1
                        results['failed'].append({
                            'source_path': file_path,
                            'error': error_msg
                        })
                        
                        # æ›´æ–°è¿ç§»é˜Ÿåˆ—ä¸­æœ€åä¸€é¡¹çš„å¤±è´¥çŠ¶æ€å¹¶ç«‹å³å†™å…¥
                        if migration_queue:
                            migration_queue[-1]['ai_result_item'].update({
                                "å¤„ç†çŠ¶æ€": "åˆ†ç±»å¤±è´¥",
                                "é”™è¯¯ä¿¡æ¯": match_reason
                            })
                            # ç«‹å³å†™å…¥å¤±è´¥è®°å½•
                            self._append_ai_result_to_file(ai_result_file, migration_queue[-1]['ai_result_item'])
                        
                        continue
                    
                    print(f"\r  âœ… æ¨èç›®å½•: {target_folder}")
                    print(f"     ç†ç”±: {match_reason}")
                    target_folder_path = Path(target_base_dir) / target_folder
                    if not target_folder_path.exists():
                        error_msg = f"ç›®æ ‡æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {target_folder}"
                        logging.error(error_msg)
                        results['errors'].append(error_msg)
                        results['failed_moves'] += 1
                        results['failed'].append({
                            'source_path': file_path,
                            'error': error_msg
                        })
                        continue
                    original_target_path = target_folder_path / filename
                    target_file_path = original_target_path
                    if target_file_path.exists():
                        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                        name_parts = filename.rsplit('.', 1)
                        if len(name_parts) == 2:
                            new_filename = f"{name_parts[0]}_{timestamp}.{name_parts[1]}"
                        else:
                            new_filename = f"{filename}_{timestamp}"
                        target_file_path = target_folder_path / new_filename
                        logging.info(f"æ–‡ä»¶åå†²çªï¼Œé‡å‘½åä¸º: {target_file_path.name}")
                    if not dry_run:
                        try:
                            if not Path(file_path).exists():
                                error_msg = f"æºæ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
                                logging.error(error_msg)
                                results['errors'].append(error_msg)
                                results['failed_moves'] += 1
                                results['failed'].append({
                                    'source_path': file_path,
                                    'error': error_msg
                                })
                                continue
                            if copy_mode:
                                shutil.copy2(file_path, str(target_file_path))
                                operation = "copy"
                                operation_cn = "å¤åˆ¶"
                            else:
                                shutil.move(file_path, str(target_file_path))
                                operation = "move"
                                operation_cn = "ç§»åŠ¨"
                            if target_file_path.exists():
                                if not copy_mode and Path(file_path).exists():
                                    error_msg = f"æ–‡ä»¶ç§»åŠ¨éªŒè¯å¤±è´¥: {filename}"
                                    logging.error(error_msg)
                                    results['errors'].append(error_msg)
                                    results['failed_moves'] += 1
                                    results['failed'].append({
                                        'source_path': file_path,
                                        'error': error_msg
                                    })
                                    continue
                                print(f"     âœ… {operation_cn}æˆåŠŸ: {filename} -> {target_folder}")
                                logging.info(f"æ–‡ä»¶å®‰å…¨{operation_cn}æˆåŠŸ: {filename} -> {target_folder} ({match_reason})")
                                results['successful_moves'] += 1
                                
                                # æ›´æ–°è¿ç§»é˜Ÿåˆ—ä¸­å¯¹åº”é¡¹çš„æœ€ç»ˆè·¯å¾„å¹¶ç«‹å³å†™å…¥
                                for queue_item in migration_queue:
                                    if queue_item['source_path'] == file_path:
                                        queue_item['ai_result_item'].update({
                                            "æœ€ç»ˆç›®æ ‡è·¯å¾„": str(target_file_path),
                                            "æ“ä½œç±»å‹": operation_cn,
                                            "å¤„ç†çŠ¶æ€": "æˆåŠŸ"
                                        })
                                        # ç«‹å³å†™å…¥æˆåŠŸè®°å½•
                                        self._append_ai_result_to_file(ai_result_file, queue_item['ai_result_item'])
                                        break
                            else:
                                error_msg = f"æ–‡ä»¶{operation_cn}éªŒè¯å¤±è´¥: {filename}"
                                logging.error(error_msg)
                                results['errors'].append(error_msg)
                                results['failed_moves'] += 1
                                results['failed'].append({
                                    'source_path': file_path,
                                    'error': error_msg
                                })
                                continue
                        except Exception as move_error:
                            error_msg = f"{operation_cn}æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {move_error}"
                            logging.error(error_msg)
                            results['errors'].append(error_msg)
                            results['failed_moves'] += 1
                            results['failed'].append({
                                'source_path': file_path,
                                'error': error_msg
                            })
                            continue
                    else:
                        operation = "copy" if copy_mode else "move"
                        operation_cn = "å¤åˆ¶" if copy_mode else "ç§»åŠ¨"
                        logging.info(f"[è¯•è¿è¡Œ] æ–‡ä»¶å°†{operation_cn}: {filename} -> {target_folder} ({match_reason})")
                        results['successful_moves'] += 1
                        
                        # è¯•è¿è¡Œæ¨¡å¼ä¸‹æ›´æ–°è¿ç§»é˜Ÿåˆ—ä¸­å¯¹åº”é¡¹å¹¶ç«‹å³å†™å…¥
                        for queue_item in migration_queue:
                            if queue_item['source_path'] == file_path:
                                queue_item['ai_result_item'].update({
                                    "æœ€ç»ˆç›®æ ‡è·¯å¾„": str(target_file_path),
                                    "æ“ä½œç±»å‹": operation_cn,
                                    "å¤„ç†çŠ¶æ€": "è¯•è¿è¡ŒæˆåŠŸ"
                                })
                                # ç«‹å³å†™å…¥è¯•è¿è¡Œè®°å½•
                                self._append_ai_result_to_file(ai_result_file, queue_item['ai_result_item'])
                                break
                    if self.enable_transfer_log and self.transfer_log_manager and not dry_run:
                        try:
                            file_size_raw = file_info.get('size', 0)
                            file_size = int(file_size_raw) if isinstance(file_size_raw, (int, float, str)) and str(file_size_raw).isdigit() else 0
                            self.transfer_log_manager.log_transfer_operation(
                                source_path=file_path,
                                target_path=str(target_file_path),
                                operation_type=operation,
                                target_folder=target_folder,
                                success=True,
                                file_size=file_size
                            )
                        except Exception as e:
                            logging.warning(f"è®°å½•è½¬ç§»æ—¥å¿—å¤±è´¥: {e}")
                    results['success'].append({
                        'source_path': file_path,
                        'target_path': str(target_file_path),
                        'target_folder': target_folder,
                        'operation': operation
                    })
                    results['move_details'].append({
                        'source_file': file_path,
                        'file_name': filename,
                        'target_folder': target_folder,
                        'target_path': str(target_file_path),
                        'match_reason': match_reason,
                        'classification_method': 'AI',
                        'renamed': str(target_file_path) != str(original_target_path),
                        'operation': operation
                    })
                except Exception as e:
                    error_msg = f"å¤„ç†æ–‡ä»¶ {filename} æ—¶å‡ºç°å¼‚å¸¸: {e}"
                    logging.error(error_msg)
                    results['errors'].append(error_msg)
                    results['failed_moves'] += 1
                    if self.enable_transfer_log and self.transfer_log_manager and not dry_run:
                        try:
                            operation_type = "copy" if copy_mode else "move"
                            self.transfer_log_manager.log_transfer_operation(
                                source_path=file_path,
                                target_path="",
                                operation_type=operation_type,
                                target_folder="",
                                success=False,
                                error_message=error_msg
                            )
                        except Exception as log_e:
                            logging.warning(f"è®°å½•å¤±è´¥è½¬ç§»æ—¥å¿—å¤±è´¥: {log_e}")
                    results['failed'].append({
                        'source_path': file_path,
                        'error': error_msg
                    })
                finally:
                    results['processed_files'] += 1
            results['end_time'] = datetime.now()
            
            # è¾“å‡ºå¤„ç†å®Œæˆæ€»ç»“
            print(f"\n=== æ–‡ä»¶æ•´ç†å®Œæˆ ===")
            print(f"æ€»æ–‡ä»¶æ•°: {results['total_files']}")
            print(f"æˆåŠŸå¤„ç†: {results['successful_moves']} ä¸ª")
            print(f"å¤„ç†å¤±è´¥: {results['failed_moves']} ä¸ª")
            print(f"è·³è¿‡æ–‡ä»¶: {results['skipped_files']} ä¸ª")
            duration = (results['end_time'] - results['start_time']).total_seconds()
            print(f"æ€»è€—æ—¶: {duration:.1f} ç§’")
            print("=" * 50)
            
            # AIç»“æœå·²åœ¨å¤„ç†è¿‡ç¨‹ä¸­å®æ—¶å†™å…¥
            results['ai_result_file'] = ai_result_file
            logging.info(f"AIåˆ†æç»“æœå·²å®æ—¶å†™å…¥: {ai_result_file}")
            
            if self.enable_transfer_log and self.transfer_log_manager and not dry_run:
                try:
                    session_summary = self.transfer_log_manager.end_transfer_session()
                    logging.info(f"è½¬ç§»æ—¥å¿—ä¼šè¯ç»“æŸ: {session_summary}")
                except Exception as e:
                    logging.warning(f"ç»“æŸè½¬ç§»æ—¥å¿—ä¼šè¯å¤±è´¥: {e}")
            
            logging.info(f"å®‰å…¨æ–‡ä»¶æ•´ç†å®Œæˆ: æˆåŠŸ {results['successful_moves']}, å¤±è´¥ {results['failed_moves']}, è·³è¿‡ {results['skipped_files']}")
            
            # åˆ é™¤æºæ–‡ä»¶åŠŸèƒ½å·²ç§»è‡³GUIç•Œé¢ï¼Œä¸å†åœ¨æ§åˆ¶å°è¯¢é—®
            if not dry_run and results['successful_moves'] > 0:
                logging.info(f"æˆåŠŸå¤„ç† {results['successful_moves']} ä¸ªæ–‡ä»¶ï¼Œåˆ é™¤æºæ–‡ä»¶åŠŸèƒ½è¯·åœ¨GUIç•Œé¢ä½¿ç”¨")
            
            return results
        except Exception as e:
            raise FileOrganizerError(f"æ‰¹é‡æ•´ç†æ–‡ä»¶å¤±è´¥: {e}")
    def get_transfer_logs(self) -> List[str]:
        if not self.enable_transfer_log or not self.transfer_log_manager:
            raise FileOrganizerError("è½¬ç§»æ—¥å¿—åŠŸèƒ½æœªå¯ç”¨")
        return self.transfer_log_manager.get_transfer_logs()
    def get_transfer_log_summary(self, log_file_path: str) -> Dict:
        if not self.enable_transfer_log or not self.transfer_log_manager:
            raise FileOrganizerError("è½¬ç§»æ—¥å¿—åŠŸèƒ½æœªå¯ç”¨")
        return self.transfer_log_manager.get_session_summary(log_file_path)
    def restore_files_from_log(self, log_file_path: str, operation_ids: Optional[List[int]] = None, dry_run: bool = True) -> Dict:
        if not self.enable_transfer_log or not self.transfer_log_manager:
            raise FileOrganizerError("è½¬ç§»æ—¥å¿—åŠŸèƒ½æœªå¯ç”¨")
        try:
            restore_results = self.transfer_log_manager.restore_from_log(
                log_file_path=log_file_path,
                operation_ids=operation_ids if operation_ids is not None else [],
                dry_run=dry_run
            )
            logging.info(f"æ–‡ä»¶æ¢å¤å®Œæˆ: æˆåŠŸ {restore_results['successful_restores']}, å¤±è´¥ {restore_results['failed_restores']}, è·³è¿‡ {restore_results['skipped_operations']}")
            return restore_results
        except Exception as e:
            raise FileOrganizerError(f"æ–‡ä»¶æ¢å¤å¤±è´¥: {e}")
    def cleanup_old_transfer_logs(self, days_to_keep: int = 30) -> int:
        if not self.enable_transfer_log or not self.transfer_log_manager:
            raise FileOrganizerError("è½¬ç§»æ—¥å¿—åŠŸèƒ½æœªå¯ç”¨")
        return self.transfer_log_manager.cleanup_old_logs(days_to_keep)
    def _append_ai_result_to_file(self, ai_result_file: str, ai_result_item: dict) -> None:
        """è¿½åŠ AIç»“æœåˆ°æ–‡ä»¶ï¼Œé¿å…åœ¨å†…å­˜ä¸­ä¿å­˜å¤§é‡æ•°æ®"""
        try:
            # è¯»å–ç°æœ‰æ•°æ®
            existing_data = []
            if os.path.exists(ai_result_file):
                try:
                    with open(ai_result_file, 'r', encoding='utf-8') as f:
                        content = f.read().strip()
                        if content:  # åªæœ‰å½“æ–‡ä»¶ä¸ä¸ºç©ºæ—¶æ‰å°è¯•è§£æJSON
                            existing_data = json.loads(content)
                        else:
                            existing_data = []  # ç©ºæ–‡ä»¶æ—¶åˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨
                except json.JSONDecodeError:
                    # å¦‚æœJSONè§£æå¤±è´¥ï¼Œåˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨
                    existing_data = []
                    logging.warning(f"AIç»“æœæ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œé‡æ–°åˆå§‹åŒ–: {ai_result_file}")
            
            # è¿½åŠ æ–°æ•°æ®
            existing_data.append(ai_result_item)
            
            # å†™å›æ–‡ä»¶
            with open(ai_result_file, 'w', encoding='utf-8') as f:
                json.dump(existing_data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            logging.error(f"è¿½åŠ AIç»“æœåˆ°æ–‡ä»¶å¤±è´¥: {e}")
    
    def _update_last_ai_result(self, ai_result_file: str, updates: dict) -> None:
        """æ›´æ–°æ–‡ä»¶ä¸­æœ€åä¸€æ¡AIç»“æœè®°å½•"""
        try:
            # è¯»å–ç°æœ‰æ•°æ®
            existing_data = []
            if os.path.exists(ai_result_file):
                try:
                    with open(ai_result_file, 'r', encoding='utf-8') as f:
                        content = f.read().strip()
                        if content:  # åªæœ‰å½“æ–‡ä»¶ä¸ä¸ºç©ºæ—¶æ‰å°è¯•è§£æJSON
                            existing_data = json.loads(content)
                        else:
                            existing_data = []  # ç©ºæ–‡ä»¶æ—¶åˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨
                except json.JSONDecodeError:
                    # å¦‚æœJSONè§£æå¤±è´¥ï¼Œåˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨
                    existing_data = []
                    logging.warning(f"AIç»“æœæ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œé‡æ–°åˆå§‹åŒ–: {ai_result_file}")
            
            # æ›´æ–°æœ€åä¸€æ¡è®°å½•
            if existing_data:
                existing_data[-1].update(updates)
                
                # å†™å›æ–‡ä»¶
                with open(ai_result_file, 'w', encoding='utf-8') as f:
                    json.dump(existing_data, f, ensure_ascii=False, indent=2)
                    
        except Exception as e:
            logging.error(f"æ›´æ–°AIç»“æœæ–‡ä»¶å¤±è´¥: {e}")

    # def _ask_delete_source_files(self, successful_moves: List[Dict]) -> None:
    #     """è¯¢é—®ç”¨æˆ·æ˜¯å¦åˆ é™¤æºæ–‡ä»¶ - å·²ç§»è‡³GUIç•Œé¢ï¼Œæ­¤æ–¹æ³•ä¸å†ä½¿ç”¨"""
    #     # åˆ é™¤æºæ–‡ä»¶åŠŸèƒ½å·²ç§»è‡³GUIç•Œé¢ï¼Œä¸å†åœ¨æ§åˆ¶å°è¯¢é—®
    #     pass

    def get_file_summary(self, file_path: str, max_length: int = 50, max_pages: int = 2, max_seconds: int = 10) -> str:
        """
        è‡ªåŠ¨é€‚é… txt/pdf/docx æ ¼å¼ï¼Œè¿”å›å‰max_lengthå­—æ‘˜è¦ï¼ŒPDF/Wordåªå–å‰max_pagesé¡µï¼Œå•æ–‡ä»¶å¤„ç†è¶…æ—¶max_secondsç§’ã€‚
        """
        ext = Path(file_path).suffix.lower()
        start_time = time.time()
        try:
            if ext == '.txt':
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    return f.read(max_length)
            elif ext == '.pdf':
                with open(file_path, 'rb') as f:
                    reader = PyPDF2.PdfReader(f)
                    text = ''
                    for i, page in enumerate(reader.pages):
                        if i >= max_pages or (time.time() - start_time) > max_seconds:
                            break
                        page_text = page.extract_text() or ''
                        text += page_text
                        if len(text) >= max_length:
                            break
                    if (time.time() - start_time) > max_seconds:
                        return 'æå–è¶…æ—¶ï¼Œå·²è·³è¿‡'
                    return text[:max_length] if text else 'æœªèƒ½æå–æ­£æ–‡'
            elif ext == '.docx':
                doc = docx.Document(file_path)
                text = ''
                for i, para in enumerate(doc.paragraphs):
                    if (time.time() - start_time) > max_seconds:
                        break
                    text += para.text
                    if len(text) >= max_length:
                        break
                if (time.time() - start_time) > max_seconds:
                    return 'æå–è¶…æ—¶ï¼Œå·²è·³è¿‡'
                return text[:max_length] if text else 'æœªèƒ½æå–æ­£æ–‡'
            else:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    return f.read(max_length)
        except Exception as e:
            if (time.time() - start_time) > max_seconds:
                return 'æå–è¶…æ—¶ï¼Œå·²è·³è¿‡'
            return f'æ‘˜è¦è·å–å¤±è´¥: {e}'
    
    def batch_read_documents(self, folder_path: str, progress_callback=None, summary_length: int = 200) -> Dict[str, Any]:
        """
        æ‰¹é‡è§£è¯»æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰æ–‡æ¡£ï¼Œç”Ÿæˆæ‘˜è¦å¹¶ä¿å­˜åˆ°AIç»“æœæ–‡ä»¶
        
        Args:
            folder_path: è¦è§£è¯»çš„æ–‡ä»¶å¤¹è·¯å¾„
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶ (current, total, filename) å‚æ•°
            summary_length: æ‘˜è¦é•¿åº¦ï¼ˆå­—æ•°ï¼‰ï¼Œé»˜è®¤200å­—
            
        Returns:
            åŒ…å«å¤„ç†ç»“æœçš„å­—å…¸
        """
        start_time = time.time()
        timing_info = {}
        
        try:
            if not self.ollama_client:
                init_start = time.time()
                self.initialize_ollama()
                timing_info['ollama_init_time'] = round(time.time() - init_start, 3)
            
            folder_path = Path(folder_path)
            if not folder_path.exists() or not folder_path.is_dir():
                raise FileOrganizerError(f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨æˆ–ä¸æ˜¯æœ‰æ•ˆç›®å½•: {folder_path}")
            
            # æ”¯æŒçš„æ–‡æ¡£æ ¼å¼
            supported_extensions = {'.txt', '.pdf', '.docx', '.doc', '.md', '.py', '.js', '.html', '.css', '.json', '.xml', '.csv'}
            
            # æ‰«ææ–‡ä»¶å¤¹è·å–æ‰€æœ‰æ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶
            document_files = []
            for file_path in folder_path.rglob('*'):
                if file_path.is_file() and file_path.suffix.lower() in supported_extensions:
                    document_files.append(file_path)
            
            if not document_files:
                return {
                    'total_files': 0,
                    'processed_files': 0,
                    'successful_reads': 0,
                    'failed_reads': 0,
                    'errors': [],
                    'timing_info': timing_info,
                    'start_time': datetime.now(),
                    'end_time': datetime.now()
                }
            
            # åˆå§‹åŒ–ç»“æœç»Ÿè®¡
            results = {
                'total_files': len(document_files),
                'processed_files': 0,
                'successful_reads': 0,
                'failed_reads': 0,
                'errors': [],
                'timing_info': timing_info,
                'start_time': datetime.now(),
                'end_time': None
            }
            
            # åˆå§‹åŒ–AIç»“æœæ–‡ä»¶
            ai_result_file = 'ai_organize_result.json'
            if not os.path.exists(ai_result_file):
                with open(ai_result_file, 'w', encoding='utf-8') as f:
                    json.dump([], f, ensure_ascii=False, indent=2)
                logging.info(f"åˆ›å»ºæ–°çš„AIç»“æœæ–‡ä»¶: {ai_result_file}")
            else:
                logging.info(f"AIç»“æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œå°†è¿½åŠ æ–°è®°å½•: {ai_result_file}")
            

            
            logging.info(f"å¼€å§‹æ‰¹é‡æ–‡æ¡£è§£è¯»ï¼Œå…± {len(document_files)} ä¸ªæ–‡ä»¶")
            print(f"\n=== å¼€å§‹æ‰¹é‡æ–‡æ¡£è§£è¯» ===")
            print(f"æºæ–‡ä»¶å¤¹: {folder_path}")
            print(f"å¾…å¤„ç†æ–‡ä»¶æ€»æ•°: {len(document_files)}")
            print("=" * 50)
            
            # é€ä¸ªå¤„ç†æ–‡æ¡£æ–‡ä»¶
            for i, file_path in enumerate(document_files, 1):
                filename = file_path.name
                
                # è°ƒç”¨è¿›åº¦å›è°ƒ
                if progress_callback:
                    progress_callback(i, len(document_files), filename)
                
                # æ§åˆ¶å°è¾“å‡ºå½“å‰å¤„ç†è¿›åº¦
                print(f"\n[{i}/{len(document_files)}] æ­£åœ¨è§£è¯»: {filename}")
                
                try:
                    logging.info(f"æ­£åœ¨è§£è¯»æ–‡ä»¶ {i}/{len(document_files)}: {filename}")
                    print(f"  ğŸ“– æ­£åœ¨æå–æ–‡ä»¶å†…å®¹...", end="", flush=True)
                    
                    # æå–æ–‡ä»¶å†…å®¹
                    extract_start = time.time()
                    extracted_content = self._extract_file_content(str(file_path))
                    extract_time = round(time.time() - extract_start, 3)
                    
                    # æ ¹æ®è®¾ç½®æˆªå–å†…å®¹ç”¨äºAIå¤„ç†
                    truncate_length = self.ai_parameters['content_extraction_length'] if self.ai_parameters['content_extraction_length'] < 2000 else len(extracted_content)
                    content_for_ai = extracted_content[:truncate_length] if extracted_content else ""
                    
                    print(f"\r  ğŸ“– æ­£åœ¨ç”Ÿæˆæ‘˜è¦...", end="", flush=True)
                    
                    # ç”Ÿæˆæ‘˜è¦
                    summary_start = time.time()
                    summary = self._generate_content_summary(content_for_ai, filename, summary_length)
                    summary_time = round(time.time() - summary_start, 3)
                    
                    print(f"\r  âœ… è§£è¯»å®Œæˆ")
                    print(f"     æ‘˜è¦: {summary[:100]}{'...' if len(summary) > 100 else ''}")
                    
                    # æ„å»ºAIç»“æœé¡¹ï¼ˆä¸è¿ç§»åŠŸèƒ½æ ¼å¼å…¼å®¹ï¼‰
                    ai_result_item = {
                        "å¤„ç†æ—¶é—´": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        "æ–‡ä»¶å": filename,
                        "æºæ–‡ä»¶è·¯å¾„": str(file_path),  # è¿™é‡Œä½œä¸ºç›®æ ‡è·¯å¾„ï¼Œå› ä¸ºæ–‡ä»¶å·²åœ¨ç›®æ ‡ä½ç½®
                        "æ–‡ä»¶æ‘˜è¦": summary,
                        "æœ€åŒ¹é…çš„ç›®æ ‡ç›®å½•": str(file_path.parent),  # å½“å‰æ–‡ä»¶å¤¹ä½œä¸ºç›®æ ‡ç›®å½•
                        "åŒ¹é…ç†ç”±": "æ‰¹é‡æ–‡æ¡£è§£è¯»",
                        "å¤„ç†è€—æ—¶ä¿¡æ¯": {
                            "æ€»è€—æ—¶(ç§’)": round(extract_time + summary_time, 3),
                            "å†…å®¹æå–è€—æ—¶(ç§’)": extract_time,
                            "æ‘˜è¦ç”Ÿæˆè€—æ—¶(ç§’)": summary_time,
                            "ç›®å½•æ¨èè€—æ—¶(ç§’)": 0
                        },
                        "æœ€ç»ˆç›®æ ‡è·¯å¾„": str(file_path),  # æ–‡ä»¶å½“å‰ä½ç½®å°±æ˜¯æœ€ç»ˆä½ç½®
                        "æ“ä½œç±»å‹": "æ–‡æ¡£è§£è¯»",
                        "å¤„ç†çŠ¶æ€": "è§£è¯»æˆåŠŸ"
                    }
                    
                    # ä¿å­˜åˆ°AIç»“æœæ–‡ä»¶
                    self._append_ai_result_to_file(ai_result_file, ai_result_item)
                    
                    results['successful_reads'] += 1
                    logging.info(f"æ–‡æ¡£è§£è¯»æˆåŠŸ: {filename}ï¼Œæ‘˜è¦é•¿åº¦: {len(summary)} å­—ç¬¦")
                    
                except Exception as e:
                    error_msg = f"è§£è¯»æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {str(e)}"
                    print(f"\r  âŒ è§£è¯»å¤±è´¥: {str(e)}")
                    logging.error(error_msg)
                    results['errors'].append(error_msg)
                    results['failed_reads'] += 1
                    
                    # ä¿å­˜å¤±è´¥è®°å½•
                    try:
                        ai_result_item = {
                            "å¤„ç†æ—¶é—´": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                            "æ–‡ä»¶å": filename,
                            "æºæ–‡ä»¶è·¯å¾„": str(file_path),
                            "æ–‡ä»¶æ‘˜è¦": "",
                            "æœ€åŒ¹é…çš„ç›®æ ‡ç›®å½•": str(file_path.parent),
                            "åŒ¹é…ç†ç”±": "æ‰¹é‡æ–‡æ¡£è§£è¯»",
                            "å¤„ç†è€—æ—¶ä¿¡æ¯": {
                                "æ€»è€—æ—¶(ç§’)": 0,
                                "å†…å®¹æå–è€—æ—¶(ç§’)": 0,
                                "æ‘˜è¦ç”Ÿæˆè€—æ—¶(ç§’)": 0,
                                "ç›®å½•æ¨èè€—æ—¶(ç§’)": 0
                            },
                            "æœ€ç»ˆç›®æ ‡è·¯å¾„": str(file_path),
                            "æ“ä½œç±»å‹": "æ–‡æ¡£è§£è¯»",
                            "å¤„ç†çŠ¶æ€": "è§£è¯»å¤±è´¥",
                            "é”™è¯¯ä¿¡æ¯": str(e)
                        }
                        self._append_ai_result_to_file(ai_result_file, ai_result_item)
                    except Exception as save_error:
                        logging.warning(f"ä¿å­˜å¤±è´¥è®°å½•æ—¶å‡ºé”™: {save_error}")
                
                finally:
                    results['processed_files'] += 1
            
            results['end_time'] = datetime.now()
            
            # è¾“å‡ºå¤„ç†å®Œæˆæ€»ç»“
            print(f"\n=== æ‰¹é‡æ–‡æ¡£è§£è¯»å®Œæˆ ===")
            print(f"æ€»æ–‡ä»¶æ•°: {results['total_files']}")
            print(f"æˆåŠŸè§£è¯»: {results['successful_reads']} ä¸ª")
            print(f"è§£è¯»å¤±è´¥: {results['failed_reads']} ä¸ª")
            duration = (results['end_time'] - results['start_time']).total_seconds()
            print(f"æ€»è€—æ—¶: {duration:.1f} ç§’")
            print("=" * 50)
            
            logging.info(f"æ‰¹é‡æ–‡æ¡£è§£è¯»å®Œæˆ: æˆåŠŸ {results['successful_reads']}, å¤±è´¥ {results['failed_reads']}")
            logging.info(f"è§£è¯»ç»“æœå·²ä¿å­˜åˆ°: {ai_result_file}")
            
            return results
            
        except Exception as e:
            raise FileOrganizerError(f"æ‰¹é‡æ–‡æ¡£è§£è¯»å¤±è´¥: {e}")